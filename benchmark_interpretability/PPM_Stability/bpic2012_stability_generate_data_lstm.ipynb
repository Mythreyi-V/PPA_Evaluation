{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 961,
     "status": "ok",
     "timestamp": 1597794848189,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh6DP4Hn-qpp593Bc6lrayXXcdQv42KAS3zJ-Ay=s64",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "TNrzy43BSk0S",
    "outputId": "b78bdb37-75ad-4e66-e3b2-e435bcdffd78"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "import sys\n",
    "#PATH = '/content/drive/My Drive/PPM_Stability/'\n",
    "PATH = \"C:/Users/velmurug/Documents/Stability Experiments/benchmark_interpretability/PPM_Stability/\"\n",
    "#PATH = \"C:/Users/mythr/Documents/GitHub/Stability-Experiments/benchmark_interpretability/PPM_Stability/\"\n",
    "#PATH = \"C:/Users/n9455647/Documents/GitHub/Stability-Experiments/benchmark_interpretability/PPM_Stability/\"\n",
    "sys.path.append(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 171
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9630,
     "status": "ok",
     "timestamp": 1597794856873,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh6DP4Hn-qpp593Bc6lrayXXcdQv42KAS3zJ-Ay=s64",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "OH7t3uEIU6e0",
    "outputId": "b8659c9d-4c9f-4c5c-d928-7cfe21348260"
   },
   "outputs": [],
   "source": [
    "#!pip install pandas==0.22.0\n",
    "#!pip install xgboost==1.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11828,
     "status": "ok",
     "timestamp": 1597794859084,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh6DP4Hn-qpp593Bc6lrayXXcdQv42KAS3zJ-Ay=s64",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "5sny80CkSAnW"
   },
   "outputs": [],
   "source": [
    "import EncoderFactory\n",
    "from DatasetManager import DatasetManager\n",
    "import BucketFactory\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "from sys import argv\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "import catboost\n",
    "\n",
    "from tensorflow.keras.backend import print_tensor\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Embedding, Flatten, Input\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import Nadam, RMSprop\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "#import lime\n",
    "#import lime.lime_tabular\n",
    "#from lime import submodular_pick;\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11818,
     "status": "ok",
     "timestamp": 1597794859089,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh6DP4Hn-qpp593Bc6lrayXXcdQv42KAS3zJ-Ay=s64",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "g32E_o5LSAne",
    "outputId": "e9c23b91-ebd4-4db7-d2b2-f015fea397fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bpic2012_accepted']\n"
     ]
    }
   ],
   "source": [
    "dataset_ref = \"bpic2012\"\n",
    "params_dir = PATH + \"params/\"\n",
    "results_dir = \"results\"\n",
    "bucket_method = \"single\"\n",
    "cls_encoding = \"agg\"\n",
    "cls_method = \"lstm\"\n",
    "gap = 1\n",
    "n_iter = 1\n",
    "\n",
    "if bucket_method == \"state\":\n",
    "    bucket_encoding = \"last\"\n",
    "else:\n",
    "    bucket_encoding = \"agg\"\n",
    "\n",
    "method_name = \"%s_%s\"%(bucket_method, cls_encoding)\n",
    "\n",
    "dataset_ref_to_datasets = {\n",
    "    #\"bpic2011\": [\"bpic2011_f%s\"%formula for formula in range(1,5)],\n",
    "    \"bpic2015\": [\"bpic2015_%s_f2\"%(municipality) for municipality in range(5,6)],\n",
    "    \"bpic2017\" : [\"bpic2017_accepted\"],\n",
    "    \"bpic2012\" : [\"bpic2012_accepted\"]\n",
    "    #\"insurance\": [\"insurance_activity\", \"insurance_followup\"],\n",
    "    #\"sepsis_cases\": [\"sepsis_cases_1\", \"sepsis_cases_2\", \"sepsis_cases_4\"]\n",
    "}\n",
    "\n",
    "encoding_dict = {\n",
    "    \"laststate\": [\"static\", \"last\"],\n",
    "    \"agg\": [\"static\", \"agg\"],\n",
    "    \"index\": [\"static\", \"index\"],\n",
    "    \"combined\": [\"static\", \"last\", \"agg\"]\n",
    "}\n",
    "\n",
    "datasets = [dataset_ref] if dataset_ref not in dataset_ref_to_datasets else dataset_ref_to_datasets[dataset_ref]\n",
    "methods = encoding_dict[cls_encoding]\n",
    "    \n",
    "train_ratio = 0.8\n",
    "random_state = 22\n",
    "\n",
    "# create results directory\n",
    "if not os.path.exists(os.path.join(params_dir)):\n",
    "    os.makedirs(os.path.join(params_dir))\n",
    "    \n",
    "print(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O5tFTI_cSAnn",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up parameters...\n",
      "{'dense_layers': {'dense2_nodes': 30, 'layers': 'two'}, 'epochs': 500, 'learning_rate': 0.8031739594630133, 'lstm1_dropouts': 0.11640683929620399, 'lstm1_nodes': 80, 'lstm_layers': {'layers': 'one'}, 'optimizer': 'rmsprop'}\n",
      "setting up data...\n",
      "Epoch 1/500\n",
      "\n",
      "Epoch 00001: saving model to C:/Users/velmurug/Documents/Stability Experiments/benchmark_interpretability/PPM_Stability/bpic2012/lstm/cls\\checkpoint.cpt\n",
      "555/555 - 20s - loss: 18.6657 - val_loss: 3.5520\n",
      "Epoch 2/500\n",
      "\n",
      "Epoch 00002: saving model to C:/Users/velmurug/Documents/Stability Experiments/benchmark_interpretability/PPM_Stability/bpic2012/lstm/cls\\checkpoint.cpt\n",
      "555/555 - 20s - loss: 0.7207 - val_loss: 9.0509\n",
      "Epoch 3/500\n",
      "\n",
      "Epoch 00003: saving model to C:/Users/velmurug/Documents/Stability Experiments/benchmark_interpretability/PPM_Stability/bpic2012/lstm/cls\\checkpoint.cpt\n",
      "555/555 - 20s - loss: 0.7234 - val_loss: 9.9411\n",
      "Epoch 4/500\n",
      "\n",
      "Epoch 00004: saving model to C:/Users/velmurug/Documents/Stability Experiments/benchmark_interpretability/PPM_Stability/bpic2012/lstm/cls\\checkpoint.cpt\n",
      "555/555 - 20s - loss: 0.7207 - val_loss: 12.2965\n",
      "Epoch 5/500\n",
      "\n",
      "Epoch 00005: saving model to C:/Users/velmurug/Documents/Stability Experiments/benchmark_interpretability/PPM_Stability/bpic2012/lstm/cls\\checkpoint.cpt\n",
      "555/555 - 20s - loss: 0.7190 - val_loss: 11.3866\n",
      "Epoch 6/500\n",
      "\n",
      "Epoch 00006: saving model to C:/Users/velmurug/Documents/Stability Experiments/benchmark_interpretability/PPM_Stability/bpic2012/lstm/cls\\checkpoint.cpt\n",
      "555/555 - 20s - loss: 0.7206 - val_loss: 9.8603\n",
      "Epoch 7/500\n",
      "\n",
      "Epoch 00007: saving model to C:/Users/velmurug/Documents/Stability Experiments/benchmark_interpretability/PPM_Stability/bpic2012/lstm/cls\\checkpoint.cpt\n",
      "555/555 - 20s - loss: 0.7217 - val_loss: 8.6420\n",
      "Epoch 8/500\n",
      "\n",
      "Epoch 00008: saving model to C:/Users/velmurug/Documents/Stability Experiments/benchmark_interpretability/PPM_Stability/bpic2012/lstm/cls\\checkpoint.cpt\n",
      "555/555 - 20s - loss: 0.7248 - val_loss: 11.2083\n",
      "Epoch 9/500\n",
      "\n",
      "Epoch 00009: saving model to C:/Users/velmurug/Documents/Stability Experiments/benchmark_interpretability/PPM_Stability/bpic2012/lstm/cls\\checkpoint.cpt\n",
      "555/555 - 20s - loss: 0.7224 - val_loss: 11.0724\n",
      "Epoch 10/500\n",
      "\n",
      "Epoch 00010: saving model to C:/Users/velmurug/Documents/Stability Experiments/benchmark_interpretability/PPM_Stability/bpic2012/lstm/cls\\checkpoint.cpt\n",
      "555/555 - 20s - loss: 0.7214 - val_loss: 9.1152\n",
      "Epoch 11/500\n",
      "\n",
      "Epoch 00011: saving model to C:/Users/velmurug/Documents/Stability Experiments/benchmark_interpretability/PPM_Stability/bpic2012/lstm/cls\\checkpoint.cpt\n",
      "555/555 - 20s - loss: 0.7220 - val_loss: 9.6978\n",
      "Epoch 12/500\n",
      "\n",
      "Epoch 00012: saving model to C:/Users/velmurug/Documents/Stability Experiments/benchmark_interpretability/PPM_Stability/bpic2012/lstm/cls\\checkpoint.cpt\n",
      "555/555 - 20s - loss: 0.7212 - val_loss: 9.7157\n",
      "Epoch 13/500\n",
      "\n",
      "Epoch 00013: saving model to C:/Users/velmurug/Documents/Stability Experiments/benchmark_interpretability/PPM_Stability/bpic2012/lstm/cls\\checkpoint.cpt\n",
      "555/555 - 20s - loss: 0.7203 - val_loss: 10.8385\n",
      "Epoch 14/500\n",
      "\n",
      "Epoch 00014: saving model to C:/Users/velmurug/Documents/Stability Experiments/benchmark_interpretability/PPM_Stability/bpic2012/lstm/cls\\checkpoint.cpt\n",
      "555/555 - 20s - loss: 0.7211 - val_loss: 12.2100\n",
      "Epoch 15/500\n",
      "\n",
      "Epoch 00015: saving model to C:/Users/velmurug/Documents/Stability Experiments/benchmark_interpretability/PPM_Stability/bpic2012/lstm/cls\\checkpoint.cpt\n",
      "555/555 - 20s - loss: 0.7211 - val_loss: 10.2251\n",
      "Epoch 16/500\n",
      "\n",
      "Epoch 00016: saving model to C:/Users/velmurug/Documents/Stability Experiments/benchmark_interpretability/PPM_Stability/bpic2012/lstm/cls\\checkpoint.cpt\n",
      "555/555 - 20s - loss: 0.7211 - val_loss: 7.9122\n",
      "Epoch 17/500\n",
      "\n",
      "Epoch 00017: saving model to C:/Users/velmurug/Documents/Stability Experiments/benchmark_interpretability/PPM_Stability/bpic2012/lstm/cls\\checkpoint.cpt\n",
      "555/555 - 20s - loss: 0.7227 - val_loss: 11.6580\n",
      "Epoch 18/500\n",
      "\n",
      "Epoch 00018: saving model to C:/Users/velmurug/Documents/Stability Experiments/benchmark_interpretability/PPM_Stability/bpic2012/lstm/cls\\checkpoint.cpt\n",
      "555/555 - 20s - loss: 0.7192 - val_loss: 10.6460\n",
      "Epoch 19/500\n",
      "\n",
      "Epoch 00019: saving model to C:/Users/velmurug/Documents/Stability Experiments/benchmark_interpretability/PPM_Stability/bpic2012/lstm/cls\\checkpoint.cpt\n",
      "555/555 - 20s - loss: 0.7229 - val_loss: 10.7184\n",
      "Epoch 20/500\n",
      "\n",
      "Epoch 00020: saving model to C:/Users/velmurug/Documents/Stability Experiments/benchmark_interpretability/PPM_Stability/bpic2012/lstm/cls\\checkpoint.cpt\n",
      "555/555 - 21s - loss: 0.7226 - val_loss: 11.3304\n",
      "Epoch 21/500\n",
      "\n",
      "Epoch 00021: saving model to C:/Users/velmurug/Documents/Stability Experiments/benchmark_interpretability/PPM_Stability/bpic2012/lstm/cls\\checkpoint.cpt\n",
      "555/555 - 20s - loss: 0.7211 - val_loss: 11.4200\n",
      "Epoch 22/500\n",
      "\n",
      "Epoch 00022: saving model to C:/Users/velmurug/Documents/Stability Experiments/benchmark_interpretability/PPM_Stability/bpic2012/lstm/cls\\checkpoint.cpt\n",
      "555/555 - 20s - loss: 0.7230 - val_loss: 9.6328\n",
      "Epoch 23/500\n",
      "\n",
      "Epoch 00023: saving model to C:/Users/velmurug/Documents/Stability Experiments/benchmark_interpretability/PPM_Stability/bpic2012/lstm/cls\\checkpoint.cpt\n",
      "555/555 - 20s - loss: 0.7187 - val_loss: 7.3173\n",
      "Epoch 24/500\n",
      "\n",
      "Epoch 00024: saving model to C:/Users/velmurug/Documents/Stability Experiments/benchmark_interpretability/PPM_Stability/bpic2012/lstm/cls\\checkpoint.cpt\n",
      "555/555 - 20s - loss: 0.7203 - val_loss: 11.1683\n",
      "Epoch 25/500\n",
      "\n",
      "Epoch 00025: saving model to C:/Users/velmurug/Documents/Stability Experiments/benchmark_interpretability/PPM_Stability/bpic2012/lstm/cls\\checkpoint.cpt\n",
      "555/555 - 20s - loss: 0.7202 - val_loss: 10.7975\n",
      "Epoch 26/500\n",
      "\n",
      "Epoch 00026: saving model to C:/Users/velmurug/Documents/Stability Experiments/benchmark_interpretability/PPM_Stability/bpic2012/lstm/cls\\checkpoint.cpt\n",
      "555/555 - 20s - loss: 0.7234 - val_loss: 11.7964\n",
      "Epoch 27/500\n",
      "\n",
      "Epoch 00027: saving model to C:/Users/velmurug/Documents/Stability Experiments/benchmark_interpretability/PPM_Stability/bpic2012/lstm/cls\\checkpoint.cpt\n",
      "555/555 - 20s - loss: 0.7221 - val_loss: 9.2321\n",
      "Epoch 28/500\n",
      "\n",
      "Epoch 00028: saving model to C:/Users/velmurug/Documents/Stability Experiments/benchmark_interpretability/PPM_Stability/bpic2012/lstm/cls\\checkpoint.cpt\n",
      "555/555 - 20s - loss: 1.3785 - val_loss: 0.9248\n",
      "Epoch 29/500\n",
      "\n",
      "Epoch 00029: saving model to C:/Users/velmurug/Documents/Stability Experiments/benchmark_interpretability/PPM_Stability/bpic2012/lstm/cls\\checkpoint.cpt\n",
      "555/555 - 20s - loss: 0.7196 - val_loss: 0.7090\n",
      "Epoch 30/500\n",
      "\n",
      "Epoch 00030: saving model to C:/Users/velmurug/Documents/Stability Experiments/benchmark_interpretability/PPM_Stability/bpic2012/lstm/cls\\checkpoint.cpt\n",
      "555/555 - 20s - loss: 0.7264 - val_loss: 0.7034\n",
      "Epoch 31/500\n",
      "\n",
      "Epoch 00031: saving model to C:/Users/velmurug/Documents/Stability Experiments/benchmark_interpretability/PPM_Stability/bpic2012/lstm/cls\\checkpoint.cpt\n",
      "555/555 - 20s - loss: 0.7205 - val_loss: 0.9497\n",
      "Epoch 32/500\n",
      "\n",
      "Epoch 00032: saving model to C:/Users/velmurug/Documents/Stability Experiments/benchmark_interpretability/PPM_Stability/bpic2012/lstm/cls\\checkpoint.cpt\n",
      "555/555 - 20s - loss: 0.7236 - val_loss: 0.9990\n",
      "Epoch 33/500\n",
      "\n",
      "Epoch 00033: saving model to C:/Users/velmurug/Documents/Stability Experiments/benchmark_interpretability/PPM_Stability/bpic2012/lstm/cls\\checkpoint.cpt\n",
      "555/555 - 20s - loss: 0.7227 - val_loss: 1.0046\n",
      "Epoch 34/500\n",
      "\n",
      "Epoch 00034: saving model to C:/Users/velmurug/Documents/Stability Experiments/benchmark_interpretability/PPM_Stability/bpic2012/lstm/cls\\checkpoint.cpt\n",
      "555/555 - 20s - loss: 0.7227 - val_loss: 0.9981\n",
      "Epoch 35/500\n",
      "\n",
      "Epoch 00035: saving model to C:/Users/velmurug/Documents/Stability Experiments/benchmark_interpretability/PPM_Stability/bpic2012/lstm/cls\\checkpoint.cpt\n",
      "555/555 - 20s - loss: 0.7230 - val_loss: 1.0727\n",
      "Epoch 36/500\n",
      "\n",
      "Epoch 00036: saving model to C:/Users/velmurug/Documents/Stability Experiments/benchmark_interpretability/PPM_Stability/bpic2012/lstm/cls\\checkpoint.cpt\n",
      "555/555 - 20s - loss: 0.7205 - val_loss: 0.8714\n",
      "Epoch 37/500\n",
      "\n",
      "Epoch 00037: saving model to C:/Users/velmurug/Documents/Stability Experiments/benchmark_interpretability/PPM_Stability/bpic2012/lstm/cls\\checkpoint.cpt\n",
      "555/555 - 20s - loss: 0.7235 - val_loss: 1.0038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/500\n",
      "\n",
      "Epoch 00038: saving model to C:/Users/velmurug/Documents/Stability Experiments/benchmark_interpretability/PPM_Stability/bpic2012/lstm/cls\\checkpoint.cpt\n",
      "555/555 - 20s - loss: 0.7213 - val_loss: 1.0175\n",
      "Epoch 39/500\n",
      "\n",
      "Epoch 00039: saving model to C:/Users/velmurug/Documents/Stability Experiments/benchmark_interpretability/PPM_Stability/bpic2012/lstm/cls\\checkpoint.cpt\n",
      "555/555 - 20s - loss: 0.7205 - val_loss: 0.9685\n",
      "Epoch 40/500\n",
      "\n",
      "Epoch 00040: saving model to C:/Users/velmurug/Documents/Stability Experiments/benchmark_interpretability/PPM_Stability/bpic2012/lstm/cls\\checkpoint.cpt\n",
      "555/555 - 20s - loss: 0.7255 - val_loss: 1.0548\n",
      "Epoch 41/500\n",
      "\n",
      "Epoch 00041: saving model to C:/Users/velmurug/Documents/Stability Experiments/benchmark_interpretability/PPM_Stability/bpic2012/lstm/cls\\checkpoint.cpt\n",
      "555/555 - 20s - loss: 0.7211 - val_loss: 1.0963\n",
      "Epoch 42/500\n",
      "\n",
      "Epoch 00042: saving model to C:/Users/velmurug/Documents/Stability Experiments/benchmark_interpretability/PPM_Stability/bpic2012/lstm/cls\\checkpoint.cpt\n",
      "555/555 - 20s - loss: 0.7195 - val_loss: 1.2056\n",
      "Epoch 43/500\n",
      "\n",
      "Epoch 00043: saving model to C:/Users/velmurug/Documents/Stability Experiments/benchmark_interpretability/PPM_Stability/bpic2012/lstm/cls\\checkpoint.cpt\n",
      "555/555 - 20s - loss: 0.7219 - val_loss: 0.9558\n",
      "Epoch 44/500\n",
      "\n",
      "Epoch 00044: saving model to C:/Users/velmurug/Documents/Stability Experiments/benchmark_interpretability/PPM_Stability/bpic2012/lstm/cls\\checkpoint.cpt\n",
      "555/555 - 20s - loss: 1.6551 - val_loss: 1.0599\n",
      "Epoch 45/500\n",
      "\n",
      "Epoch 00045: saving model to C:/Users/velmurug/Documents/Stability Experiments/benchmark_interpretability/PPM_Stability/bpic2012/lstm/cls\\checkpoint.cpt\n",
      "555/555 - 20s - loss: 0.7259 - val_loss: 3.6100\n",
      "Epoch 46/500\n",
      "\n",
      "Epoch 00046: saving model to C:/Users/velmurug/Documents/Stability Experiments/benchmark_interpretability/PPM_Stability/bpic2012/lstm/cls\\checkpoint.cpt\n",
      "555/555 - 20s - loss: 0.7211 - val_loss: 4.7135\n",
      "Epoch 47/500\n",
      "\n",
      "Epoch 00047: saving model to C:/Users/velmurug/Documents/Stability Experiments/benchmark_interpretability/PPM_Stability/bpic2012/lstm/cls\\checkpoint.cpt\n",
      "555/555 - 20s - loss: 0.7210 - val_loss: 5.8898\n",
      "Epoch 48/500\n",
      "\n",
      "Epoch 00048: saving model to C:/Users/velmurug/Documents/Stability Experiments/benchmark_interpretability/PPM_Stability/bpic2012/lstm/cls\\checkpoint.cpt\n",
      "555/555 - 20s - loss: 0.7228 - val_loss: 9.7294\n",
      "Epoch 49/500\n",
      "\n",
      "Epoch 00049: saving model to C:/Users/velmurug/Documents/Stability Experiments/benchmark_interpretability/PPM_Stability/bpic2012/lstm/cls\\checkpoint.cpt\n",
      "555/555 - 20s - loss: 0.7223 - val_loss: 11.8552\n",
      "Epoch 50/500\n",
      "\n",
      "Epoch 00050: saving model to C:/Users/velmurug/Documents/Stability Experiments/benchmark_interpretability/PPM_Stability/bpic2012/lstm/cls\\checkpoint.cpt\n",
      "555/555 - 20s - loss: 0.7228 - val_loss: 10.9948\n",
      "Epoch 51/500\n",
      "\n",
      "Epoch 00051: saving model to C:/Users/velmurug/Documents/Stability Experiments/benchmark_interpretability/PPM_Stability/bpic2012/lstm/cls\\checkpoint.cpt\n",
      "555/555 - 20s - loss: 0.7218 - val_loss: 16.9540\n",
      "Epoch 52/500\n",
      "\n",
      "Epoch 00052: saving model to C:/Users/velmurug/Documents/Stability Experiments/benchmark_interpretability/PPM_Stability/bpic2012/lstm/cls\\checkpoint.cpt\n",
      "555/555 - 20s - loss: 0.7177 - val_loss: 13.7896\n",
      "Epoch 53/500\n",
      "\n",
      "Epoch 00053: saving model to C:/Users/velmurug/Documents/Stability Experiments/benchmark_interpretability/PPM_Stability/bpic2012/lstm/cls\\checkpoint.cpt\n",
      "555/555 - 20s - loss: 0.7236 - val_loss: 14.6645\n",
      "Epoch 54/500\n",
      "\n",
      "Epoch 00054: saving model to C:/Users/velmurug/Documents/Stability Experiments/benchmark_interpretability/PPM_Stability/bpic2012/lstm/cls\\checkpoint.cpt\n",
      "555/555 - 20s - loss: 0.7227 - val_loss: 15.8590\n",
      "Epoch 55/500\n"
     ]
    }
   ],
   "source": [
    "for dataset_name in datasets:\n",
    "    \n",
    "    # load optimal params\n",
    "    print(\"Setting up parameters...\")\n",
    "    optimal_params_filename = os.path.join(params_dir, \"optimal_params_%s_%s_%s.pickle\" % (cls_method, dataset_name, method_name))\n",
    "\n",
    "    if not os.path.isfile(optimal_params_filename) or os.path.getsize(optimal_params_filename) <= 0:\n",
    "        print(\"Parameters not found\")\n",
    "        \n",
    "    with open(optimal_params_filename, \"rb\") as fin:\n",
    "        args = pickle.load(fin)\n",
    "        \n",
    "    print(args)\n",
    "            \n",
    "    # read the data\n",
    "    print(\"setting up data...\")\n",
    "    dataset_manager = DatasetManager(dataset_name)\n",
    "    data = dataset_manager.read_dataset()\n",
    "    #print('Case ID column', dataset_manager.case_id_col)\n",
    "    cls_encoder_args = {'case_id_col': dataset_manager.case_id_col, \n",
    "                        'static_cat_cols': dataset_manager.static_cat_cols,\n",
    "                        'static_num_cols': dataset_manager.static_num_cols, \n",
    "                        'dynamic_cat_cols': dataset_manager.dynamic_cat_cols,\n",
    "                        'dynamic_num_cols': dataset_manager.dynamic_num_cols, \n",
    "                        'fillna': True}\n",
    "\n",
    "    # determine min and max (truncated) prefix lengths\n",
    "    min_prefix_length = 1\n",
    "    if \"traffic_fines\" in dataset_name:\n",
    "        max_prefix_length = 10\n",
    "    elif \"bpic2017\" in dataset_name:\n",
    "        max_prefix_length = min(20, dataset_manager.get_pos_case_length_quantile(data, 0.90))\n",
    "    else:\n",
    "        max_prefix_length = min(40, dataset_manager.get_pos_case_length_quantile(data, 0.90))\n",
    "\n",
    "    # split into training and test\n",
    "    train, test = dataset_manager.split_data_strict(data, train_ratio, split=\"temporal\")\n",
    "    train, val = dataset_manager.split_data_strict(train, 0.2)\n",
    "    \n",
    "    if gap > 1:\n",
    "        outfile = os.path.join(results_dir, \"performance_results_%s_%s_%s_gap%s.csv\" % (cls_method, dataset_name, method_name, gap))\n",
    "    else:\n",
    "        outfile = os.path.join(results_dir, \"performance_results_%s_%s_%s.csv\" % (cls_method, dataset_name, method_name))\n",
    "        \n",
    "    start_test_prefix_generation = time.time()\n",
    "    dt_test_prefixes = dataset_manager.generate_prefix_data(test, min_prefix_length, max_prefix_length)\n",
    "    test_prefix_generation_time = time.time() - start_test_prefix_generation\n",
    "            \n",
    "    offline_total_times = []\n",
    "    online_event_times = []\n",
    "    train_prefix_generation_times = []\n",
    "\n",
    "    for ii in range(n_iter):\n",
    "        if cls_method == \"lstm\":\n",
    "            max_len = max_prefix_length\n",
    "            \n",
    "            preds_all = []\n",
    "            probas_all = []\n",
    "            pred_types_all = []\n",
    "            test_y_all = []\n",
    "            nr_events_all = []\n",
    "            \n",
    "            dt_train = dataset_manager.encode_data_for_lstm(train)\n",
    "            dt_test = dataset_manager.encode_data_for_lstm(test)\n",
    "            dt_val = dataset_manager.encode_data_for_lstm(val)\n",
    "\n",
    "            data_dim = dt_train.shape[1]-3\n",
    "\n",
    "            dt_train_bucket, train_y = dataset_manager.generate_3d_data(dt_train, max_len)\n",
    "            dt_test_bucket, y_test= dataset_manager.generate_3d_data(dt_test, max_len)\n",
    "            dt_val_bucket, val_y = dataset_manager.generate_3d_data(dt_val, max_len)\n",
    "            test_y = [np.where(act == 1)[0][0] for act in y_test]\n",
    "            \n",
    "            #save model parameters and create callback for model\n",
    "            params_path = os.path.join(PATH, \"%s/%s/cls/params.pickle\" % (dataset_ref, cls_method))\n",
    "            with open(params_path, 'wb') as f:\n",
    "                pickle.dump(args, f)\n",
    "            \n",
    "            checkpoint_path = os.path.join(PATH, \"%s/%s/cls/checkpoint.cpt\" % (dataset_ref, cls_method))\n",
    "            cp_callback = ModelCheckpoint(filepath=checkpoint_path, save_weights_only=True, verbose=1)\n",
    "            \n",
    "            early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose = 1, patience = 30)\n",
    "            \n",
    "            #create model\n",
    "            main_input = Input(shape=(max_len, data_dim), name='main_input')\n",
    "\n",
    "            if args[\"lstm_layers\"][\"layers\"] == \"one\":\n",
    "                l2_3 = LSTM(args['lstm1_nodes'], input_shape=(max_len, data_dim), implementation=2,\n",
    "                            kernel_initializer='glorot_uniform', return_sequences=False, \n",
    "                            dropout=args['lstm1_dropouts'])(main_input)\n",
    "                b2_3 = BatchNormalization()(l2_3)\n",
    "\n",
    "            if args[\"lstm_layers\"][\"layers\"] == \"two\":\n",
    "                l1 = LSTM(args['lstm1_nodes'], input_shape=(max_len, data_dim), implementation=2,\n",
    "                            kernel_initializer='glorot_uniform', return_sequences=True, \n",
    "                            dropout=args['lstm1_dropouts'])(main_input)\n",
    "                b1 = BatchNormalization()(l1)\n",
    "                l2_3 = LSTM(args[\"lstm_layers\"][\"lstm2_nodes\"], activation=\"sigmoid\", \n",
    "                            implementation=2, kernel_initializer='glorot_uniform', return_sequences=False, \n",
    "                            dropout=args[\"lstm_layers\"][\"lstm2_dropouts\"])(b1)\n",
    "                b2_3 = BatchNormalization()(l2_3)\n",
    "\n",
    "            if args['dense_layers']['layers'] == \"two\":\n",
    "                d1 = Dense(args['dense_layers']['dense2_nodes'], activation = \"relu\")(b2_3)\n",
    "                outcome_output = Dense(2, activation='sigmoid', kernel_initializer='glorot_uniform', name='outcome_output')(d1)\n",
    "\n",
    "            else:\n",
    "                outcome_output = Dense(2, activation='sigmoid', kernel_initializer='glorot_uniform', name='outcome_output')(d1)\n",
    "\n",
    "            cls = Model(inputs=[main_input], outputs=[outcome_output])\n",
    "\n",
    "            if args['optimizer'] == \"adam\":\n",
    "                opt = Nadam(lr=args['learning_rate'], beta_1=0.9, beta_2=0.999, epsilon=1e-08, schedule_decay=0.004, clipvalue=3)\n",
    "            elif args['optimizer'] == \"rmsprop\":\n",
    "                opt = RMSprop(lr=args['learning_rate'], rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "            cls.compile(loss='binary_crossentropy', optimizer=opt)\n",
    "\n",
    "            history = cls.fit(dt_train_bucket, train_y, validation_data = (dt_val_bucket,val_y), verbose = 2, \n",
    "                                              epochs = args['epochs'], callbacks=[early_stop, cp_callback])\n",
    "            \n",
    "            print(cls.summary())\n",
    "            \n",
    "            # predict separately for each prefix case\n",
    "            preds = []\n",
    "            probas = []\n",
    "            pred_types = []\n",
    "\n",
    "            tp_list = []\n",
    "            tn_list = []\n",
    "            fp_list = []\n",
    "            fn_list = []\n",
    "            \n",
    "            #preds_pos_label_idx = np.where(cls.classes_ == 1)[0][0]\n",
    "            #probas = []\n",
    "            #for each in dt_test_bucket:\n",
    "            #    proba = cls.predict(np.array([each,]))#[:,preds_pos_label_idx]\n",
    "            #    probas.append(proba[0])\n",
    "\n",
    "            #preds = []\n",
    "            #for each in probas:\n",
    "            #    pred = each.argmax(axis=-1)\n",
    "            #    preds.append(pred)\n",
    "                \n",
    "            iteration = 1\n",
    "\n",
    "            test_all_grouped = dt_test.groupby(dataset_manager.case_id_col)\n",
    "\n",
    "\n",
    "            for _ , each in test_all_grouped:\n",
    "                print (\"Testing instance\", inst+1, 'of', len(test_all_grouped))\n",
    "                nr_length = each.shape[0]\n",
    "                group, y_test_group, case_ids = dataset_manager.generate_3d_data_for_single_case(each, max_len, nr_events = 1)\n",
    "                test_y_group = np.argmax(y_test_group)\n",
    "                test_y_all.append(test_y_group)\n",
    "\n",
    "                start = time.time()\n",
    "                case_id = case_ids[0]\n",
    "\n",
    "                proba = cls.predict(group)\n",
    "                pred = np.argmax(axis=-1)\n",
    "\n",
    "                if test_y_group == pred & pred == 0:\n",
    "                  pred_type = 'TN'\n",
    "\n",
    "                  instance_dict = {}\n",
    "                  instance_dict['caseID'] = case_id\n",
    "                  instance_dict['input'] = group\n",
    "                  instance_dict['actual'] = test_y_group[0]\n",
    "                  instance_dict['predicted'] = pred\n",
    "                  instance_dict['proba'] = 1 - proba[0]\n",
    "                  instance_dict['nr_events'] = nr_length\n",
    "                  instance_dict['pred_type'] = pred_type\n",
    "\n",
    "                  tn_list.append(instance_dict)\n",
    "\n",
    "                elif test_y_group == pred & pred == 1:\n",
    "                  pred_type = 'TP'\n",
    "\n",
    "                  instance_dict = {}\n",
    "                  instance_dict['caseID'] = case_id\n",
    "                  instance_dict['input'] = group\n",
    "                  instance_dict['actual'] = test_y_group[0]\n",
    "                  instance_dict['predicted'] = pred\n",
    "                  instance_dict['proba'] = proba[0]\n",
    "                  instance_dict['nr_events'] = nr_length\n",
    "                  instance_dict['pred_type'] = pred_type\n",
    "\n",
    "                  tp_list.append(instance_dict)\n",
    "\n",
    "                elif test_y_group != pred & pred == 0:\n",
    "                  pred_type = 'FN'\n",
    "\n",
    "                  instance_dict = {}\n",
    "                  instance_dict['caseID'] = case_id\n",
    "                  instance_dict['input'] = group\n",
    "                  instance_dict['actual'] = test_y_group[0]\n",
    "                  instance_dict['predicted'] = pred\n",
    "                  instance_dict['proba'] = 1 - proba[0]\n",
    "                  instance_dict['nr_events'] = nr_length\n",
    "                  instance_dict['pred_type'] = pred_type\n",
    "\n",
    "                  fn_list.append(instance_dict)\n",
    "\n",
    "                elif test_y_group != pred & pred == 1:\n",
    "                  pred_type = 'FP'\n",
    "\n",
    "                  instance_dict = {}\n",
    "                  instance_dict['caseID'] = case_id\n",
    "                  instance_dict['input'] = group\n",
    "                  instance_dict['actual'] = test_y_group[0]\n",
    "                  instance_dict['predicted'] = pred\n",
    "                  instance_dict['proba'] = proba[0]\n",
    "                  instance_dict['nr_events'] = nr_length\n",
    "                  instance_dict['pred_type'] = pred_type\n",
    "\n",
    "                  fp_list.append(instance_dict)\n",
    "                else:\n",
    "                  print(\"ERROR: Prediction doesn't fit into any category. Check code.\")\n",
    "\n",
    "                preds.append(pred)\n",
    "                probas.extend(proba)\n",
    "                pred_types.append(pred_type)\n",
    "                iteration+=1\n",
    "\n",
    "            preds_all.extend(preds)\n",
    "            probas_all.extend(probas)\n",
    "            pred_types_all.extend(pred_types)\n",
    "\n",
    "            tn_path = os.path.join(PATH, \"%s/%s/instances/true_neg.pickle\" % (dataset_ref, cls_method))\n",
    "            tp_path = os.path.join(PATH, \"%s/%s/instances/true_pos.pickle\" % (dataset_ref, cls_method))\n",
    "            fn_path = os.path.join(PATH, \"%s/%s/instances/false_neg.pickle\" % (dataset_ref, cls_method))\n",
    "            fp_path = os.path.join(PATH, \"%s/%s/instances/false_pos.pickle\" % (dataset_ref, cls_method))\n",
    "\n",
    "            with open (tn_path, 'wb') as f:\n",
    "              pickle.dump(tn_list, f)\n",
    "            with open (tp_path, 'wb') as f:\n",
    "              pickle.dump(tp_list, f)\n",
    "            with open (fn_path, 'wb') as f:\n",
    "              pickle.dump(fn_list, f)\n",
    "            with open (fp_path, 'wb') as f:\n",
    "              pickle.dump(fp_list, f)\n",
    "\n",
    "            #Save training data\n",
    "            print(\"saving data...\")\n",
    "            X_train_path = os.path.join(PATH, \"%s/%s/train_data/prefixes.pickle\" % (dataset_ref, cls_method))\n",
    "            Y_train_path = os.path.join(PATH, \"%s/%s/train_data/labels.pickle\" % (dataset_ref, cls_method))\n",
    "            with open(X_train_path, 'wb') as f:\n",
    "              pickle.dump(dt_train_bucket, f)\n",
    "            with open(Y_train_path, 'wb') as f:\n",
    "              pickle.dump(train_y, f)\n",
    "\n",
    "            #Save testing data\n",
    "            X_test_path = os.path.join(PATH, \"%s/%s/test_data/prefixes.pickle\" % (dataset_ref, cls_method))\n",
    "            Y_test_path = os.path.join(PATH, \"%s/%s/test_data/labels.pickle\" % (dataset_ref, cls_method))\n",
    "            with open(X_test_path, 'wb') as f:\n",
    "                pickle.dump(dt_test_bucket, f)\n",
    "            with open(Y_test_path, 'wb') as f:\n",
    "                pickle.dump(y_test, f)\n",
    "\n",
    "\n",
    "            acc = accuracy_score(test_y, preds)\n",
    "            roc_auc = roc_auc_score(test_y, preds)\n",
    "            print(\"Accuracy:\", auc, \"\\tROCAUC:\", roc_auc)\n",
    "\n",
    "        else:\n",
    "            # create prefix logs\n",
    "            print(\"Creating logs...\")\n",
    "            start_train_prefix_generation = time.time()\n",
    "            dt_train_prefixes = dataset_manager.generate_prefix_data(train, min_prefix_length, max_prefix_length, gap)\n",
    "            train_prefix_generation_time = time.time() - start_train_prefix_generation\n",
    "            train_prefix_generation_times.append(train_prefix_generation_time)\n",
    "\n",
    "            # Bucketing prefixes based on control flow\n",
    "            print(\"bucketing prefixes...\")\n",
    "            bucketer_args = {'encoding_method':bucket_encoding, \n",
    "                             'case_id_col':dataset_manager.case_id_col, \n",
    "                             'cat_cols':[dataset_manager.activity_col], \n",
    "                             'num_cols':[], \n",
    "                             'random_state':random_state}\n",
    "            if bucket_method == \"cluster\":\n",
    "                bucketer_args[\"n_clusters\"] = int(args[\"n_clusters\"])\n",
    "            bucketer = BucketFactory.get_bucketer(bucket_method, **bucketer_args)\n",
    "\n",
    "            start_offline_time_bucket = time.time()\n",
    "            bucket_assignments_train = bucketer.fit_predict(dt_train_prefixes)\n",
    "            offline_time_bucket = time.time() - start_offline_time_bucket\n",
    "\n",
    "            bucket_assignments_test = bucketer.predict(dt_test_prefixes)\n",
    "\n",
    "            preds_all = []\n",
    "            probas_all = []\n",
    "            pred_types_all = []\n",
    "            test_y_all = []\n",
    "            nr_events_all = []\n",
    "            offline_time_fit = 0\n",
    "            current_online_event_times = []\n",
    "\n",
    "            for bucket in set(bucket_assignments_test):\n",
    "                print(\"Bucket\" , bucket )\n",
    "                print(\"sorting bucket...\")\n",
    "                if bucket_method == \"prefix\":\n",
    "                    current_args = args[bucket]\n",
    "                else:\n",
    "                    current_args = args\n",
    "                relevant_train_cases_bucket = dataset_manager.get_indexes(dt_train_prefixes)[bucket_assignments_train == bucket]\n",
    "                relevant_test_cases_bucket = dataset_manager.get_indexes(dt_test_prefixes)[bucket_assignments_test == bucket]\n",
    "                dt_test_bucket = dataset_manager.get_relevant_data_by_indexes(dt_test_prefixes, relevant_test_cases_bucket)\n",
    "\n",
    "                nr_events_all.extend(list(dataset_manager.get_prefix_lengths(dt_test_bucket)))\n",
    "                print('number events', len(nr_events_all))\n",
    "\n",
    "                if len(relevant_train_cases_bucket) == 0:\n",
    "                    preds = [dataset_manager.get_class_ratio(train)] * len(relevant_test_cases_bucket)\n",
    "                    current_online_event_times.extend([0] * len(preds))\n",
    "                else:\n",
    "                    dt_train_bucket = dataset_manager.get_relevant_data_by_indexes(dt_train_prefixes, relevant_train_cases_bucket) # one row per event\n",
    "                    train_y = dataset_manager.get_label_numeric(dt_train_bucket)\n",
    "\n",
    "                    if len(set(train_y)) < 2:\n",
    "                        preds = [train_y[0]] * len(relevant_test_cases_bucket)\n",
    "                        current_online_event_times.extend([0] * len(preds))\n",
    "                        test_y_all.extend(dataset_manager.get_label_numeric(dt_test_bucket))\n",
    "                    else:\n",
    "                        print(\"choosing classifier...\")\n",
    "                        start_offline_time_fit = time.time()\n",
    "                        feature_combiner = FeatureUnion([(method, EncoderFactory.get_encoder(method, **cls_encoder_args)) for method in methods])\n",
    "\n",
    "                        if cls_method == \"rf\":\n",
    "                            cls = RandomForestClassifier(n_estimators=500,\n",
    "                                                         max_features=current_args['max_features'],\n",
    "                                                         random_state=random_state)\n",
    "\n",
    "                        elif cls_method == \"xgboost\":\n",
    "                            cls = xgb.XGBClassifier(objective='binary:logistic',\n",
    "                                                    n_estimators=500,\n",
    "                                                    learning_rate= current_args['learning_rate'],\n",
    "                                                    subsample=current_args['subsample'],\n",
    "                                                    max_depth=int(current_args['max_depth']),\n",
    "                                                    colsample_bytree=current_args['colsample_bytree'],\n",
    "                                                    min_child_weight=int(current_args['min_child_weight']),\n",
    "                                                    seed=random_state)\n",
    "                        elif cls_method == \"cb\":\n",
    "                            cls = catboost.CatBoostClassifier(learning_rate=current_args['learning_rate'],\n",
    "                                                               subsample=current_args['subsample'], \n",
    "                                                              depth=current_args['depth'])\n",
    "                        #elif cls_method == \"lstm\":\n",
    "                        #    break\n",
    "\n",
    "                        elif cls_method == \"logit\":\n",
    "                            cls = LogisticRegression(C=2**current_args['C'],\n",
    "                                                     random_state=random_state)\n",
    "\n",
    "                        elif cls_method == \"svm\":\n",
    "                            cls = SVC(C=2**current_args['C'],\n",
    "                                      gamma=2**current_args['gamma'],\n",
    "                                      random_state=random_state)\n",
    "\n",
    "                        if cls_method == \"svm\" or cls_method == \"logit\":\n",
    "                            pipeline = Pipeline([('encoder', feature_combiner), ('scaler', StandardScaler()), ('cls', cls)])\n",
    "                        else:\n",
    "                            pipeline = Pipeline([('encoder', feature_combiner), ('cls', cls)])\n",
    "\n",
    "                        print(\"fitting pipeline...\")\n",
    "                        pipeline.fit(dt_train_bucket, train_y)\n",
    "\n",
    "                        offline_time_fit += time.time() - start_offline_time_fit\n",
    "\n",
    "                        # predict separately for each prefix case\n",
    "                        preds = []\n",
    "                        probas = []\n",
    "                        pred_types = []\n",
    "\n",
    "                        test_all_grouped = dt_test_bucket.groupby(dataset_manager.case_id_col)\n",
    "                        print(\"test data shape\", dt_test_bucket.shape)\n",
    "                        count_d=0 # count for deviant\n",
    "                        count_r=0 #count for regular\n",
    "\n",
    "                        tp_list = []\n",
    "                        tn_list = []\n",
    "                        fp_list = []\n",
    "                        fn_list = []\n",
    "\n",
    "                        iteration = 1\n",
    "\n",
    "                        for _, group in test_all_grouped:\n",
    "                            print (\"Testing instance\", iteration, 'of', len(test_all_grouped))\n",
    "                            test_y_group = dataset_manager.get_label_numeric(group)\n",
    "                            test_y_all.extend(test_y_group)\n",
    "\n",
    "                            start = time.time()\n",
    "                            _ = bucketer.predict(group)\n",
    "\n",
    "                            if cls_method == \"svm\":\n",
    "                                pred = pipeline.decision_function(group)\n",
    "                            else:\n",
    "                              preds_pos_label_idx = np.where(cls.classes_ == 1)[0][0]\n",
    "                              pred = pipeline.predict(group)[0]\n",
    "                              proba = pipeline.predict_proba(group)[:,preds_pos_label_idx]\n",
    "                            pipeline_pred_time = time.time() - start\n",
    "                            current_online_event_times.append(pipeline_pred_time / len(group))\n",
    "                            case_id = dataset_manager.get_case_ids(group)[0]\n",
    "\n",
    "                            if test_y_group == pred & pred == 0:\n",
    "                              pred_type = 'TN'\n",
    "\n",
    "                              instance_dict = {}\n",
    "                              instance_dict['caseID'] = case_id\n",
    "                              instance_dict['input'] = group\n",
    "                              instance_dict['actual'] = test_y_group[0]\n",
    "                              instance_dict['predicted'] = pred\n",
    "                              instance_dict['proba'] = 1 - proba[0]\n",
    "                              instance_dict['nr_events'] = dataset_manager.get_prefix_lengths(group)[0]\n",
    "                              instance_dict['pred_type'] = pred_type\n",
    "\n",
    "                              tn_list.append(instance_dict)\n",
    "\n",
    "                            elif test_y_group == pred & pred == 1:\n",
    "                              pred_type = 'TP'\n",
    "\n",
    "                              instance_dict = {}\n",
    "                              instance_dict['caseID'] = case_id\n",
    "                              instance_dict['input'] = group\n",
    "                              instance_dict['actual'] = test_y_group[0]\n",
    "                              instance_dict['predicted'] = pred\n",
    "                              instance_dict['proba'] = proba[0]\n",
    "                              instance_dict['nr_events'] = dataset_manager.get_prefix_lengths(group)[0]\n",
    "                              instance_dict['pred_type'] = pred_type\n",
    "\n",
    "                              tp_list.append(instance_dict)\n",
    "\n",
    "                            elif test_y_group != pred & pred == 0:\n",
    "                              pred_type = 'FN'\n",
    "\n",
    "                              instance_dict = {}\n",
    "                              instance_dict['caseID'] = case_id\n",
    "                              instance_dict['input'] = group\n",
    "                              instance_dict['actual'] = test_y_group[0]\n",
    "                              instance_dict['predicted'] = pred\n",
    "                              instance_dict['proba'] = 1 - proba[0]\n",
    "                              instance_dict['nr_events'] = dataset_manager.get_prefix_lengths(group)[0]\n",
    "                              instance_dict['pred_type'] = pred_type\n",
    "\n",
    "                              fn_list.append(instance_dict)\n",
    "\n",
    "                            elif test_y_group != pred & pred == 1:\n",
    "                              pred_type = 'FP'\n",
    "\n",
    "                              instance_dict = {}\n",
    "                              instance_dict['caseID'] = case_id\n",
    "                              instance_dict['input'] = group\n",
    "                              instance_dict['actual'] = test_y_group[0]\n",
    "                              instance_dict['predicted'] = pred\n",
    "                              instance_dict['proba'] = proba[0]\n",
    "                              instance_dict['nr_events'] = dataset_manager.get_prefix_lengths(group)[0]\n",
    "                              instance_dict['pred_type'] = pred_type\n",
    "\n",
    "                              fp_list.append(instance_dict)\n",
    "                            else:\n",
    "                              print(\"ERROR: Prediction doesn't fit into any category. Check code.\")\n",
    "\n",
    "                            preds.append(pred)\n",
    "                            probas.extend(proba)\n",
    "                            pred_types.append(pred_type)\n",
    "                            iteration+=1\n",
    "                        \n",
    "                    preds_all.extend(preds)\n",
    "                    probas_all.extend(probas)\n",
    "                    pred_types_all.extend(pred_types)\n",
    "\n",
    "                    tn_path = os.path.join(PATH, \"%s/%s_%s/instances/true_neg_bucket_%s_.pickle\" % (dataset_ref, cls_method, method_name, bucket))\n",
    "                    tp_path = os.path.join(PATH, \"%s/%s_%s/instances/true_pos_bucket_%s_.pickle\" % (dataset_ref, cls_method, method_name, bucket))\n",
    "                    fn_path = os.path.join(PATH, \"%s/%s_%s/instances/false_neg_bucket_%s_.pickle\" % (dataset_ref, cls_method, method_name, bucket))\n",
    "                    fp_path = os.path.join(PATH, \"%s/%s_%s/instances/false_pos_bucket_%s_.pickle\" % (dataset_ref, cls_method, method_name, bucket))\n",
    "                    \n",
    "                    with open (tn_path, 'wb') as f:\n",
    "                      pickle.dump(tn_list, f)\n",
    "                    with open (tp_path, 'wb') as f:\n",
    "                      pickle.dump(tp_list, f)\n",
    "                    with open (fn_path, 'wb') as f:\n",
    "                      pickle.dump(fn_list, f)\n",
    "                    with open (fp_path, 'wb') as f:\n",
    "                      pickle.dump(fp_list, f)\n",
    "\n",
    "            #Save models and encoders\n",
    "            print('Saving models...')\n",
    "            pipeline_path = os.path.join(PATH, \"%s/%s_%s/pipelines/pipeline_bucket_%s.joblib\" % (dataset_ref, cls_method, method_name, bucket))\n",
    "            feat_comb_path = os.path.join(PATH, \"%s/%s_%s/bucketers_and_encoders/feature_combiner_bucket_%s.joblib\" % (dataset_ref, cls_method, method_name, bucket))\n",
    "            bucketer_path = os.path.join(PATH, \"%s/%s_%s/bucketers_and_encoders/bucketer_bucket_%s.joblib\" % (dataset_ref, cls_method, method_name, bucket))\n",
    "            cls_path = os.path.join(PATH, \"%s/%s_%s/models/cls_bucket_%s.joblib\" % (dataset_ref, cls_method, method_name, bucket))\n",
    "            joblib.dump(pipeline, pipeline_path)\n",
    "            joblib.dump(feature_combiner, feat_comb_path)\n",
    "            joblib.dump(cls, cls_path)\n",
    "            joblib.dump(bucketer, bucketer_path)\n",
    "\n",
    "            #Save training data\n",
    "            print(\"saving data...\")\n",
    "            X_train_path = os.path.join(PATH, \"%s/%s_%s/train_data/bucket_%s_prefixes.pickle\" % (dataset_ref, cls_method, method_name, bucket))\n",
    "            Y_train_path = os.path.join(PATH, \"%s/%s_%s/train_data/bucket_%s_labels.pickle\" % (dataset_ref, cls_method, method_name, bucket))\n",
    "            with open(X_train_path, 'wb') as f:\n",
    "              pickle.dump(dt_train_bucket, f)\n",
    "            with open(Y_train_path, 'wb') as f:\n",
    "              pickle.dump(train_y, f)\n",
    "\n",
    "            #Save testing data\n",
    "            X_test_path = os.path.join(PATH, \"%s/%s_%s/test_data/bucket_%s_prefixes.pickle\" % (dataset_ref, cls_method, method_name, bucket))\n",
    "            Y_test_path = os.path.join(PATH, \"%s/%s_%s/test_data/bucket_%s_labels.pickle\" % (dataset_ref, cls_method, method_name, bucket))\n",
    "            with open(X_test_path, 'wb') as f:\n",
    "                pickle.dump(dt_test_bucket, f)\n",
    "            with open(Y_test_path, 'wb') as f:\n",
    "                pickle.dump(test_y_group, f)\n",
    "        \n",
    "        print(\"compiling results...\")\n",
    "        dt_results = pd.DataFrame({\"actual\": test_y_all, \"predicted\": preds_all, \"nr_events\": nr_events_all})\n",
    "        for nr_events, group in dt_results.groupby(\"nr_events\"):\n",
    "            if len(set(group.actual)) < 2:\n",
    "                print(dataset_name, method_name, cls_method, nr_events, \"auc\", np.nan)\n",
    "            else:\n",
    "                print(dataset_name, method_name, cls_method, nr_events, \"auc\", roc_auc_score(group.actual, group.predicted))\n",
    "        print(dataset_name, method_name, cls_method, -1, -1, \"auc\", roc_auc_score(dt_results.actual, dt_results.predicted))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DatasetManager import DatasetManager\n",
    "\n",
    "dataset_manager = DatasetManager(dataset_name)\n",
    "\n",
    "iteration = 1\n",
    "\n",
    "test_all_grouped = dt_test.groupby(dataset_manager.case_id_col)\n",
    "\n",
    "\n",
    "for _ , each in test_all_grouped:\n",
    "    print (\"Testing instance\", inst+1, 'of', len(test_all_grouped))\n",
    "    nr_length = each.shape[0]\n",
    "    group, y_test_group, case_ids = dataset_manager.generate_3d_data_for_single_case(each, max_len, nr_events = 1)\n",
    "    test_y_group = np.argmax(y_test_group)\n",
    "    test_y_all.append(test_y_group)\n",
    "\n",
    "    start = time.time()\n",
    "    case_id = case_ids[0]\n",
    "\n",
    "    proba = cls.predict(group)\n",
    "    pred = np.argmax(axis=-1)\n",
    "\n",
    "    if test_y_group == pred & pred == 0:\n",
    "      pred_type = 'TN'\n",
    "\n",
    "      instance_dict = {}\n",
    "      instance_dict['caseID'] = case_id\n",
    "      instance_dict['input'] = group\n",
    "      instance_dict['actual'] = test_y_group[0]\n",
    "      instance_dict['predicted'] = pred\n",
    "      instance_dict['proba'] = 1 - proba[0]\n",
    "      instance_dict['nr_events'] = nr_length\n",
    "      instance_dict['pred_type'] = pred_type\n",
    "\n",
    "      tn_list.append(instance_dict)\n",
    "\n",
    "    elif test_y_group == pred & pred == 1:\n",
    "      pred_type = 'TP'\n",
    "\n",
    "      instance_dict = {}\n",
    "      instance_dict['caseID'] = case_id\n",
    "      instance_dict['input'] = group\n",
    "      instance_dict['actual'] = test_y_group[0]\n",
    "      instance_dict['predicted'] = pred\n",
    "      instance_dict['proba'] = proba[0]\n",
    "      instance_dict['nr_events'] = nr_length\n",
    "      instance_dict['pred_type'] = pred_type\n",
    "\n",
    "      tp_list.append(instance_dict)\n",
    "\n",
    "    elif test_y_group != pred & pred == 0:\n",
    "      pred_type = 'FN'\n",
    "\n",
    "      instance_dict = {}\n",
    "      instance_dict['caseID'] = case_id\n",
    "      instance_dict['input'] = group\n",
    "      instance_dict['actual'] = test_y_group[0]\n",
    "      instance_dict['predicted'] = pred\n",
    "      instance_dict['proba'] = 1 - proba[0]\n",
    "      instance_dict['nr_events'] = nr_length\n",
    "      instance_dict['pred_type'] = pred_type\n",
    "\n",
    "      fn_list.append(instance_dict)\n",
    "\n",
    "    elif test_y_group != pred & pred == 1:\n",
    "      pred_type = 'FP'\n",
    "\n",
    "      instance_dict = {}\n",
    "      instance_dict['caseID'] = case_id\n",
    "      instance_dict['input'] = group\n",
    "      instance_dict['actual'] = test_y_group[0]\n",
    "      instance_dict['predicted'] = pred\n",
    "      instance_dict['proba'] = proba[0]\n",
    "      instance_dict['nr_events'] = nr_length\n",
    "      instance_dict['pred_type'] = pred_type\n",
    "\n",
    "      fp_list.append(instance_dict)\n",
    "    else:\n",
    "      print(\"ERROR: Prediction doesn't fit into any category. Check code.\")\n",
    "\n",
    "    preds.append(pred)\n",
    "    probas.extend(proba)\n",
    "    pred_types.append(pred_type)\n",
    "    iteration+=1\n",
    "\n",
    "preds_all.extend(preds)\n",
    "probas_all.extend(probas)\n",
    "pred_types_all.extend(pred_types)\n",
    "\n",
    "tn_path = os.path.join(PATH, \"%s/%s/instances/true_neg.pickle\" % (dataset_ref, cls_method))\n",
    "tp_path = os.path.join(PATH, \"%s/%s/instances/true_pos.pickle\" % (dataset_ref, cls_method))\n",
    "fn_path = os.path.join(PATH, \"%s/%s/instances/false_neg.pickle\" % (dataset_ref, cls_method))\n",
    "fp_path = os.path.join(PATH, \"%s/%s/instances/false_pos.pickle\" % (dataset_ref, cls_method))\n",
    "\n",
    "with open (tn_path, 'wb') as f:\n",
    "  pickle.dump(tn_list, f)\n",
    "with open (tp_path, 'wb') as f:\n",
    "  pickle.dump(tp_list, f)\n",
    "with open (fn_path, 'wb') as f:\n",
    "  pickle.dump(fn_list, f)\n",
    "with open (fp_path, 'wb') as f:\n",
    "  pickle.dump(fp_list, f)\n",
    "\n",
    "#Save training data\n",
    "print(\"saving data...\")\n",
    "X_train_path = os.path.join(PATH, \"%s/%s/train_data/prefixes.pickle\" % (dataset_ref, cls_method))\n",
    "Y_train_path = os.path.join(PATH, \"%s/%s/train_data/labels.pickle\" % (dataset_ref, cls_method))\n",
    "with open(X_train_path, 'wb') as f:\n",
    "  pickle.dump(dt_train_bucket, f)\n",
    "with open(Y_train_path, 'wb') as f:\n",
    "  pickle.dump(train_y, f)\n",
    "\n",
    "#Save testing data\n",
    "X_test_path = os.path.join(PATH, \"%s/%s/test_data/prefixes.pickle\" % (dataset_ref, cls_method))\n",
    "Y_test_path = os.path.join(PATH, \"%s/%s/test_data/labels.pickle\" % (dataset_ref, cls_method))\n",
    "with open(X_test_path, 'wb') as f:\n",
    "    pickle.dump(dt_test_bucket, f)\n",
    "with open(Y_test_path, 'wb') as f:\n",
    "    pickle.dump(y_test, f)\n",
    "    \n",
    "    \n",
    "acc = accuracy_score(test_y, preds)\n",
    "roc_auc = roc_auc_score(test_y, preds)\n",
    "print(\"Accuracy:\", auc, \"\\tROCAUC:\", roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls.predict(group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group.shape"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "bpic2012_stability_generate_data.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
