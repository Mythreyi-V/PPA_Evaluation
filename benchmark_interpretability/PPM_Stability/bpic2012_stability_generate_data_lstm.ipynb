{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 961,
     "status": "ok",
     "timestamp": 1597794848189,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh6DP4Hn-qpp593Bc6lrayXXcdQv42KAS3zJ-Ay=s64",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "TNrzy43BSk0S",
    "outputId": "b78bdb37-75ad-4e66-e3b2-e435bcdffd78"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "import sys\n",
    "#PATH = '/content/drive/My Drive/PPM_Stability/'\n",
    "#PATH = \"C:/Users/velmurug/Documents/Stability Experiments/benchmark_interpretability/PPM_Stability/\"\n",
    "#PATH = \"C:/Users/mythr/Documents/GitHub/Stability-Experiments/benchmark_interpretability/PPM_Stability/\"\n",
    "PATH = \"C:/Users/n9455647/Documents/GitHub/Stability-Experiments/benchmark_interpretability/PPM_Stability/\"\n",
    "sys.path.append(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 171
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9630,
     "status": "ok",
     "timestamp": 1597794856873,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh6DP4Hn-qpp593Bc6lrayXXcdQv42KAS3zJ-Ay=s64",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "OH7t3uEIU6e0",
    "outputId": "b8659c9d-4c9f-4c5c-d928-7cfe21348260"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost==1.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (1.0.0)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (from xgboost==1.0.0) (1.5.2)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from xgboost==1.0.0) (1.19.1)\n"
     ]
    }
   ],
   "source": [
    "#!pip install pandas==0.22.0\n",
    "!pip install xgboost==1.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11828,
     "status": "ok",
     "timestamp": 1597794859084,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh6DP4Hn-qpp593Bc6lrayXXcdQv42KAS3zJ-Ay=s64",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "5sny80CkSAnW"
   },
   "outputs": [],
   "source": [
    "import EncoderFactory\n",
    "from DatasetManager import DatasetManager\n",
    "import BucketFactory\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "from sys import argv\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "import catboost\n",
    "\n",
    "from tensorflow.keras.backend import print_tensor\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Embedding, Flatten, Input\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import Nadam, RMSprop\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "#import lime\n",
    "#import lime.lime_tabular\n",
    "#from lime import submodular_pick;\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11818,
     "status": "ok",
     "timestamp": 1597794859089,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh6DP4Hn-qpp593Bc6lrayXXcdQv42KAS3zJ-Ay=s64",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "g32E_o5LSAne",
    "outputId": "e9c23b91-ebd4-4db7-d2b2-f015fea397fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bpic2012_accepted']\n"
     ]
    }
   ],
   "source": [
    "dataset_ref = \"bpic2012\"\n",
    "params_dir = PATH + \"params/\"\n",
    "results_dir = \"results\"\n",
    "bucket_method = \"single\"\n",
    "cls_encoding = \"agg\"\n",
    "cls_method = \"xgboost\"\n",
    "gap = 1\n",
    "n_iter = 1\n",
    "\n",
    "if bucket_method == \"state\":\n",
    "    bucket_encoding = \"last\"\n",
    "else:\n",
    "    bucket_encoding = \"agg\"\n",
    "\n",
    "method_name = \"%s_%s\"%(bucket_method, cls_encoding)\n",
    "\n",
    "dataset_ref_to_datasets = {\n",
    "    #\"bpic2011\": [\"bpic2011_f%s\"%formula for formula in range(1,5)],\n",
    "    \"bpic2015\": [\"bpic2015_%s_f2\"%(municipality) for municipality in range(5,6)],\n",
    "    \"bpic2017\" : [\"bpic2017_accepted\"],\n",
    "    \"bpic2012\" : [\"bpic2012_accepted\"]\n",
    "    #\"insurance\": [\"insurance_activity\", \"insurance_followup\"],\n",
    "    #\"sepsis_cases\": [\"sepsis_cases_1\", \"sepsis_cases_2\", \"sepsis_cases_4\"]\n",
    "}\n",
    "\n",
    "encoding_dict = {\n",
    "    \"laststate\": [\"static\", \"last\"],\n",
    "    \"agg\": [\"static\", \"agg\"],\n",
    "    \"index\": [\"static\", \"index\"],\n",
    "    \"combined\": [\"static\", \"last\", \"agg\"]\n",
    "}\n",
    "\n",
    "datasets = [dataset_ref] if dataset_ref not in dataset_ref_to_datasets else dataset_ref_to_datasets[dataset_ref]\n",
    "methods = encoding_dict[cls_encoding]\n",
    "    \n",
    "train_ratio = 0.8\n",
    "random_state = 22\n",
    "\n",
    "# create results directory\n",
    "if not os.path.exists(os.path.join(params_dir)):\n",
    "    os.makedirs(os.path.join(params_dir))\n",
    "    \n",
    "print(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O5tFTI_cSAnn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up parameters...\n",
      "{'colsample_bytree': 0.5353160434296517, 'learning_rate': 0.05959997729599409, 'max_depth': 22, 'min_child_weight': 3, 'subsample': 0.8342285287391545}\n",
      "setting up data...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'labeled_logs_csv_processed\\\\bpic2012_O_ACCEPTED-COMPLETE.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-b1996eee960f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"setting up data...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mdataset_manager\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDatasetManager\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset_manager\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m     \u001b[1;31m#print('Case ID column', dataset_manager.case_id_col)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     cls_encoder_args = {'case_id_col': dataset_manager.case_id_col, \n",
      "\u001b[1;32m~\\Documents\\Github\\Stability-Experiments\\benchmark_interpretability\\PPM_Stability\\DatasetManager.py\u001b[0m in \u001b[0;36mread_dataset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     38\u001b[0m             \u001b[0mdtypes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"float\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_confs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\";\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m         \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimestamp_col\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimestamp_col\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    684\u001b[0m     )\n\u001b[0;32m    685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 452\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    934\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    935\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 936\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    937\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    938\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1166\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1167\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1168\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1169\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1170\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1996\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1997\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1998\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1999\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2000\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'labeled_logs_csv_processed\\\\bpic2012_O_ACCEPTED-COMPLETE.csv'"
     ]
    }
   ],
   "source": [
    "for dataset_name in datasets:\n",
    "    \n",
    "    # load optimal params\n",
    "    print(\"Setting up parameters...\")\n",
    "    optimal_params_filename = os.path.join(params_dir, \"optimal_params_%s_%s_%s.pickle\" % (cls_method, dataset_name, method_name))\n",
    "\n",
    "    if not os.path.isfile(optimal_params_filename) or os.path.getsize(optimal_params_filename) <= 0:\n",
    "        print(\"Parameters not found\")\n",
    "        \n",
    "    with open(optimal_params_filename, \"rb\") as fin:\n",
    "        args = pickle.load(fin)\n",
    "        \n",
    "    #print(args)\n",
    "            \n",
    "    # read the data\n",
    "    print(\"setting up data...\")\n",
    "    dataset_manager = DatasetManager(dataset_name)\n",
    "    data = dataset_manager.read_dataset()\n",
    "    #print('Case ID column', dataset_manager.case_id_col)\n",
    "    cls_encoder_args = {'case_id_col': dataset_manager.case_id_col, \n",
    "                        'static_cat_cols': dataset_manager.static_cat_cols,\n",
    "                        'static_num_cols': dataset_manager.static_num_cols, \n",
    "                        'dynamic_cat_cols': dataset_manager.dynamic_cat_cols,\n",
    "                        'dynamic_num_cols': dataset_manager.dynamic_num_cols, \n",
    "                        'fillna': True}\n",
    "\n",
    "    # determine min and max (truncated) prefix lengths\n",
    "    min_prefix_length = 1\n",
    "    if \"traffic_fines\" in dataset_name:\n",
    "        max_prefix_length = 10\n",
    "    elif \"bpic2017\" in dataset_name:\n",
    "        max_prefix_length = min(20, dataset_manager.get_pos_case_length_quantile(data, 0.90))\n",
    "    else:\n",
    "        max_prefix_length = min(40, dataset_manager.get_pos_case_length_quantile(data, 0.90))\n",
    "\n",
    "    # split into training and test\n",
    "    train, test = dataset_manager.split_data_strict(data, train_ratio, split=\"temporal\")\n",
    "    \n",
    "    if gap > 1:\n",
    "        outfile = os.path.join(results_dir, \"performance_results_%s_%s_%s_gap%s.csv\" % (cls_method, dataset_name, method_name, gap))\n",
    "    else:\n",
    "        outfile = os.path.join(results_dir, \"performance_results_%s_%s_%s.csv\" % (cls_method, dataset_name, method_name))\n",
    "        \n",
    "    start_test_prefix_generation = time.time()\n",
    "    dt_test_prefixes = dataset_manager.generate_prefix_data(test, min_prefix_length, max_prefix_length)\n",
    "    test_prefix_generation_time = time.time() - start_test_prefix_generation\n",
    "            \n",
    "    offline_total_times = []\n",
    "    online_event_times = []\n",
    "    train_prefix_generation_times = []\n",
    "\n",
    "    for ii in range(n_iter):\n",
    "        # create prefix logs\n",
    "        print(\"Creating logs...\")\n",
    "        start_train_prefix_generation = time.time()\n",
    "        dt_train_prefixes = dataset_manager.generate_prefix_data(train, min_prefix_length, max_prefix_length, gap)\n",
    "        train_prefix_generation_time = time.time() - start_train_prefix_generation\n",
    "        train_prefix_generation_times.append(train_prefix_generation_time)\n",
    "            \n",
    "        # Bucketing prefixes based on control flow\n",
    "        print(\"bucketing prefixes...\")\n",
    "        bucketer_args = {'encoding_method':bucket_encoding, \n",
    "                         'case_id_col':dataset_manager.case_id_col, \n",
    "                         'cat_cols':[dataset_manager.activity_col], \n",
    "                         'num_cols':[], \n",
    "                         'random_state':random_state}\n",
    "        if bucket_method == \"cluster\":\n",
    "            bucketer_args[\"n_clusters\"] = int(args[\"n_clusters\"])\n",
    "        bucketer = BucketFactory.get_bucketer(bucket_method, **bucketer_args)\n",
    "\n",
    "        start_offline_time_bucket = time.time()\n",
    "        bucket_assignments_train = bucketer.fit_predict(dt_train_prefixes)\n",
    "        offline_time_bucket = time.time() - start_offline_time_bucket\n",
    "\n",
    "        bucket_assignments_test = bucketer.predict(dt_test_prefixes)\n",
    "\n",
    "        preds_all = []\n",
    "        probas_all = []\n",
    "        pred_types_all = []\n",
    "        test_y_all = []\n",
    "        nr_events_all = []\n",
    "        offline_time_fit = 0\n",
    "        current_online_event_times = []\n",
    "        \n",
    "        for bucket in set(bucket_assignments_test):\n",
    "            print(\"Bucket\" , bucket )\n",
    "            print(\"sorting bucket...\")\n",
    "            if bucket_method == \"prefix\":\n",
    "                current_args = args[bucket]\n",
    "            else:\n",
    "                current_args = args\n",
    "            relevant_train_cases_bucket = dataset_manager.get_indexes(dt_train_prefixes)[bucket_assignments_train == bucket]\n",
    "            relevant_test_cases_bucket = dataset_manager.get_indexes(dt_test_prefixes)[bucket_assignments_test == bucket]\n",
    "            dt_test_bucket = dataset_manager.get_relevant_data_by_indexes(dt_test_prefixes, relevant_test_cases_bucket)\n",
    "            \n",
    "            nr_events_all.extend(list(dataset_manager.get_prefix_lengths(dt_test_bucket)))\n",
    "            print('number events', len(nr_events_all))\n",
    "            \n",
    "            if len(relevant_train_cases_bucket) == 0:\n",
    "                preds = [dataset_manager.get_class_ratio(train)] * len(relevant_test_cases_bucket)\n",
    "                current_online_event_times.extend([0] * len(preds))\n",
    "            else:\n",
    "                dt_train_bucket = dataset_manager.get_relevant_data_by_indexes(dt_train_prefixes, relevant_train_cases_bucket) # one row per event\n",
    "                train_y = dataset_manager.get_label_numeric(dt_train_bucket)\n",
    "\n",
    "                if len(set(train_y)) < 2:\n",
    "                    preds = [train_y[0]] * len(relevant_test_cases_bucket)\n",
    "                    current_online_event_times.extend([0] * len(preds))\n",
    "                    test_y_all.extend(dataset_manager.get_label_numeric(dt_test_bucket))\n",
    "                else:\n",
    "                    print(\"choosing classifier...\")\n",
    "                    start_offline_time_fit = time.time()\n",
    "                    feature_combiner = FeatureUnion([(method, EncoderFactory.get_encoder(method, **cls_encoder_args)) for method in methods])\n",
    "\n",
    "                    if cls_method == \"rf\":\n",
    "                        cls = RandomForestClassifier(n_estimators=500,\n",
    "                                                     max_features=current_args['max_features'],\n",
    "                                                     random_state=random_state)\n",
    "\n",
    "                    elif cls_method == \"xgboost\":\n",
    "                        cls = xgb.XGBClassifier(objective='binary:logistic',\n",
    "                                                n_estimators=500,\n",
    "                                                learning_rate= current_args['learning_rate'],\n",
    "                                                subsample=current_args['subsample'],\n",
    "                                                max_depth=int(current_args['max_depth']),\n",
    "                                                colsample_bytree=current_args['colsample_bytree'],\n",
    "                                                min_child_weight=int(current_args['min_child_weight']),\n",
    "                                                seed=random_state)\n",
    "                    elif cls_method == \"cb\":\n",
    "                        cls = catboost.CatBoostClassifier(learning_rate=current_args['learning_rate'],\n",
    "                                                           subsample=current_args['subsample'], \n",
    "                                                          depth=current_args['depth'])\n",
    "                    elif cls_method == \"lstm\":\n",
    "                        \n",
    "\n",
    "                    elif cls_method == \"logit\":\n",
    "                        cls = LogisticRegression(C=2**current_args['C'],\n",
    "                                                 random_state=random_state)\n",
    "\n",
    "                    elif cls_method == \"svm\":\n",
    "                        cls = SVC(C=2**current_args['C'],\n",
    "                                  gamma=2**current_args['gamma'],\n",
    "                                  random_state=random_state)\n",
    "\n",
    "                    if cls_method == \"svm\" or cls_method == \"logit\":\n",
    "                        pipeline = Pipeline([('encoder', feature_combiner), ('scaler', StandardScaler()), ('cls', cls)])\n",
    "                    else:\n",
    "                        pipeline = Pipeline([('encoder', feature_combiner), ('cls', cls)])\n",
    "\n",
    "                    print(\"fitting pipeline...\")\n",
    "                    pipeline.fit(dt_train_bucket, train_y)\n",
    "                    \n",
    "                    offline_time_fit += time.time() - start_offline_time_fit\n",
    "\n",
    "                    # predict separately for each prefix case\n",
    "                    preds = []\n",
    "                    probas = []\n",
    "                    pred_types = []\n",
    "                    \n",
    "                    test_all_grouped = dt_test_bucket.groupby(dataset_manager.case_id_col)\n",
    "                    print(\"test data shape\", dt_test_bucket.shape)\n",
    "                    count_d=0 # count for deviant\n",
    "                    count_r=0 #count for regular\n",
    "\n",
    "                    tp_list = []\n",
    "                    tn_list = []\n",
    "                    fp_list = []\n",
    "                    fn_list = []\n",
    "\n",
    "                    iteration = 1\n",
    "                    \n",
    "                    for _, group in test_all_grouped:\n",
    "                        print (\"Testing instance\", iteration, 'of', len(test_all_grouped))\n",
    "                        test_y_group = dataset_manager.get_label_numeric(group)\n",
    "                        test_y_all.extend(test_y_group)\n",
    "                            \n",
    "                        start = time.time()\n",
    "                        _ = bucketer.predict(group)\n",
    "                        \n",
    "                        if cls_method == \"svm\":\n",
    "                            pred = pipeline.decision_function(group)\n",
    "                        else:\n",
    "                          preds_pos_label_idx = np.where(cls.classes_ == 1)[0][0]\n",
    "                          pred = pipeline.predict(group)[0]\n",
    "                          proba = pipeline.predict_proba(group)[:,preds_pos_label_idx]\n",
    "                        pipeline_pred_time = time.time() - start\n",
    "                        current_online_event_times.append(pipeline_pred_time / len(group))\n",
    "                        case_id = dataset_manager.get_case_ids(group)[0]\n",
    "                        \n",
    "                        if test_y_group == pred & pred == 0:\n",
    "                          pred_type = 'TN'\n",
    "\n",
    "                          instance_dict = {}\n",
    "                          instance_dict['caseID'] = case_id\n",
    "                          instance_dict['input'] = group\n",
    "                          instance_dict['actual'] = test_y_group[0]\n",
    "                          instance_dict['predicted'] = pred\n",
    "                          instance_dict['proba'] = 1 - proba[0]\n",
    "                          instance_dict['nr_events'] = dataset_manager.get_prefix_lengths(group)[0]\n",
    "                          instance_dict['pred_type'] = pred_type\n",
    "\n",
    "                          tn_list.append(instance_dict)\n",
    "\n",
    "                        elif test_y_group == pred & pred == 1:\n",
    "                          pred_type = 'TP'\n",
    "                                            \n",
    "                          instance_dict = {}\n",
    "                          instance_dict['caseID'] = case_id\n",
    "                          instance_dict['input'] = group\n",
    "                          instance_dict['actual'] = test_y_group[0]\n",
    "                          instance_dict['predicted'] = pred\n",
    "                          instance_dict['proba'] = proba[0]\n",
    "                          instance_dict['nr_events'] = dataset_manager.get_prefix_lengths(group)[0]\n",
    "                          instance_dict['pred_type'] = pred_type\n",
    "\n",
    "                          tp_list.append(instance_dict)\n",
    "\n",
    "                        elif test_y_group != pred & pred == 0:\n",
    "                          pred_type = 'FN'\n",
    "                          \n",
    "                          instance_dict = {}\n",
    "                          instance_dict['caseID'] = case_id\n",
    "                          instance_dict['input'] = group\n",
    "                          instance_dict['actual'] = test_y_group[0]\n",
    "                          instance_dict['predicted'] = pred\n",
    "                          instance_dict['proba'] = 1 - proba[0]\n",
    "                          instance_dict['nr_events'] = dataset_manager.get_prefix_lengths(group)[0]\n",
    "                          instance_dict['pred_type'] = pred_type\n",
    "\n",
    "                          fn_list.append(instance_dict)\n",
    "\n",
    "                        elif test_y_group != pred & pred == 1:\n",
    "                          pred_type = 'FP'\n",
    "                          \n",
    "                          instance_dict = {}\n",
    "                          instance_dict['caseID'] = case_id\n",
    "                          instance_dict['input'] = group\n",
    "                          instance_dict['actual'] = test_y_group[0]\n",
    "                          instance_dict['predicted'] = pred\n",
    "                          instance_dict['proba'] = proba[0]\n",
    "                          instance_dict['nr_events'] = dataset_manager.get_prefix_lengths(group)[0]\n",
    "                          instance_dict['pred_type'] = pred_type\n",
    "\n",
    "                          fp_list.append(instance_dict)\n",
    "                        else:\n",
    "                          print(\"ERROR: Prediction doesn't fit into any category. Check code.\")\n",
    "                              \n",
    "                        preds.append(pred)\n",
    "                        probas.extend(proba)\n",
    "                        pred_types.append(pred_type)\n",
    "                        iteration+=1\n",
    "                        \n",
    "                    preds_all.extend(preds)\n",
    "                    probas_all.extend(probas)\n",
    "                    pred_types_all.extend(pred_types)\n",
    "\n",
    "                    tn_path = os.path.join(PATH, \"%s/%s_%s/instances/true_neg_bucket_%s_.pickle\" % (dataset_ref, cls_method, method_name, bucket))\n",
    "                    tp_path = os.path.join(PATH, \"%s/%s_%s/instances/true_pos_bucket_%s_.pickle\" % (dataset_ref, cls_method, method_name, bucket))\n",
    "                    fn_path = os.path.join(PATH, \"%s/%s_%s/instances/false_neg_bucket_%s_.pickle\" % (dataset_ref, cls_method, method_name, bucket))\n",
    "                    fp_path = os.path.join(PATH, \"%s/%s_%s/instances/false_pos_bucket_%s_.pickle\" % (dataset_ref, cls_method, method_name, bucket))\n",
    "                    \n",
    "                    with open (tn_path, 'wb') as f:\n",
    "                      pickle.dump(tn_list, f)\n",
    "                    with open (tp_path, 'wb') as f:\n",
    "                      pickle.dump(tp_list, f)\n",
    "                    with open (fn_path, 'wb') as f:\n",
    "                      pickle.dump(fn_list, f)\n",
    "                    with open (fp_path, 'wb') as f:\n",
    "                      pickle.dump(fp_list, f)\n",
    "\n",
    "            #Save models and encoders\n",
    "            print('Saving models...')\n",
    "            pipeline_path = os.path.join(PATH, \"%s/%s_%s/pipelines/pipeline_bucket_%s.joblib\" % (dataset_ref, cls_method, method_name, bucket))\n",
    "            feat_comb_path = os.path.join(PATH, \"%s/%s_%s/bucketers_and_encoders/feature_combiner_bucket_%s.joblib\" % (dataset_ref, cls_method, method_name, bucket))\n",
    "            bucketer_path = os.path.join(PATH, \"%s/%s_%s/bucketers_and_encoders/bucketer_bucket_%s.joblib\" % (dataset_ref, cls_method, method_name, bucket))\n",
    "            cls_path = os.path.join(PATH, \"%s/%s_%s/models/cls_bucket_%s.joblib\" % (dataset_ref, cls_method, method_name, bucket))\n",
    "            joblib.dump(pipeline, pipeline_path)\n",
    "            joblib.dump(feature_combiner, feat_comb_path)\n",
    "            joblib.dump(cls, cls_path)\n",
    "            joblib.dump(bucketer, bucketer_path)\n",
    "\n",
    "            #Save training data\n",
    "            print(\"saving data...\")\n",
    "            X_train_path = os.path.join(PATH, \"%s/%s_%s/train_data/bucket_%s_prefixes.pickle\" % (dataset_ref, cls_method, method_name, bucket))\n",
    "            Y_train_path = os.path.join(PATH, \"%s/%s_%s/train_data/bucket_%s_labels.pickle\" % (dataset_ref, cls_method, method_name, bucket))\n",
    "            with open(X_train_path, 'wb') as f:\n",
    "              pickle.dump(dt_train_bucket, f)\n",
    "            with open(Y_train_path, 'wb') as f:\n",
    "              pickle.dump(train_y, f)\n",
    "\n",
    "            #Save testing data\n",
    "            X_test_path = os.path.join(PATH, \"%s/%s_%s/test_data/bucket_%s_prefixes.pickle\" % (dataset_ref, cls_method, method_name, bucket))\n",
    "            Y_test_path = os.path.join(PATH, \"%s/%s_%s/test_data/bucket_%s_labels.pickle\" % (dataset_ref, cls_method, method_name, bucket))\n",
    "            with open(X_test_path, 'wb') as f:\n",
    "                pickle.dump(dt_test_bucket, f)\n",
    "            with open(Y_test_path, 'wb') as f:\n",
    "                pickle.dump(test_y_group, f)\n",
    "        \n",
    "        print(\"compiling results...\")\n",
    "        dt_results = pd.DataFrame({\"actual\": test_y_all, \"predicted\": preds_all, \"nr_events\": nr_events_all})\n",
    "        for nr_events, group in dt_results.groupby(\"nr_events\"):\n",
    "            if len(set(group.actual)) < 2:\n",
    "                print(dataset_name, method_name, cls_method, nr_events, \"auc\", np.nan)\n",
    "            else:\n",
    "                print(dataset_name, method_name, cls_method, nr_events, \"auc\", roc_auc_score(group.actual, group.predicted))\n",
    "        print(dataset_name, method_name, cls_method, -1, -1, \"auc\", roc_auc_score(dt_results.actual, dt_results.predicted))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "bpic2012_stability_generate_data.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
