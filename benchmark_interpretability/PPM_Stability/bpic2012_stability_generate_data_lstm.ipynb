{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 961,
     "status": "ok",
     "timestamp": 1597794848189,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh6DP4Hn-qpp593Bc6lrayXXcdQv42KAS3zJ-Ay=s64",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "TNrzy43BSk0S",
    "outputId": "b78bdb37-75ad-4e66-e3b2-e435bcdffd78"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "import sys\n",
    "#PATH = '/content/drive/My Drive/PPM_Stability/'\n",
    "PATH = \"C:/Users/velmurug/Documents/Stability Experiments/benchmark_interpretability/PPM_Stability/\"\n",
    "#PATH = \"C:/Users/mythr/Documents/GitHub/Stability-Experiments/benchmark_interpretability/PPM_Stability/\"\n",
    "#PATH = \"C:/Users/n9455647/Documents/GitHub/Stability-Experiments/benchmark_interpretability/PPM_Stability/\"\n",
    "sys.path.append(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 171
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9630,
     "status": "ok",
     "timestamp": 1597794856873,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh6DP4Hn-qpp593Bc6lrayXXcdQv42KAS3zJ-Ay=s64",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "OH7t3uEIU6e0",
    "outputId": "b8659c9d-4c9f-4c5c-d928-7cfe21348260"
   },
   "outputs": [],
   "source": [
    "#!pip install pandas==0.22.0\n",
    "#!pip install xgboost==1.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11828,
     "status": "ok",
     "timestamp": 1597794859084,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh6DP4Hn-qpp593Bc6lrayXXcdQv42KAS3zJ-Ay=s64",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "5sny80CkSAnW"
   },
   "outputs": [],
   "source": [
    "import EncoderFactory\n",
    "from DatasetManager import DatasetManager\n",
    "import BucketFactory\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "from sys import argv\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "import catboost\n",
    "\n",
    "from tensorflow.keras.backend import print_tensor\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Dense, Embedding, Flatten, Input\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import Nadam, RMSprop\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "#import lime\n",
    "#import lime.lime_tabular\n",
    "#from lime import submodular_pick;\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11818,
     "status": "ok",
     "timestamp": 1597794859089,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh6DP4Hn-qpp593Bc6lrayXXcdQv42KAS3zJ-Ay=s64",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "g32E_o5LSAne",
    "outputId": "e9c23b91-ebd4-4db7-d2b2-f015fea397fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bpic2012_accepted']\n"
     ]
    }
   ],
   "source": [
    "dataset_ref = \"bpic2012\"\n",
    "params_dir = PATH + \"params/\"\n",
    "results_dir = \"results\"\n",
    "bucket_method = \"single\"\n",
    "cls_encoding = \"3d\"\n",
    "cls_method = \"lstm\"\n",
    "gap = 1\n",
    "n_iter = 1\n",
    "\n",
    "if bucket_method == \"state\":\n",
    "    bucket_encoding = \"last\"\n",
    "else:\n",
    "    bucket_encoding = \"agg\"\n",
    "\n",
    "method_name = \"%s_%s\"%(bucket_method, cls_encoding)\n",
    "\n",
    "dataset_ref_to_datasets = {\n",
    "    #\"bpic2011\": [\"bpic2011_f%s\"%formula for formula in range(1,5)],\n",
    "    \"bpic2015\": [\"bpic2015_%s_f2\"%(municipality) for municipality in range(5,6)],\n",
    "    \"bpic2017\" : [\"bpic2017_accepted\"],\n",
    "    \"bpic2012\" : [\"bpic2012_accepted\"]\n",
    "    #\"insurance\": [\"insurance_activity\", \"insurance_followup\"],\n",
    "    #\"sepsis_cases\": [\"sepsis_cases_1\", \"sepsis_cases_2\", \"sepsis_cases_4\"]\n",
    "}\n",
    "\n",
    "encoding_dict = {\n",
    "    \"laststate\": [\"static\", \"last\"],\n",
    "    \"agg\": [\"static\", \"agg\"],\n",
    "    \"index\": [\"static\", \"index\"],\n",
    "    \"combined\": [\"static\", \"last\", \"agg\"],\n",
    "    \"3d\":[]\n",
    "}\n",
    "\n",
    "datasets = [dataset_ref] if dataset_ref not in dataset_ref_to_datasets else dataset_ref_to_datasets[dataset_ref]\n",
    "methods = encoding_dict[cls_encoding]\n",
    "    \n",
    "train_ratio = 0.8\n",
    "random_state = 22\n",
    "\n",
    "# create results directory\n",
    "if not os.path.exists(os.path.join(params_dir)):\n",
    "    os.makedirs(os.path.join(params_dir))\n",
    "    \n",
    "print(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lstm_layers': {'lstm2_dropouts': 0.18,\n",
       "  'lstm3_dropouts': 0.18,\n",
       "  'lstm2_nodes': 30,\n",
       "  'lstm3_nodes': 30,\n",
       "  'layers': 'three'},\n",
       " 'dense_layers': {'layers': 'one', 'dense2_nodes': 8},\n",
       " 'learning_rate': 3e-05,\n",
       " 'lstm1_nodes': 30,\n",
       " 'batch_size': 8,\n",
       " 'lstm1_dropouts': 0.18,\n",
       " 'optimizer': 'adam',\n",
       " 'epochs': 100}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = {'lstm_layers':{}}\n",
    "args['dense_layers'] = {'layers': 'one'}\n",
    "args['dense_layers']['dense2_nodes'] = 8\n",
    "args['learning_rate'] = 0.00003\n",
    "args['lstm_layers']['lstm2_dropouts'] = 0.18\n",
    "args['lstm_layers']['lstm3_dropouts'] = 0.18\n",
    "args['lstm1_nodes'] = 30\n",
    "args['lstm_layers']['lstm2_nodes'] = 30\n",
    "args['lstm_layers']['lstm3_nodes'] = 30\n",
    "args['batch_size'] = 8\n",
    "args['lstm1_dropouts'] = 0.18\n",
    "args['lstm_layers']['layers'] = 'three'\n",
    "args['optimizer'] = 'adam'\n",
    "args['epochs'] = 100\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up parameters...\n",
      "{'batch_size': 1, 'dense_layers': {'dense2_nodes': 8, 'layers': 'two'}, 'epochs': 100, 'learning_rate': 0.00011586868216910904, 'lstm1_dropouts': 0.012852567994582877, 'lstm1_nodes': 50, 'lstm_layers': {'layers': 'two', 'lstm2_dropouts': 0.15970018052469231, 'lstm2_nodes': 50}, 'optimizer': 'adam'}\n",
      "setting up data...\n",
      "102\n",
      "(93128, 40, 102)\n",
      "(18714, 40, 102)\n",
      "(30637, 40, 102)\n",
      "Epoch 1/100\n",
      "93127/93128 [============================>.] - ETA: 0s - loss: 0.6931\n",
      "Epoch 00001: saving model to C:/Users/velmurug/Documents/Stability Experiments/benchmark_interpretability/PPM_Stability/bpic2012/lstm_single_3d/cls\\checkpoint.cpt\n",
      "93128/93128 [==============================] - 1763s 19ms/step - loss: 0.6931 - val_loss: 2.2438\n",
      "Epoch 2/100\n",
      "93128/93128 [==============================] - ETA: 0s - loss: 0.6932- ETA: 0s - lo\n",
      "Epoch 00002: saving model to C:/Users/velmurug/Documents/Stability Experiments/benchmark_interpretability/PPM_Stability/bpic2012/lstm_single_3d/cls\\checkpoint.cpt\n",
      "93128/93128 [==============================] - 1841s 20ms/step - loss: 0.6932 - val_loss: 2.2299\n",
      "Epoch 3/100\n",
      "93128/93128 [==============================] - ETA: 0s - loss: 0.6932\n",
      "Epoch 00003: saving model to C:/Users/velmurug/Documents/Stability Experiments/benchmark_interpretability/PPM_Stability/bpic2012/lstm_single_3d/cls\\checkpoint.cpt\n",
      "93128/93128 [==============================] - 1901s 20ms/step - loss: 0.6932 - val_loss: 2.2205\n",
      "Epoch 4/100\n",
      "93128/93128 [==============================] - ETA: 0s - loss: 0.6932\n",
      "Epoch 00004: saving model to C:/Users/velmurug/Documents/Stability Experiments/benchmark_interpretability/PPM_Stability/bpic2012/lstm_single_3d/cls\\checkpoint.cpt\n",
      "93128/93128 [==============================] - 1896s 20ms/step - loss: 0.6932 - val_loss: 2.1329\n",
      "Epoch 5/100\n",
      "93126/93128 [============================>.] - ETA: 0s - loss: 0.6932- ETA: 0s\n",
      "Epoch 00005: saving model to C:/Users/velmurug/Documents/Stability Experiments/benchmark_interpretability/PPM_Stability/bpic2012/lstm_single_3d/cls\\checkpoint.cpt\n",
      "93128/93128 [==============================] - 1917s 21ms/step - loss: 0.6932 - val_loss: 2.3010\n",
      "Epoch 6/100\n",
      "93128/93128 [==============================] - ETA: 0s - loss: 0.6932-\n",
      "Epoch 00006: saving model to C:/Users/velmurug/Documents/Stability Experiments/benchmark_interpretability/PPM_Stability/bpic2012/lstm_single_3d/cls\\checkpoint.cpt\n",
      "93128/93128 [==============================] - 1936s 21ms/step - loss: 0.6932 - val_loss: 2.1532\n",
      "Epoch 7/100\n",
      "93127/93128 [============================>.] - ETA: 0s - loss: 0.6931\n",
      "Epoch 00007: saving model to C:/Users/velmurug/Documents/Stability Experiments/benchmark_interpretability/PPM_Stability/bpic2012/lstm_single_3d/cls\\checkpoint.cpt\n",
      "93128/93128 [==============================] - 1968s 21ms/step - loss: 0.6931 - val_loss: 2.1304\n",
      "Epoch 8/100\n",
      "93127/93128 [============================>.] - ETA: 0s - loss: 0.6931\n",
      "Epoch 00008: saving model to C:/Users/velmurug/Documents/Stability Experiments/benchmark_interpretability/PPM_Stability/bpic2012/lstm_single_3d/cls\\checkpoint.cpt\n",
      "93128/93128 [==============================] - 1961s 21ms/step - loss: 0.6931 - val_loss: 2.1623\n",
      "Epoch 9/100\n",
      "93127/93128 [============================>.] - ETA: 0s - loss: 0.6931\n",
      "Epoch 00009: saving model to C:/Users/velmurug/Documents/Stability Experiments/benchmark_interpretability/PPM_Stability/bpic2012/lstm_single_3d/cls\\checkpoint.cpt\n",
      "93128/93128 [==============================] - 1934s 21ms/step - loss: 0.6931 - val_loss: 2.1611\n",
      "Epoch 10/100\n",
      "93127/93128 [============================>.] - ETA: 0s - loss: 0.6932- ETA: 0s - loss: 0\n",
      "Epoch 00010: saving model to C:/Users/velmurug/Documents/Stability Experiments/benchmark_interpretability/PPM_Stability/bpic2012/lstm_single_3d/cls\\checkpoint.cpt\n",
      "93128/93128 [==============================] - 1934s 21ms/step - loss: 0.6932 - val_loss: 2.2195\n",
      "Epoch 11/100\n",
      "93126/93128 [============================>.] - ETA: 0s - loss: 0.6932\n",
      "Epoch 00011: saving model to C:/Users/velmurug/Documents/Stability Experiments/benchmark_interpretability/PPM_Stability/bpic2012/lstm_single_3d/cls\\checkpoint.cpt\n",
      "93128/93128 [==============================] - 1961s 21ms/step - loss: 0.6932 - val_loss: 2.1809\n",
      "Epoch 12/100\n",
      "24250/93128 [======>.......................] - ETA: 23:13 - loss: 0.6931"
     ]
    }
   ],
   "source": [
    "for dataset_name in datasets:\n",
    "    \n",
    "    # load optimal params\n",
    "    print(\"Setting up parameters...\")\n",
    "    optimal_params_filename = os.path.join(params_dir, \"optimal_params_%s_%s_%s.pickle\" % (cls_method, dataset_name, method_name))\n",
    "\n",
    "    if not os.path.isfile(optimal_params_filename) or os.path.getsize(optimal_params_filename) <= 0:\n",
    "        print(\"Parameters not found\")\n",
    "        \n",
    "    with open(optimal_params_filename, \"rb\") as fin:\n",
    "        args = pickle.load(fin)\n",
    "    \n",
    "    args['batch_size'] = 1\n",
    "    \n",
    "    print(args)\n",
    "            \n",
    "    # read the data\n",
    "    print(\"setting up data...\")\n",
    "    dataset_manager = DatasetManager(dataset_name)\n",
    "    data = dataset_manager.read_dataset()\n",
    "    #print('Case ID column', dataset_manager.case_id_col)\n",
    "    cls_encoder_args = {'case_id_col': dataset_manager.case_id_col, \n",
    "                        'static_cat_cols': dataset_manager.static_cat_cols,\n",
    "                        'static_num_cols': dataset_manager.static_num_cols, \n",
    "                        'dynamic_cat_cols': dataset_manager.dynamic_cat_cols,\n",
    "                        'dynamic_num_cols': dataset_manager.dynamic_num_cols, \n",
    "                        'fillna': True}\n",
    "\n",
    "    # determine min and max (truncated) prefix lengths\n",
    "    min_prefix_length = 1\n",
    "    if \"traffic_fines\" in dataset_name:\n",
    "        max_prefix_length = 10\n",
    "    elif \"bpic2017\" in dataset_name:\n",
    "        max_prefix_length = min(20, dataset_manager.get_pos_case_length_quantile(data, 0.90))\n",
    "    else:\n",
    "        max_prefix_length = min(40, dataset_manager.get_pos_case_length_quantile(data, 0.90))\n",
    "\n",
    "    # split into training and test\n",
    "    train, test = dataset_manager.split_data_strict(data, train_ratio, split=\"temporal\")\n",
    "    train, val = dataset_manager.split_data_strict(train, 0.8)\n",
    "    \n",
    "    if gap > 1:\n",
    "        outfile = os.path.join(results_dir, \"performance_results_%s_%s_%s_gap%s.csv\" % (cls_method, dataset_name, method_name, gap))\n",
    "    else:\n",
    "        outfile = os.path.join(results_dir, \"performance_results_%s_%s_%s.csv\" % (cls_method, dataset_name, method_name))\n",
    "        \n",
    "    start_test_prefix_generation = time.time()\n",
    "    dt_test_prefixes = dataset_manager.generate_prefix_data(test, min_prefix_length, max_prefix_length)\n",
    "    test_prefix_generation_time = time.time() - start_test_prefix_generation\n",
    "            \n",
    "    offline_total_times = []\n",
    "    online_event_times = []\n",
    "    train_prefix_generation_times = []\n",
    "\n",
    "    for ii in range(n_iter):\n",
    "        if cls_method == \"lstm\":\n",
    "            bucket = 'all'\n",
    "            max_len = max_prefix_length\n",
    "            \n",
    "            preds_all = []\n",
    "            probas_all = []\n",
    "            pred_types_all = []\n",
    "            test_y_all = []\n",
    "            nr_events_all = []\n",
    "            \n",
    "            dt_train = dataset_manager.encode_data_for_lstm(train)\n",
    "            dt_test = dataset_manager.encode_data_for_lstm(test)\n",
    "            dt_val = dataset_manager.encode_data_for_lstm(val)\n",
    "\n",
    "            data_dim = dt_train.shape[1]-3\n",
    "            \n",
    "            print(data_dim)\n",
    "\n",
    "            dt_train_bucket, train_y = dataset_manager.generate_3d_data(dt_train, max_len)\n",
    "            dt_test_bucket, y_test= dataset_manager.generate_3d_data(dt_test, max_len)\n",
    "            dt_val_bucket, val_y = dataset_manager.generate_3d_data(dt_val, max_len)\n",
    "            test_y = [np.where(act == 1)[0][0] for act in y_test]\n",
    "            \n",
    "            #remove instances from validation set that won't fit with batch size\n",
    "            #print(\"original validation shape:\", dt_val_bucket.shape, val_y.shape)\n",
    "            val_excess = dt_val_bucket.shape[0]%args['batch_size']\n",
    "            #print(\"tail:\", val_excess)\n",
    "            to_keep = dt_val_bucket.shape[0] - val_excess\n",
    "            #print(\"final size:\", to_keep)\n",
    "            dt_val_bucket = dt_val_bucket[:to_keep,:,:]\n",
    "            val_y = val_y[:to_keep,:]\n",
    "            #print(\"new validation shape:\", dt_val_bucket.shape, val_y.shape)\n",
    "            \n",
    "            print(dt_train_bucket.shape)\n",
    "            print(dt_val_bucket.shape)\n",
    "            print(dt_test_bucket.shape)\n",
    "\n",
    "            \n",
    "            #save model parameters and create callback for model\n",
    "            params_path = os.path.join(PATH, \"%s/%s_%s/cls/params.pickle\" % (dataset_ref, cls_method, method_name))\n",
    "            with open(params_path, 'wb') as f:\n",
    "                pickle.dump(args, f)\n",
    "            \n",
    "            checkpoint_path = os.path.join(PATH, \"%s/%s_%s/cls/checkpoint.cpt\" % (dataset_ref, cls_method, method_name))\n",
    "            cp_callback = ModelCheckpoint(filepath=checkpoint_path, save_weights_only=True, verbose=1)\n",
    "            \n",
    "            early_stop = EarlyStopping(monitor = 'val_loss', mode = 'min', verbose = 1, patience = 5)\n",
    "            \n",
    "            #create model\n",
    "            main_input = Input(shape=(max_len, data_dim), batch_size = args['batch_size'], name='main_input')\n",
    "\n",
    "            if args[\"lstm_layers\"][\"layers\"] == \"one\":\n",
    "                l2_3 = LSTM(args['lstm1_nodes'], batch_input_shape=(args['batch_size'], max_len, data_dim), \n",
    "                            implementation=2, kernel_initializer='glorot_uniform', return_sequences=False, \n",
    "                            recurrent_dropout=args['lstm1_dropouts'], stateful = True)(main_input)\n",
    "                b2_3 = BatchNormalization()(l2_3)\n",
    "\n",
    "            if args[\"lstm_layers\"][\"layers\"] == \"two\":\n",
    "                l1 = LSTM(args['lstm1_nodes'], batch_input_shape=(args['batch_size'], max_len, data_dim), \n",
    "                          implementation=2, kernel_initializer='glorot_uniform', return_sequences=True, \n",
    "                          recurrent_dropout=args['lstm1_dropouts'], stateful = True)(main_input)\n",
    "                b1 = BatchNormalization()(l1)\n",
    "                l2_3 = LSTM(args[\"lstm_layers\"][\"lstm2_nodes\"], activation=\"sigmoid\", \n",
    "                            implementation=2, kernel_initializer='glorot_uniform', return_sequences=False, \n",
    "                            recurrent_dropout=args[\"lstm_layers\"][\"lstm2_dropouts\"], stateful = True)(b1)\n",
    "                b2_3 = BatchNormalization()(l2_3)\n",
    "                \n",
    "            if args[\"lstm_layers\"][\"layers\"] == \"three\":\n",
    "                l1 = LSTM(args['lstm1_nodes'], batch_input_shape=(args['batch_size'], max_len, data_dim), \n",
    "                          implementation=2, kernel_initializer='glorot_uniform', return_sequences=True, \n",
    "                          recurrent_dropout=args['lstm1_dropouts'], stateful = True)(main_input)\n",
    "                b1 = BatchNormalization()(l1)\n",
    "                l2 = LSTM(args[\"lstm_layers\"][\"lstm2_nodes\"], activation=\"sigmoid\", \n",
    "                            implementation=2, kernel_initializer='glorot_uniform', return_sequences=True, \n",
    "                            recurrent_dropout=args[\"lstm_layers\"][\"lstm2_dropouts\"], stateful = True)(b1)\n",
    "                b2 = BatchNormalization()(l2)\n",
    "                l2_3 = LSTM(args[\"lstm_layers\"][\"lstm3_nodes\"], activation=\"sigmoid\", \n",
    "                            implementation=2, kernel_initializer='glorot_uniform', return_sequences=False, \n",
    "                            recurrent_dropout=args[\"lstm_layers\"][\"lstm3_dropouts\"], stateful = True)(b2)\n",
    "                b2_3 = BatchNormalization()(l2_3)\n",
    "\n",
    "\n",
    "            if args['dense_layers']['layers'] == \"two\":\n",
    "                d1 = Dense(args['dense_layers']['dense2_nodes'], activation = \"relu\")(b2_3)\n",
    "                outcome_output = Dense(2, activation='sigmoid', kernel_initializer='glorot_uniform', name='outcome_output')(d1)\n",
    "\n",
    "            else:\n",
    "                outcome_output = Dense(2, activation='sigmoid', kernel_initializer='glorot_uniform', name='outcome_output')(b2_3)\n",
    "\n",
    "            cls = Model(inputs=[main_input], outputs=[outcome_output])\n",
    "\n",
    "            if args['optimizer'] == \"adam\":\n",
    "                opt = Nadam(lr=args['learning_rate'], beta_1=0.9, beta_2=0.999, epsilon=1e-08, schedule_decay=0.004, clipvalue=3)\n",
    "            elif args['optimizer'] == \"rmsprop\":\n",
    "                opt = RMSprop(lr=args['learning_rate'], rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "                \n",
    "            #cls = load_model(\"%s/%s_%s/cls/backup_0.5_with_all.h5\" % (dataset_ref, cls_method, method_name))\n",
    "\n",
    "            cls.compile(loss='binary_crossentropy', optimizer=opt)\n",
    "\n",
    "            history = cls.fit(dt_train_bucket, train_y, validation_data = (dt_val_bucket,val_y), verbose = 1, \n",
    "                              epochs = args['epochs'], batch_size = args['batch_size'],\n",
    "                              callbacks=[early_stop, cp_callback], steps_per_epoch = dt_train_bucket.shape[0]//args['batch_size'])\n",
    "            \n",
    "            cls.save(\"%s/%s_%s/cls/cls.h5\" % (dataset_ref, cls_method, method_name))\n",
    "            \n",
    "            print(cls.summary())\n",
    "            \n",
    "            weights = cls.get_weights()\n",
    "\n",
    "            main_input = Input(shape=(max_len, data_dim), batch_size = 1, name='main_input')\n",
    "\n",
    "            if args[\"lstm_layers\"][\"layers\"] == \"one\":\n",
    "                l2_3 = LSTM(args['lstm1_nodes'], batch_input_shape=(1, max_len, data_dim), \n",
    "                            implementation=2, kernel_initializer='glorot_uniform', return_sequences=False, \n",
    "                            recurrent_dropout=args['lstm1_dropouts'], stateful = True)(main_input)\n",
    "                b2_3 = BatchNormalization()(l2_3)\n",
    "\n",
    "            if args[\"lstm_layers\"][\"layers\"] == \"two\":\n",
    "                l1 = LSTM(args['lstm1_nodes'], batch_input_shape=(1, max_len, data_dim), \n",
    "                          implementation=2, kernel_initializer='glorot_uniform', return_sequences=True, \n",
    "                          recurrent_dropout=args['lstm1_dropouts'], stateful = True)(main_input)\n",
    "                b1 = BatchNormalization()(l1)\n",
    "                l2_3 = LSTM(args[\"lstm_layers\"][\"lstm2_nodes\"], activation=\"sigmoid\", \n",
    "                            implementation=2, kernel_initializer='glorot_uniform', return_sequences=False, \n",
    "                            recurrent_dropout=args[\"lstm_layers\"][\"lstm2_dropouts\"], stateful = True)(b1)\n",
    "                b2_3 = BatchNormalization()(l2_3)\n",
    "\n",
    "            if args[\"lstm_layers\"][\"layers\"] == \"three\":\n",
    "                l1 = LSTM(args['lstm1_nodes'], batch_input_shape=(1, max_len, data_dim), \n",
    "                          implementation=2, kernel_initializer='glorot_uniform', return_sequences=True, \n",
    "                          recurrent_dropout=args['lstm1_dropouts'], stateful = True)(main_input)\n",
    "                b1 = BatchNormalization()(l1)\n",
    "                l2 = LSTM(args[\"lstm_layers\"][\"lstm2_nodes\"], activation=\"sigmoid\", \n",
    "                            implementation=2, kernel_initializer='glorot_uniform', return_sequences=True, \n",
    "                            recurrent_dropout=args[\"lstm_layers\"][\"lstm2_dropouts\"], stateful = True)(b1)\n",
    "                b2 = BatchNormalization()(l2)\n",
    "                l2_3 = LSTM(args[\"lstm_layers\"][\"lstm3_nodes\"], activation=\"sigmoid\", \n",
    "                            implementation=2, kernel_initializer='glorot_uniform', return_sequences=False, \n",
    "                            recurrent_dropout=args[\"lstm_layers\"][\"lstm3_dropouts\"], stateful = True)(b2)\n",
    "                b2_3 = BatchNormalization()(l2_3)\n",
    "\n",
    "\n",
    "            if args['dense_layers']['layers'] == \"two\":\n",
    "                d1 = Dense(args['dense_layers']['dense2_nodes'], activation = \"relu\")(b2_3)\n",
    "                outcome_output = Dense(2, activation='sigmoid', kernel_initializer='glorot_uniform', name='outcome_output')(d1)\n",
    "\n",
    "            else:\n",
    "                outcome_output = Dense(2, activation='sigmoid', kernel_initializer='glorot_uniform', name='outcome_output')(b2_3)\n",
    "\n",
    "            pred_cls = Model(inputs=[main_input], outputs=[outcome_output])\n",
    "            pred_cls.set_weights(weights)\n",
    "            pred_cls.compile(loss='binary_crossentropy', optimizer=opt)\n",
    "\n",
    "            pred_cls.summary()\n",
    "\n",
    "            preds = []\n",
    "            probas = []\n",
    "            pred_types = []\n",
    "\n",
    "            tp_list = []\n",
    "            tn_list = []\n",
    "            fp_list = []\n",
    "            fn_list = []\n",
    "\n",
    "            #preds_pos_label_idx = np.where(cls.classes_ == 1)[0][0]\n",
    "            #probas = []\n",
    "            #for each in dt_test_bucket:\n",
    "            #    proba = cls.predict(np.array([each,]))#[:,preds_pos_label_idx]\n",
    "            #    probas.append(proba[0])\n",
    "\n",
    "            #preds = []\n",
    "            #for each in probas:\n",
    "            #    pred = each.argmax(axis=-1)\n",
    "            #    preds.append(pred)\n",
    "\n",
    "            iteration = 1\n",
    "\n",
    "            test_all_grouped = dt_test.groupby(dataset_manager.case_id_col)\n",
    "\n",
    "            for _ , each in test_all_grouped:\n",
    "                print (\"Testing instance\", iteration, 'of', len(test_all_grouped))\n",
    "                nr_length = each.shape[0]\n",
    "                group, y_test_group, case_ids = dataset_manager.generate_3d_data_for_prefix_length(each, max_len, nr_events = 1)\n",
    "                test_y_group = np.argmax(y_test_group)\n",
    "                test_y_all.append(test_y_group)\n",
    "                print(group.shape)\n",
    "\n",
    "                start = time.time()\n",
    "                case_id = case_ids[0]\n",
    "\n",
    "                proba = pred_cls.predict(group)\n",
    "                pred = np.argmax(proba, axis=-1)[0]\n",
    "\n",
    "                if test_y_group == pred & pred == 0:\n",
    "                  pred_type = 'TN'\n",
    "\n",
    "                  instance_dict = {}\n",
    "                  instance_dict['caseID'] = case_id\n",
    "                  instance_dict['input'] = group\n",
    "                  instance_dict['actual'] = test_y_group\n",
    "                  instance_dict['predicted'] = pred\n",
    "                  instance_dict['proba'] = 1 - proba[0]\n",
    "                  instance_dict['nr_events'] = nr_length\n",
    "                  instance_dict['pred_type'] = pred_type\n",
    "\n",
    "                  tn_list.append(instance_dict)\n",
    "\n",
    "                elif test_y_group == pred & pred == 1:\n",
    "                  pred_type = 'TP'\n",
    "\n",
    "                  instance_dict = {}\n",
    "                  instance_dict['caseID'] = case_id\n",
    "                  instance_dict['input'] = group\n",
    "                  instance_dict['actual'] = test_y_group\n",
    "                  instance_dict['predicted'] = pred\n",
    "                  instance_dict['proba'] = proba[0]\n",
    "                  instance_dict['nr_events'] = nr_length\n",
    "                  instance_dict['pred_type'] = pred_type\n",
    "\n",
    "                  tp_list.append(instance_dict)\n",
    "\n",
    "                elif test_y_group != pred & pred == 0:\n",
    "                  pred_type = 'FN'\n",
    "\n",
    "                  instance_dict = {}\n",
    "                  instance_dict['caseID'] = case_id\n",
    "                  instance_dict['input'] = group\n",
    "                  instance_dict['actual'] = test_y_group\n",
    "                  instance_dict['predicted'] = pred\n",
    "                  instance_dict['proba'] = 1 - proba[0]\n",
    "                  instance_dict['nr_events'] = nr_length\n",
    "                  instance_dict['pred_type'] = pred_type\n",
    "\n",
    "                  fn_list.append(instance_dict)\n",
    "\n",
    "                elif test_y_group != pred & pred == 1:\n",
    "                  pred_type = 'FP'\n",
    "\n",
    "                  instance_dict = {}\n",
    "                  instance_dict['caseID'] = case_id\n",
    "                  instance_dict['input'] = group\n",
    "                  instance_dict['actual'] = test_y_group\n",
    "                  instance_dict['predicted'] = pred\n",
    "                  instance_dict['proba'] = proba[0]\n",
    "                  instance_dict['nr_events'] = nr_length\n",
    "                  instance_dict['pred_type'] = pred_type\n",
    "\n",
    "                  fp_list.append(instance_dict)\n",
    "                else:\n",
    "                  print(\"ERROR: Prediction doesn't fit into any category. Check code.\")\n",
    "\n",
    "                preds.append(int(pred))\n",
    "                probas.extend(proba)\n",
    "                pred_types.append(pred_type)\n",
    "                iteration+=1\n",
    "\n",
    "            for each in preds:\n",
    "                preds_all.append(each)\n",
    "            probas_all.extend(probas)\n",
    "            pred_types_all.extend(pred_types)\n",
    "            \n",
    "            pred_cls.save(\"%s/%s_%s/cls/pred_cls.h5\" % (dataset_ref, cls_method, method_name))\n",
    "\n",
    "\n",
    "            tn_path = os.path.join(PATH, \"%s/%s_%s/instances/true_neg_bucket_%s_.pickle\" % (dataset_ref, cls_method, method_name, bucket))\n",
    "            tp_path = os.path.join(PATH, \"%s/%s_%s/instances/true_pos_bucket_%s_.pickle\" % (dataset_ref, cls_method, method_name, bucket))\n",
    "            fn_path = os.path.join(PATH, \"%s/%s_%s/instances/false_neg_bucket_%s_.pickle\" % (dataset_ref, cls_method, method_name, bucket))\n",
    "            fp_path = os.path.join(PATH, \"%s/%s_%s/instances/false_pos_bucket_%s_.pickle\" % (dataset_ref, cls_method, method_name, bucket))\n",
    "\n",
    "            with open (tn_path, 'wb') as f:\n",
    "              pickle.dump(tn_list, f)\n",
    "            with open (tp_path, 'wb') as f:\n",
    "              pickle.dump(tp_list, f)\n",
    "            with open (fn_path, 'wb') as f:\n",
    "              pickle.dump(fn_list, f)\n",
    "            with open (fp_path, 'wb') as f:\n",
    "              pickle.dump(fp_list, f)\n",
    "\n",
    "            #Save training data\n",
    "            print(\"saving data...\")\n",
    "            X_train_path = os.path.join(PATH, \"%s/%s_%s/train_data/bucket_%s_prefixes.pickle\" % (dataset_ref, cls_method, method_name, bucket))\n",
    "            Y_train_path = os.path.join(PATH, \"%s/%s_%s/train_data/bucket_%s_labels.pickle\" % (dataset_ref, cls_method, method_name, bucket))\n",
    "            with open(X_train_path, 'wb') as f:\n",
    "              pickle.dump(dt_train_bucket, f)\n",
    "            with open(Y_train_path, 'wb') as f:\n",
    "              pickle.dump(train_y, f)\n",
    "\n",
    "            #Save testing data\n",
    "            X_test_path = os.path.join(PATH, \"%s/%s_%s/test_data/bucket_%s_prefixes.pickle\" % (dataset_ref, cls_method, method_name, bucket))\n",
    "            Y_test_path = os.path.join(PATH, \"%s/%s_%s/test_data/bucket_%s_labels.pickle\" % (dataset_ref, cls_method, method_name, bucket))\n",
    "            with open(X_test_path, 'wb') as f:\n",
    "                pickle.dump(dt_test_bucket, f)\n",
    "            with open(Y_test_path, 'wb') as f:\n",
    "                pickle.dump(test_y_group, f)\n",
    "\n",
    "            acc = accuracy_score(test_y_all, preds_all)\n",
    "            roc_auc = roc_auc_score(test_y_all, preds_all)\n",
    "            print(\"Accuracy:\", acc, \"\\tROCAUC:\", roc_auc)\n",
    "\n",
    "        else:\n",
    "            # create prefix logs\n",
    "            print(\"Creating logs...\")\n",
    "            start_train_prefix_generation = time.time()\n",
    "            dt_train_prefixes = dataset_manager.generate_prefix_data(train, min_prefix_length, max_prefix_length, gap)\n",
    "            train_prefix_generation_time = time.time() - start_train_prefix_generation\n",
    "            train_prefix_generation_times.append(train_prefix_generation_time)\n",
    "\n",
    "            # Bucketing prefixes based on control flow\n",
    "            print(\"bucketing prefixes...\")\n",
    "            bucketer_args = {'encoding_method':bucket_encoding,\n",
    "                             'case_id_col':dataset_manager.case_id_col, \n",
    "                             'cat_cols':[dataset_manager.activity_col], \n",
    "                             'num_cols':[], \n",
    "                             'random_state':random_state}\n",
    "            if bucket_method == \"cluster\":\n",
    "                bucketer_args[\"n_clusters\"] = int(args[\"n_clusters\"])\n",
    "            bucketer = BucketFactory.get_bucketer(bucket_method, **bucketer_args)\n",
    "\n",
    "            start_offline_time_bucket = time.time()\n",
    "            bucket_assignments_train = bucketer.fit_predict(dt_train_prefixes)\n",
    "            offline_time_bucket = time.time() - start_offline_time_bucket\n",
    "\n",
    "            bucket_assignments_test = bucketer.predict(dt_test_prefixes)\n",
    "\n",
    "            preds_all = []\n",
    "            probas_all = []\n",
    "            pred_types_all = []\n",
    "            test_y_all = []\n",
    "            nr_events_all = []\n",
    "            offline_time_fit = 0\n",
    "            current_online_event_times = []\n",
    "\n",
    "            for bucket in set(bucket_assignments_test):\n",
    "                print(\"Bucket\" , bucket )\n",
    "                print(\"sorting bucket...\")\n",
    "                if bucket_method == \"prefix\":\n",
    "                    current_args = args[bucket]\n",
    "                else:\n",
    "                    current_args = args\n",
    "                relevant_train_cases_bucket = dataset_manager.get_indexes(dt_train_prefixes)[bucket_assignments_train == bucket]\n",
    "                relevant_test_cases_bucket = dataset_manager.get_indexes(dt_test_prefixes)[bucket_assignments_test == bucket]\n",
    "                dt_test_bucket = dataset_manager.get_relevant_data_by_indexes(dt_test_prefixes, relevant_test_cases_bucket)\n",
    "\n",
    "                nr_events_all.extend(list(dataset_manager.get_prefix_lengths(dt_test_bucket)))\n",
    "                print('number events', len(nr_events_all))\n",
    "\n",
    "                if len(relevant_train_cases_bucket) == 0:\n",
    "                    preds = [dataset_manager.get_class_ratio(train)] * len(relevant_test_cases_bucket)\n",
    "                    current_online_event_times.extend([0] * len(preds))\n",
    "                else:\n",
    "                    dt_train_bucket = dataset_manager.get_relevant_data_by_indexes(dt_train_prefixes, relevant_train_cases_bucket) # one row per event\n",
    "                    train_y = dataset_manager.get_label_numeric(dt_train_bucket)\n",
    "\n",
    "                    if len(set(train_y)) < 2:\n",
    "                        preds = [train_y[0]] * len(relevant_test_cases_bucket)\n",
    "                        current_online_event_times.extend([0] * len(preds))\n",
    "                        test_y_all.extend(dataset_manager.get_label_numeric(dt_test_bucket))\n",
    "                    else:\n",
    "                        print(\"choosing classifier...\")\n",
    "                        start_offline_time_fit = time.time()\n",
    "                        feature_combiner = FeatureUnion([(method, EncoderFactory.get_encoder(method, **cls_encoder_args)) for method in methods])\n",
    "\n",
    "                        if cls_method == \"rf\":\n",
    "                            cls = RandomForestClassifier(n_estimators=500,\n",
    "                                                         max_features=current_args['max_features'],\n",
    "                                                         random_state=random_state)\n",
    "\n",
    "                        elif cls_method == \"xgboost\":\n",
    "                            cls = xgb.XGBClassifier(objective='binary:logistic',\n",
    "                                                    n_estimators=500,\n",
    "                                                    learning_rate= current_args['learning_rate'],\n",
    "                                                    subsample=current_args['subsample'],\n",
    "                                                    max_depth=int(current_args['max_depth']),\n",
    "                                                    colsample_bytree=current_args['colsample_bytree'],\n",
    "                                                    min_child_weight=int(current_args['min_child_weight']),\n",
    "                                                    seed=random_state)\n",
    "                        elif cls_method == \"cb\":\n",
    "                            cls = catboost.CatBoostClassifier(learning_rate=current_args['learning_rate'],\n",
    "                                                               subsample=current_args['subsample'], \n",
    "                                                              depth=current_args['depth'])\n",
    "                        #elif cls_method == \"lstm\":\n",
    "                        #    break\n",
    "\n",
    "                        elif cls_method == \"logit\":\n",
    "                            cls = LogisticRegression(C=2**current_args['C'],\n",
    "                                                     random_state=random_state)\n",
    "\n",
    "                        elif cls_method == \"svm\":\n",
    "                            cls = SVC(C=2**current_args['C'],\n",
    "                                      gamma=2**current_args['gamma'],\n",
    "                                      random_state=random_state)\n",
    "\n",
    "                        if cls_method == \"svm\" or cls_method == \"logit\":\n",
    "                            pipeline = Pipeline([('encoder', feature_combiner), ('scaler', StandardScaler()), ('cls', cls)])\n",
    "                        else:\n",
    "                            pipeline = Pipeline([('encoder', feature_combiner), ('cls', cls)])\n",
    "\n",
    "                        print(\"fitting pipeline...\")\n",
    "                        pipeline.fit(dt_train_bucket, train_y)\n",
    "\n",
    "                        offline_time_fit += time.time() - start_offline_time_fit\n",
    "\n",
    "                        # predict separately for each prefix case\n",
    "                        preds = []\n",
    "                        probas = []\n",
    "                        pred_types = []\n",
    "\n",
    "                        test_all_grouped = dt_test_bucket.groupby(dataset_manager.case_id_col)\n",
    "                        print(\"test data shape\", dt_test_bucket.shape)\n",
    "                        count_d=0 # count for deviant\n",
    "                        count_r=0 #count for regular\n",
    "\n",
    "                        tp_list = []\n",
    "                        tn_list = []\n",
    "                        fp_list = []\n",
    "                        fn_list = []\n",
    "\n",
    "                        iteration = 1\n",
    "\n",
    "                        for _, group in test_all_grouped:\n",
    "                            print (\"Testing instance\", iteration, 'of', len(test_all_grouped))\n",
    "                            test_y_group = dataset_manager.get_label_numeric(group)\n",
    "                            test_y_all.extend(test_y_group)\n",
    "\n",
    "                            start = time.time()\n",
    "                            _ = bucketer.predict(group)\n",
    "\n",
    "                            if cls_method == \"svm\":\n",
    "                                pred = pipeline.decision_function(group)\n",
    "                            else:\n",
    "                              preds_pos_label_idx = np.where(cls.classes_ == 1)[0][0]\n",
    "                              pred = pipeline.predict(group)[0]\n",
    "                              proba = pipeline.predict_proba(group)[:,preds_pos_label_idx]\n",
    "                            pipeline_pred_time = time.time() - start\n",
    "                            current_online_event_times.append(pipeline_pred_time / len(group))\n",
    "                            case_id = dataset_manager.get_case_ids(group)[0]\n",
    "\n",
    "                            if test_y_group == pred & pred == 0:\n",
    "                              pred_type = 'TN'\n",
    "\n",
    "                              instance_dict = {}\n",
    "                              instance_dict['caseID'] = case_id\n",
    "                              instance_dict['input'] = group\n",
    "                              instance_dict['actual'] = test_y_group[0]\n",
    "                              instance_dict['predicted'] = pred\n",
    "                              instance_dict['proba'] = 1 - proba[0]\n",
    "                              instance_dict['nr_events'] = dataset_manager.get_prefix_lengths(group)[0]\n",
    "                              instance_dict['pred_type'] = pred_type\n",
    "\n",
    "                              tn_list.append(instance_dict)\n",
    "\n",
    "                            elif test_y_group == pred & pred == 1:\n",
    "                              pred_type = 'TP'\n",
    "\n",
    "                              instance_dict = {}\n",
    "                              instance_dict['caseID'] = case_id\n",
    "                              instance_dict['input'] = group\n",
    "                              instance_dict['actual'] = test_y_group[0]\n",
    "                              instance_dict['predicted'] = pred\n",
    "                              instance_dict['proba'] = proba[0]\n",
    "                              instance_dict['nr_events'] = dataset_manager.get_prefix_lengths(group)[0]\n",
    "                              instance_dict['pred_type'] = pred_type\n",
    "\n",
    "                              tp_list.append(instance_dict)\n",
    "\n",
    "                            elif test_y_group != pred & pred == 0:\n",
    "                              pred_type = 'FN'\n",
    "\n",
    "                              instance_dict = {}\n",
    "                              instance_dict['caseID'] = case_id\n",
    "                              instance_dict['input'] = group\n",
    "                              instance_dict['actual'] = test_y_group[0]\n",
    "                              instance_dict['predicted'] = pred\n",
    "                              instance_dict['proba'] = 1 - proba[0]\n",
    "                              instance_dict['nr_events'] = dataset_manager.get_prefix_lengths(group)[0]\n",
    "                              instance_dict['pred_type'] = pred_type\n",
    "\n",
    "                              fn_list.append(instance_dict)\n",
    "\n",
    "                            elif test_y_group != pred & pred == 1:\n",
    "                              pred_type = 'FP'\n",
    "\n",
    "                              instance_dict = {}\n",
    "                              instance_dict['caseID'] = case_id\n",
    "                              instance_dict['input'] = group\n",
    "                              instance_dict['actual'] = test_y_group[0]\n",
    "                              instance_dict['predicted'] = pred\n",
    "                              instance_dict['proba'] = proba[0]\n",
    "                              instance_dict['nr_events'] = dataset_manager.get_prefix_lengths(group)[0]\n",
    "                              instance_dict['pred_type'] = pred_type\n",
    "\n",
    "                              fp_list.append(instance_dict)\n",
    "                            else:\n",
    "                              print(\"ERROR: Prediction doesn't fit into any category. Check code.\")\n",
    "\n",
    "                            preds.append(pred)\n",
    "                            probas.extend(proba)\n",
    "                            pred_types.append(pred_type)\n",
    "                            iteration+=1\n",
    "                        \n",
    "                    preds_all.extend(preds)\n",
    "                    probas_all.extend(probas)\n",
    "                    pred_types_all.extend(pred_types)\n",
    "\n",
    "                    tn_path = os.path.join(PATH, \"%s/%s_%s/instances/true_neg_bucket_%s_.pickle\" % (dataset_ref, cls_method, method_name, bucket))\n",
    "                    tp_path = os.path.join(PATH, \"%s/%s_%s/instances/true_pos_bucket_%s_.pickle\" % (dataset_ref, cls_method, method_name, bucket))\n",
    "                    fn_path = os.path.join(PATH, \"%s/%s_%s/instances/false_neg_bucket_%s_.pickle\" % (dataset_ref, cls_method, method_name, bucket))\n",
    "                    fp_path = os.path.join(PATH, \"%s/%s_%s/instances/false_pos_bucket_%s_.pickle\" % (dataset_ref, cls_method, method_name, bucket))\n",
    "                    \n",
    "                    with open (tn_path, 'wb') as f:\n",
    "                      pickle.dump(tn_list, f)\n",
    "                    with open (tp_path, 'wb') as f:\n",
    "                      pickle.dump(tp_list, f)\n",
    "                    with open (fn_path, 'wb') as f:\n",
    "                      pickle.dump(fn_list, f)\n",
    "                    with open (fp_path, 'wb') as f:\n",
    "                      pickle.dump(fp_list, f)\n",
    "\n",
    "            #Save models and encoders\n",
    "            print('Saving models...')\n",
    "            pipeline_path = os.path.join(PATH, \"%s/%s_%s/pipelines/pipeline_bucket_%s.joblib\" % (dataset_ref, cls_method, method_name, bucket))\n",
    "            feat_comb_path = os.path.join(PATH, \"%s/%s_%s/bucketers_and_encoders/feature_combiner_bucket_%s.joblib\" % (dataset_ref, cls_method, method_name, bucket))\n",
    "            bucketer_path = os.path.join(PATH, \"%s/%s_%s/bucketers_and_encoders/bucketer_bucket_%s.joblib\" % (dataset_ref, cls_method, method_name, bucket))\n",
    "            cls_path = os.path.join(PATH, \"%s/%s_%s/models/cls_bucket_%s.joblib\" % (dataset_ref, cls_method, method_name, bucket))\n",
    "            joblib.dump(pipeline, pipeline_path)\n",
    "            joblib.dump(feature_combiner, feat_comb_path)\n",
    "            joblib.dump(cls, cls_path)\n",
    "            joblib.dump(bucketer, bucketer_path)\n",
    "\n",
    "            #Save training data\n",
    "            print(\"saving data...\")\n",
    "            X_train_path = os.path.join(PATH, \"%s/%s_%s/train_data/bucket_%s_prefixes.pickle\" % (dataset_ref, cls_method, method_name, bucket))\n",
    "            Y_train_path = os.path.join(PATH, \"%s/%s_%s/train_data/bucket_%s_labels.pickle\" % (dataset_ref, cls_method, method_name, bucket))\n",
    "            with open(X_train_path, 'wb') as f:\n",
    "              pickle.dump(dt_train_bucket, f)\n",
    "            with open(Y_train_path, 'wb') as f:\n",
    "              pickle.dump(train_y, f)\n",
    "\n",
    "            #Save testing data\n",
    "            X_test_path = os.path.join(PATH, \"%s/%s_%s/test_data/bucket_%s_prefixes.pickle\" % (dataset_ref, cls_method, method_name, bucket))\n",
    "            Y_test_path = os.path.join(PATH, \"%s/%s_%s/test_data/bucket_%s_labels.pickle\" % (dataset_ref, cls_method, method_name, bucket))\n",
    "            with open(X_test_path, 'wb') as f:\n",
    "                pickle.dump(dt_test_bucket, f)\n",
    "            with open(Y_test_path, 'wb') as f:\n",
    "                pickle.dump(test_y_group, f)\n",
    "        \n",
    "       # print(\"compiling results...\")\n",
    "        #dt_results = pd.DataFrame({\"actual\": test_y_all, \"predicted\": preds_all, \"nr_events\": nr_events_all})\n",
    "        #for nr_events, group in dt_results.groupby(\"nr_events\"):\n",
    "        #    if len(set(group.actual)) < 2:\n",
    "        #        print(dataset_name, method_name, cls_method, nr_events, \"auc\", np.nan)\n",
    "        #    else:\n",
    "        #        print(dataset_name, method_name, cls_method, nr_events, \"auc\", roc_auc_score(group.actual, group.predicted))\n",
    "        #print(dataset_name, method_name, cls_method, -1, -1, \"auc\", roc_auc_score(dt_results.actual, dt_results.predicted))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls.summary()\n",
    "\n",
    "plot_model(\n",
    "    cls,\n",
    "    to_file=\"model.png\",\n",
    "    show_shapes=True,\n",
    "    show_layer_names=True,\n",
    "    rankdir=\"TB\",\n",
    "    expand_nested=False,\n",
    "    dpi=96,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label = \"train\")\n",
    "plt.plot(history.history['val_loss'], label = \"test\")\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion = confusion_matrix(preds_all, test_y_all)\n",
    "sns.heatmap(confusion, annot=True, cmap='viridis')#, categories = ['Negative', 'Positive'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn_path = os.path.join(PATH, \"%s/%s_%s/instances/true_neg.pickle\" % (dataset_ref, cls_method, method_name))\n",
    "tp_path = os.path.join(PATH, \"%s/%s_%s/instances/true_pos.pickle\" % (dataset_ref, cls_method, method_name))\n",
    "fn_path = os.path.join(PATH, \"%s/%s_%s/instances/false_neg.pickle\" % (dataset_ref, cls_method, method_name))\n",
    "fp_path = os.path.join(PATH, \"%s/%s_%s/instances/false_pos.pickle\" % (dataset_ref, cls_method, method_name))\n",
    "\n",
    "with open (tn_path, 'rb') as f:\n",
    "  tn_list = pickle.load(f)\n",
    "with open (tp_path, 'rb') as f:\n",
    "  tp_list = pickle.load(f)\n",
    "with open (fn_path, 'rb') as f:\n",
    "  fn_list = pickle.load(f)\n",
    "with open (fp_path, 'rb') as f:\n",
    "  fp_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(fp_list))\n",
    "print(len(fn_list))\n",
    "print(len(tp_list))\n",
    "print(len(tn_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = [np.argmax(i) for i in train_y]\n",
    "print(len([i for i in y_train if i ==0]))\n",
    "print(len([i for i in y_train if i ==1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val = [np.argmax(i) for i in val_y]\n",
    "print(len([i for i in y_val if i ==0]))\n",
    "print(len([i for i in y_val if i ==1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_val = [np.argmax(i) for i in val_y]\n",
    "print(len([i for i in test_y if i ==0]))\n",
    "print(len([i for i in test_y if i ==1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls = load_model(\"%s/%s_%s/cls/backup_0.5_with_all.h5\" % (dataset_ref, cls_method, method_name))\n",
    "cls.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_all = []\n",
    "probas_all = []\n",
    "pred_types_all = []\n",
    "test_y_all = []\n",
    "nr_events_all = []\n",
    "\n",
    "preds = []\n",
    "probas = []\n",
    "pred_types = []\n",
    "\n",
    "tp_list = []\n",
    "tn_list = []\n",
    "fp_list = []\n",
    "fn_list = []\n",
    "\n",
    "#preds_pos_label_idx = np.where(cls.classes_ == 1)[0][0]\n",
    "#probas = []\n",
    "#for each in dt_test_bucket:\n",
    "#    proba = cls.predict(np.array([each,]))#[:,preds_pos_label_idx]\n",
    "#    probas.append(proba[0])\n",
    "\n",
    "#preds = []\n",
    "#for each in probas:\n",
    "#    pred = each.argmax(axis=-1)\n",
    "#    preds.append(pred)\n",
    "\n",
    "iteration = 1\n",
    "\n",
    "test_all_grouped = dt_val.groupby(dataset_manager.case_id_col)\n",
    "\n",
    "for _ , each in test_all_grouped:\n",
    "    print (\"Testing instance\", iteration, 'of', len(test_all_grouped))\n",
    "    nr_length = each.shape[0]\n",
    "    group, y_test_group, case_ids = dataset_manager.generate_3d_data_for_prefix_length(each, max_len, nr_events = 1)\n",
    "    test_y_group = np.argmax(y_test_group)\n",
    "    test_y_all.append(test_y_group)\n",
    "    print(group.shape)\n",
    "\n",
    "    start = time.time()\n",
    "    case_id = case_ids[0]\n",
    "\n",
    "    proba = pred_cls.predict(group)\n",
    "    pred = np.argmax(proba, axis=-1)[0]\n",
    "\n",
    "    if test_y_group == pred & pred == 0:\n",
    "      pred_type = 'TN'\n",
    "\n",
    "      instance_dict = {}\n",
    "      instance_dict['caseID'] = case_id\n",
    "      instance_dict['input'] = group\n",
    "      instance_dict['actual'] = test_y_group\n",
    "      instance_dict['predicted'] = pred\n",
    "      instance_dict['proba'] = 1 - proba[0]\n",
    "      instance_dict['nr_events'] = nr_length\n",
    "      instance_dict['pred_type'] = pred_type\n",
    "\n",
    "      tn_list.append(instance_dict)\n",
    "\n",
    "    elif test_y_group == pred & pred == 1:\n",
    "      pred_type = 'TP'\n",
    "\n",
    "      instance_dict = {}\n",
    "      instance_dict['caseID'] = case_id\n",
    "      instance_dict['input'] = group\n",
    "      instance_dict['actual'] = test_y_group\n",
    "      instance_dict['predicted'] = pred\n",
    "      instance_dict['proba'] = proba[0]\n",
    "      instance_dict['nr_events'] = nr_length\n",
    "      instance_dict['pred_type'] = pred_type\n",
    "\n",
    "      tp_list.append(instance_dict)\n",
    "\n",
    "    elif test_y_group != pred & pred == 0:\n",
    "      pred_type = 'FN'\n",
    "\n",
    "      instance_dict = {}\n",
    "      instance_dict['caseID'] = case_id\n",
    "      instance_dict['input'] = group\n",
    "      instance_dict['actual'] = test_y_group\n",
    "      instance_dict['predicted'] = pred\n",
    "      instance_dict['proba'] = 1 - proba[0]\n",
    "      instance_dict['nr_events'] = nr_length\n",
    "      instance_dict['pred_type'] = pred_type\n",
    "\n",
    "      fn_list.append(instance_dict)\n",
    "\n",
    "    elif test_y_group != pred & pred == 1:\n",
    "      pred_type = 'FP'\n",
    "\n",
    "      instance_dict = {}\n",
    "      instance_dict['caseID'] = case_id\n",
    "      instance_dict['input'] = group\n",
    "      instance_dict['actual'] = test_y_group\n",
    "      instance_dict['predicted'] = pred\n",
    "      instance_dict['proba'] = proba[0]\n",
    "      instance_dict['nr_events'] = nr_length\n",
    "      instance_dict['pred_type'] = pred_type\n",
    "\n",
    "      fp_list.append(instance_dict)\n",
    "    else:\n",
    "      print(\"ERROR: Prediction doesn't fit into any category. Check code.\")\n",
    "\n",
    "    preds.append(int(pred))\n",
    "    probas.extend(proba)\n",
    "    pred_types.append(pred_type)\n",
    "    iteration+=1\n",
    "\n",
    "for each in preds:\n",
    "    preds_all.append(each)\n",
    "probas_all.extend(probas)\n",
    "pred_types_all.extend(pred_types)\n",
    "\n",
    "tn_path = os.path.join(PATH, \"%s/%s_%s/instances/true_neg_bucket_%s_backup.pickle\" % (dataset_ref, cls_method, method_name, bucket))\n",
    "tp_path = os.path.join(PATH, \"%s/%s_%s/instances/true_pos_bucket_%s_backup.pickle\" % (dataset_ref, cls_method, method_name, bucket))\n",
    "fn_path = os.path.join(PATH, \"%s/%s_%s/instances/false_neg_bucket_%s_backup.pickle\" % (dataset_ref, cls_method, method_name, bucket))\n",
    "fp_path = os.path.join(PATH, \"%s/%s_%s/instances/false_pos_bucket_%s_backup.pickle\" % (dataset_ref, cls_method, method_name, bucket))\n",
    "\n",
    "with open (tn_path, 'wb') as f:\n",
    "  pickle.dump(tn_list, f)\n",
    "with open (tp_path, 'wb') as f:\n",
    "  pickle.dump(tp_list, f)\n",
    "with open (fn_path, 'wb') as f:\n",
    "  pickle.dump(fn_list, f)\n",
    "with open (fp_path, 'wb') as f:\n",
    "  pickle.dump(fp_list, f)\n",
    "\n",
    "#Save training data\n",
    "print(\"saving data...\")\n",
    "X_train_path = os.path.join(PATH, \"%s/%s_%s/train_data/bucket_%s_prefixes_backup.pickle\" % (dataset_ref, cls_method, method_name, bucket))\n",
    "Y_train_path = os.path.join(PATH, \"%s/%s_%s/train_data/bucket_%s_labels_backup.pickle\" % (dataset_ref, cls_method, method_name, bucket))\n",
    "with open(X_train_path, 'wb') as f:\n",
    "  pickle.dump(dt_train_bucket, f)\n",
    "with open(Y_train_path, 'wb') as f:\n",
    "  pickle.dump(train_y, f)\n",
    "\n",
    "#Save testing data\n",
    "X_test_path = os.path.join(PATH, \"%s/%s_%s/test_data/bucket_%s_prefixes_backup.pickle\" % (dataset_ref, cls_method, method_name, bucket))\n",
    "Y_test_path = os.path.join(PATH, \"%s/%s_%s/test_data/bucket_%s_labels_backup.pickle\" % (dataset_ref, cls_method, method_name, bucket))\n",
    "with open(X_test_path, 'wb') as f:\n",
    "    pickle.dump(dt_test_bucket, f)\n",
    "with open(Y_test_path, 'wb') as f:\n",
    "    pickle.dump(test_y_group, f)\n",
    "\n",
    "acc = accuracy_score(test_y_all, preds_all)\n",
    "roc_auc = roc_auc_score(test_y_all, preds_all)\n",
    "print(\"Accuracy:\", acc, \"\\tROCAUC:\", roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep = dt_test_bucket.shape[0] - (dt_test_bucket.shape[0]%args['batch_size'])\n",
    "dt_test_bucket_new = dt_test_bucket[:keep, :, :]\n",
    "test_y_new = test_y[:keep]\n",
    "#acc = accuracy_score(test_y_all, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = cls.predict(dt_test_bucket_new, batch_size = args['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_new = [np.argmax(proba, axis = -1) for proba in preds]\n",
    "preds_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = accuracy_score(test_y_new, preds_new)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion = confusion_matrix(preds_new, test_y_new)\n",
    "sns.heatmap(confusion, annot=True, cmap='viridis')#, categories = ['Negative', 'Positive'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open (\"%s/%s_%s/cls/params.pickle\"% (dataset_ref, cls_method, method_name), 'rb') as f:\n",
    "#    params = pickle.load(f)\n",
    "\n",
    "#params"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "bpic2012_stability_generate_data.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
