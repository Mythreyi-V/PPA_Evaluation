{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "import sys\n",
    "#PATH = '/content/drive/My Drive/PPM_Stability/'\n",
    "PATH = \"C:/Users/velmurug/Documents/Stability Experiments/benchmark_interpretability/PPM_Stability/\"\n",
    "#PATH = \"C:/Users/Mythreyi/Documents/GitHub/Stability-Experiments/benchmark_interpretability/PPM_Stability/\"\n",
    "#PATH = \"C:/Users/mythr/Documents/GitHub/Stability-Experiments/benchmark_interpretability/PPM_Stability/\"\n",
    "sys.path.append(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost==1.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (1.0.0)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (from xgboost==1.0.0) (1.4.1)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from xgboost==1.0.0) (1.18.5)\n"
     ]
    }
   ],
   "source": [
    "#!pip install lime\n",
    "#!pip install shap\n",
    "#!pip install pandas==0.19.2\n",
    "!pip install xgboost==1.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import EncoderFactory\n",
    "from DatasetManager import DatasetManager\n",
    "import BucketFactory\n",
    "import stability as st #Nogueira, Sechidis, Brown.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "from sys import argv\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import joblib\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "from lime import submodular_pick;\n",
    "\n",
    "import shap\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def imp_df(column_names, importances):\n",
    "        df = pd.DataFrame({'feature': column_names,\n",
    "                       'feature_importance': importances}) \\\n",
    "           .sort_values('feature_importance', ascending = False) \\\n",
    "           .reset_index(drop = True)\n",
    "        return df\n",
    "\n",
    "# plotting a feature importance dataframe (horizontal barchart)\n",
    "def var_imp_plot(imp_df, title, num_feat):\n",
    "        imp_df.columns = ['feature', 'feature_importance']\n",
    "        b= sns.barplot(x = 'feature_importance', y = 'feature', data = imp_df.head(num_feat), orient = 'h', palette=\"Blues_r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_global_explanations(train_X,train_Y, cls, feature_combiner):\n",
    "    \n",
    "    print(\"The number of testing instances is \",len(train_Y))\n",
    "    print(\"The total number of columns is\",train_X.shape[1]);\n",
    "    print(\"The total accuracy is \",cls.score(train_X,train_Y));\n",
    "       \n",
    "    sns.set(rc={'figure.figsize':(10,10), \"font.size\":18,\"axes.titlesize\":18,\"axes.labelsize\":18})\n",
    "    sns.set\n",
    "    feat_names = feature_combiner.get_feature_names()\n",
    "    base_imp = imp_df(feat_names, cls.feature_importances_)\n",
    "    base_imp.head(15)\n",
    "    var_imp_plot(base_imp, 'Feature importance using XGBoost', 15)\n",
    "    return base_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime import submodular_pick\n",
    "def generate_lime_explanations(explainer,test_xi, cls,test_y, submod=False, test_all_data=None, max_feat = 10):\n",
    "    \n",
    "    #print(\"Actual value \", test_y)\n",
    "    exp = explainer.explain_instance(test_xi, \n",
    "                                 cls.predict_proba, num_features=max_feat, labels=[0,1])\n",
    "    \n",
    "    return exp\n",
    "        \n",
    "    if submod==True:\n",
    "        sp_obj=submodular_pick.SubmodularPick(explainer, test_all_data, cls.predict_proba, \n",
    "                                      sample_size=20, num_features=num_features,num_exps_desired=4)\n",
    "        [exp.as_pyplot_figure(label=exp.available_labels()[0]) for exp in sp_obj.sp_explanations];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_samples(shap_explainer, iterations, row, features, top = None):\n",
    "    length = len(features)\n",
    "    \n",
    "    exp = []\n",
    "    rel_exp = []\n",
    "    \n",
    "    for j in range(iterations):\n",
    "        \n",
    "        #if shap_type == \"kernel\":\n",
    "        #    shap_explainer = shap.KernelExplainer(cls.predict, trainingsample)\n",
    "        #elif shap_type == \"tree\":\n",
    "        #    shap_explainer = shap.TreeExplainer(cls)\n",
    "        #elif shap_type == \"deep\":\n",
    "        #    shap_explainer = shap.DeepExplainer(cls, background)\n",
    "        \n",
    "        #print(X_test_frame.loc[row])\n",
    "        shap_values = shap_explainer.shap_values(row)\n",
    "        #print(shap_values)\n",
    "\n",
    "        importances = []\n",
    "        \n",
    "        if type(shap_explainer) == shap.explainers.kernel.KernelExplainer:\n",
    "            for i in range(length):\n",
    "                feat = features[i]\n",
    "                shap_val = shap_values[0][i]\n",
    "                abs_val = abs(shap_values[0][i])\n",
    "                entry = (feat, shap_val, abs_val)\n",
    "                importances.append(entry)\n",
    "                \n",
    "        elif type(shap_explainer) == shap.explainers.tree.TreeExplainer:\n",
    "            for i in range(length):\n",
    "                feat = features[i]\n",
    "                shap_val = shap_values[0][i]\n",
    "                abs_val = abs(shap_values[0][i])\n",
    "                entry = (feat, shap_val, abs_val)\n",
    "                importances.append(entry)\n",
    "        \n",
    "        elif type(shap_explainer) == shap.explainers.deep.DeepExplainer:\n",
    "            for i in range(length):\n",
    "                feat = features[i]\n",
    "                shap_val = shap_values[0][0][i]\n",
    "                abs_val = abs(shap_values[0][0][i])\n",
    "                entry = (feat, shap_val, abs_val)\n",
    "                importances.append(entry)\n",
    "        \n",
    "        importances.sort(key=lambda tup: tup[2], reverse = True)\n",
    "        \n",
    "        exp.append(importances)\n",
    "\n",
    "        rel_feat = []\n",
    "\n",
    "        if top != None:\n",
    "            for i in range(top):\n",
    "                feat = importances[i]\n",
    "                if feat[2] > 0:\n",
    "                    rel_feat.append(feat)\n",
    "\n",
    "            rel_exp.append(rel_feat)\n",
    "        else:\n",
    "            rel_exp = exp\n",
    "        \n",
    "    return exp, rel_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dispersal(weights, features):\n",
    "    feat_len = len(features)\n",
    "    #print(feat_len)\n",
    "    weights_by_feat = []\n",
    "       \n",
    "    for i in list(range(feat_len)):\n",
    "        feat_weight = []\n",
    "        for iteration in weights:\n",
    "            feat_weight.append(iteration[i])\n",
    "        weights_by_feat.append(feat_weight)\n",
    "        \n",
    "    #for iteration in weights:\n",
    "     #   for val in iteration:\n",
    "      #      idx = iteration.index(val)\n",
    "       #     print(idx)\n",
    "        #    weights_by_feat[idx].append(val)\n",
    "    \n",
    "    dispersal = []\n",
    "    dispersal_no_outlier = []\n",
    "    \n",
    "    for each in weights_by_feat:\n",
    "        #print(\"Feature\", weights_by_feat.index(each)+1)\n",
    "        mean = np.mean(each)\n",
    "        std_dev = np.std(each)\n",
    "        var = std_dev**2\n",
    "        \n",
    "        if mean == 0:\n",
    "            dispersal.append(0)\n",
    "            dispersal_no_outlier.append(0)\n",
    "        #print(each)\n",
    "        else:\n",
    "            #dispersal with outliers\n",
    "            rel_var = var/abs(mean)\n",
    "            dispersal.append(rel_var)\n",
    "            \n",
    "            #dispersal without outliers - remove anything with a z-score higher\n",
    "            #than 3 (more than 3 standard deviations away from the mean)\n",
    "            rem_outlier = []\n",
    "            z_scores = stats.zscore(each)\n",
    "            #print(z_scores)\n",
    "            #print(\"New list:\")\n",
    "            for i in range(len(z_scores)):\n",
    "                #print(each[i],\":\",z_scores[i])\n",
    "                if -3 < z_scores[i] < 3:\n",
    "                    rem_outlier.append(each[i])\n",
    "                #print(rem_outlier)\n",
    "            if rem_outlier != []:\n",
    "                new_mean = np.mean(rem_outlier)\n",
    "                if new_mean == 0:\n",
    "                    dispersal_no_outlier.append(0)\n",
    "                else:\n",
    "                    new_std = np.std(rem_outlier)\n",
    "                    new_var = new_std**2\n",
    "                    new_rel_var = new_var/abs(new_mean)\n",
    "                    dispersal_no_outlier.append(new_rel_var)\n",
    "            else:\n",
    "                dispersal_no_outlier.append(rel_var)\n",
    "    #print(dispersal_no_outlier)\n",
    "    return dispersal, dispersal_no_outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "['bpic2012_accepted']"
      ],
      "text/plain": [
       "['bpic2012_accepted']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_ref = \"bpic2012\"\n",
    "params_dir = PATH + \"params\"\n",
    "results_dir = \"results\"\n",
    "bucket_method = \"single\"\n",
    "cls_encoding = \"agg\"\n",
    "cls_method = \"xgboost\"\n",
    "\n",
    "gap = 1\n",
    "n_iter = 1\n",
    "\n",
    "method_name = \"%s_%s\"%(bucket_method, cls_encoding)\n",
    "\n",
    "generate_samples = False\n",
    "generate_lime = True\n",
    "generate_kernel_shap = False\n",
    "generate_model_shap = True\n",
    "\n",
    "sample_size = 2\n",
    "exp_iter = 1\n",
    "max_feat = 10\n",
    "\n",
    "dataset_ref_to_datasets = {\n",
    "    #\"bpic2011\": [\"bpic2011_f%s\"%formula for formula in range(1,5)],\n",
    "    \"bpic2015\": [\"bpic2015_%s_f2\"%(municipality) for municipality in range(5,6)],\n",
    "    \"bpic2017\" : [\"bpic2017_accepted\"],\n",
    "    \"bpic2012\" : [\"bpic2012_accepted\"]\n",
    "    #\"insurance\": [\"insurance_activity\", \"insurance_followup\"],\n",
    "    #\"sepsis_cases\": [\"sepsis_cases_1\", \"sepsis_cases_2\", \"sepsis_cases_4\"]\n",
    "}\n",
    "\n",
    "datasets = [dataset_ref] if dataset_ref not in dataset_ref_to_datasets else dataset_ref_to_datasets[dataset_ref]\n",
    "\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket 1\n"
     ]
    }
   ],
   "source": [
    "for dataset_name in datasets:\n",
    "\n",
    "    dataset_manager = DatasetManager(dataset_name)\n",
    "\n",
    "    for ii in range(n_iter):\n",
    "        num_buckets = range(len([name for name in os.listdir(os.path.join(PATH,'%s/%s_%s/models'% (dataset_ref, cls_method, method_name)))]))\n",
    "\n",
    "        all_shap_changes = []\n",
    "        all_lens = []\n",
    "        all_probas = []\n",
    "        all_case_ids = []\n",
    "        \n",
    "        pos_shap_changes = []\n",
    "        pos_probas = []\n",
    "        pos_nr_events = []\n",
    "        pos_case_ids = []\n",
    "                \n",
    "        neg_shap_changes = []\n",
    "        neg_probas = []\n",
    "        neg_nr_events = []\n",
    "        neg_case_ids = []\n",
    "            \n",
    "        for bucket in list(num_buckets):\n",
    "            bucketID = bucket+1\n",
    "            print ('Bucket', bucketID)\n",
    "\n",
    "            #import everything needed to sort and predict\n",
    "            feat_comb_path = os.path.join(PATH, \"%s/%s_%s/bucketers_and_encoders/feature_combiner_bucket_%s.joblib\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "            cls_path = os.path.join(PATH, \"%s/%s_%s/models/cls_bucket_%s.joblib\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "            cls = joblib.load(cls_path)\n",
    "            feature_combiner = joblib.load(feat_comb_path)\n",
    "\n",
    "            #import data for bucket\n",
    "            X_train_path = os.path.join(PATH, \"%s/%s_%s/train_data/bucket_%s_prefixes.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "            Y_train_path = os.path.join(PATH, \"%s/%s_%s/train_data/bucket_%s_labels.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "\n",
    "            with open (X_train_path, 'rb') as f:\n",
    "                dt_train_bucket = pickle.load(f)\n",
    "            with open (Y_train_path, 'rb') as f:\n",
    "                train_y = pickle.load(f)\n",
    "            #with open (X_test_path, 'rb') as f:\n",
    "            #    dt_test_bucket = pickle.load(f)\n",
    "            #with open (Y_test_path, 'rb') as f:\n",
    "            #    test_y = pickle.load(f)\n",
    "\n",
    "            #import previously identified samples\n",
    "            tn_path = os.path.join(PATH, \"%s/%s_%s/samples/true_neg_bucket_%s_.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "            tp_path = os.path.join(PATH, \"%s/%s_%s/samples/true_pos_bucket_%s_.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "            #fn_path = os.path.join(PATH, \"%s/%s_%s/samples/false_neg_bucket_%s_.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "            #fp_path = os.path.join(PATH, \"%s/%s_%s/samples/false_pos_bucket_%s_.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "\n",
    "            sample_instances = []\n",
    "\n",
    "            with open (tn_path, 'rb') as f:\n",
    "                tn_list = pickle.load(f)\n",
    "            with open (tp_path, 'rb') as f:\n",
    "                tp_list = pickle.load(f)\n",
    "            #with open (fn_path, 'rb') as f:\n",
    "            #    fn_list = pickle.load(f)\n",
    "            #with open (fp_path, 'rb') as f:\n",
    "            #    fp_list = pickle.load(f)\n",
    "\n",
    "            #save results to a list\n",
    "            sample_instances.append(tn_list)\n",
    "            sample_instances.append(tp_list)\n",
    "            #sample_instances.append(fn_list)\n",
    "            #sample_instances.append(fp_list)\n",
    "                    \n",
    "            tree_explainer = shap.TreeExplainer(cls)\n",
    "            feat_list = feature_combiner.get_feature_names()\n",
    "            type_list = ['True Negatives', 'True Positives', 'False Negatives', 'False Positives']\n",
    "\n",
    "\n",
    "            for i in range(len(sample_instances)):\n",
    "                changes = []\n",
    "                probas = []\n",
    "                nr_events = []\n",
    "                case_ids = []\n",
    "            \n",
    "                for j in range(len(sample_instances[i])):\n",
    "                    print(\"Category %s of %s. Instance %s of %s\" %(i+1, len(sample_instances), j+1, len(sample_instances[i])))\n",
    "                    each = sample_instances[i][j]\n",
    "\n",
    "                    ind = each['predicted']\n",
    "                    case_ids.append(each['caseID'])\n",
    "                    p1 = each['proba']\n",
    "                    probas.append(p1)\n",
    "                    nr_events.append(each['nr_events'])\n",
    "                    input_ = each['input']\n",
    "\n",
    "                    test_x_group = feature_combiner.fit_transform(input_) \n",
    "                    #test_x=np.transpose(test_x_group[0])\n",
    "                    #print(test_x)\n",
    "                    #print(p1)\n",
    "\n",
    "                    exp, rel_exp = create_samples(tree_explainer, exp_iter, test_x_group, feat_list, top = max_feat)\n",
    "                    #print(rel_exp)\n",
    "\n",
    "                    feat_name = rel_exp[0][0][0]\n",
    "                    #print(feat_name)\n",
    "                    feat_loc = feat_list.index(feat_name)\n",
    "\n",
    "                    if test_x_group[0][feat_loc] == 0 or test_x_group[0][feat_loc] < 0:\n",
    "                        new_val = 3\n",
    "                    else:\n",
    "                        new_val = 0\n",
    "\n",
    "                    alt_x = test_x_group\n",
    "                    alt_x[0][feat_loc] = new_val\n",
    "\n",
    "                    p2 = cls.predict_proba(alt_x)[0][ind]\n",
    "                    #print(p2)\n",
    "\n",
    "                    diff = p2-p1\n",
    "                    changes.append(abs(diff))\n",
    "                    #print(changes)\n",
    "                    \n",
    "                    if ind == 0:\n",
    "                        pos_shap_changes.append(abs(diff))\n",
    "                        pos_probas.append(p1)\n",
    "                        pos_nr_events.append(each['nr_events'])\n",
    "                        pos_case_ids.append(each['caseID'])\n",
    "                    else:\n",
    "                        neg_shap_changes.append(abs(diff))\n",
    "                        neg_probas.append(p1)\n",
    "                        neg_nr_events.append(each['nr_events'])\n",
    "                        neg_case_ids.append(each['caseID'])\n",
    "\n",
    "                fig, ax = plt.subplots()\n",
    "                ax.plot(case_ids, changes, 'ro', label = \"SHAP\")\n",
    "                ax.set_xlabel(\"Prefix Length\")\n",
    "                ax.set_ylabel(\"Change in prediction probability\")\n",
    "                #ax.legend(frameon = False, bbox_to_anchor=(1, 1), loc = 'upper left')\n",
    "                plt.yticks(np.arange(0,1, 0.1))\n",
    "                plt.title(\"Prefix length and change in prediction probability - %s (Bucket %s)\" %(type_list[i], bucketID))\n",
    "                plt.show()\n",
    "\n",
    "                fig2, ax2 = plt.subplots()\n",
    "                ax2.plot(nr_events, changes, 'ro', label = \"SHAP\")\n",
    "                ax2.set_xlabel(\"Prediction Probability\")\n",
    "                ax2.set_ylabel(\"Change in prediction probability\")\n",
    "                #ax2.legend(frameon = False, bbox_to_anchor=(1, 1), loc = 'upper left')\n",
    "                plt.yticks(np.arange(0,1, 0.1))\n",
    "                plt.title(\"Prediction probability and change in prediction probability - %s (Bucket %s)\" %(type_list[i], bucketID))\n",
    "                plt.show()\n",
    "\n",
    "                all_shap_changes.extend(changes)\n",
    "                all_lens.extend(nr_events)\n",
    "                all_probas.extend(probas)\n",
    "                all_case_ids.extend(case_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(all_lens, all_shap_changes, 'ro', label = \"SHAP\")\n",
    "ax.set_xlabel(\"Prefix Length\")\n",
    "ax.set_ylabel(\"Change in prediction probability\")\n",
    "#ax.legend(frameon = False, bbox_to_anchor=(1, 1), loc = 'upper left')\n",
    "plt.yticks(np.arange(0,1, 0.1))\n",
    "plt.title(\"Prefix length and change in prediction probability - All\")\n",
    "plt.show()\n",
    "\n",
    "fig2, ax2 = plt.subplots()\n",
    "ax2.plot(all_probas, all_shap_changes, 'ro', label = \"SHAP\")\n",
    "ax2.set_xlabel(\"Prediction Probability\")\n",
    "ax2.set_ylabel(\"Change in prediction probability\")\n",
    "#ax2.legend(frameon = False, bbox_to_anchor=(1, 1), loc = 'upper left')\n",
    "plt.yticks(np.arange(0,1, 0.1))\n",
    "plt.title(\"Prediction probability and change in prediction probability - All\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2, ax2 = plt.subplots()\n",
    "ax2.plot(pos_probas, pos_shap_changes, 'ro', label = \"SHAP\")\n",
    "ax2.set_xlabel(\"Prediction Probability\")\n",
    "ax2.set_ylabel(\"Change in prediction probability\")\n",
    "#ax2.legend(frameon = False, bbox_to_anchor=(1, 1), loc = 'upper left')\n",
    "plt.yticks(np.arange(0,1, 0.1))\n",
    "plt.title(\"Prediction probability and change in prediction probability - Positives\")\n",
    "plt.show()\n",
    "\n",
    "fig2, ax2 = plt.subplots()\n",
    "ax2.plot(neg_probas, neg_shap_changes, 'ro', label = \"SHAP\")\n",
    "ax2.set_xlabel(\"Prediction Probability\")\n",
    "ax2.set_ylabel(\"Change in prediction probability\")\n",
    "#ax2.legend(frameon = False, bbox_to_anchor=(1, 1), loc = 'upper left')\n",
    "plt.yticks(np.arange(0,1, 0.1))\n",
    "plt.title(\"Prediction probability and change in prediction probability - Negatives\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for dataset_name in datasets:\n",
    "\n",
    "    dataset_manager = DatasetManager(dataset_name)\n",
    "\n",
    "    for ii in range(n_iter):\n",
    "        num_buckets = range(len([name for name in os.listdir(os.path.join(PATH,'%s/%s_%s/models'% (dataset_ref, cls_method, method_name)))]))\n",
    "        \n",
    "        all_lime_changes = []\n",
    "        all_lens = []\n",
    "        all_probas = []\n",
    "        all_case_ids = []\n",
    "        \n",
    "        pos_lime_changes = []\n",
    "        pos_probas = []\n",
    "        pos_nr_events = []\n",
    "        pos_case_ids = []\n",
    "                \n",
    "        neg_lime_changes = []\n",
    "        neg_probas = []\n",
    "        neg_nr_events = []\n",
    "        neg_case_ids = []\n",
    "\n",
    "        for bucket in list(num_buckets):\n",
    "            bucketID = bucket+1\n",
    "            print ('Bucket', bucketID)\n",
    "\n",
    "            #import everything needed to sort and predict\n",
    "            feat_comb_path = os.path.join(PATH, \"%s/%s_%s/bucketers_and_encoders/feature_combiner_bucket_%s.joblib\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "            cls_path = os.path.join(PATH, \"%s/%s_%s/models/cls_bucket_%s.joblib\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "            cls = joblib.load(cls_path)\n",
    "            feature_combiner = joblib.load(feat_comb_path)\n",
    "\n",
    "            #import data for bucket\n",
    "            X_train_path = os.path.join(PATH, \"%s/%s_%s/train_data/bucket_%s_prefixes.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "            Y_train_path = os.path.join(PATH, \"%s/%s_%s/train_data/bucket_%s_labels.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "\n",
    "            with open (X_train_path, 'rb') as f:\n",
    "                dt_train_bucket = pickle.load(f)\n",
    "            with open (Y_train_path, 'rb') as f:\n",
    "                train_y = pickle.load(f)\n",
    "            #with open (X_test_path, 'rb') as f:\n",
    "            #    dt_test_bucket = pickle.load(f)\n",
    "            #with open (Y_test_path, 'rb') as f:\n",
    "            #    test_y = pickle.load(f)\n",
    "\n",
    "            #import previously identified samples\n",
    "            tn_path = os.path.join(PATH, \"%s/%s_%s/samples/true_neg_bucket_%s_.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "            tp_path = os.path.join(PATH, \"%s/%s_%s/samples/true_pos_bucket_%s_.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "            #fn_path = os.path.join(PATH, \"%s/%s_%s/samples/false_neg_bucket_%s_.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "            #fp_path = os.path.join(PATH, \"%s/%s_%s/samples/false_pos_bucket_%s_.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "\n",
    "            sample_instances = []\n",
    "\n",
    "            with open (tn_path, 'rb') as f:\n",
    "                tn_list = pickle.load(f)\n",
    "            with open (tp_path, 'rb') as f:\n",
    "                tp_list = pickle.load(f)\n",
    "            #with open (fn_path, 'rb') as f:\n",
    "            #    fn_list = pickle.load(f)\n",
    "            #with open (fp_path, 'rb') as f:\n",
    "            #    fp_list = pickle.load(f)\n",
    "\n",
    "            #save results to a list\n",
    "            sample_instances.append(tn_list)\n",
    "            sample_instances.append(tp_list)\n",
    "            #sample_instances.append(fn_list)\n",
    "            #sample_instances.append(fp_list)\n",
    "\n",
    "            #get the training data as a matrix\n",
    "            trainingdata = feature_combiner.fit_transform(dt_train_bucket);\n",
    "            #importance = generate_global_explanations(trainingdata,train_y, cls, feature_combiner)\n",
    "\n",
    "            feat_list = feature_combiner.get_feature_names()\n",
    "            class_names=['regular','deviant']# regular is 0, deviant is 1, 0 is left, 1 is right\n",
    "            lime_explainer = lime.lime_tabular.LimeTabularExplainer(trainingdata, feature_names = feat_list, \n",
    "                                                                    class_names=class_names, discretize_continuous=True)\n",
    "            type_list = ['True Negatives', 'True Positives', 'False Negatives', 'False Positives']\n",
    "\n",
    "            for i in list(range(len(sample_instances))):\n",
    "                changes = []\n",
    "                probas = []\n",
    "                nr_events = []\n",
    "                case_ids = []\n",
    "\n",
    "                for j in list(range(len(sample_instances[i]))):\n",
    "                    print(\"Category %s of %s. Instance %s of %s\" %(i+1, len(sample_instances), j+1, len(sample_instances[i])))\n",
    "                    each = sample_instances[i][j]\n",
    "\n",
    "                    ind = each['predicted']\n",
    "                    case_ids.append(each['caseID'])\n",
    "                    p1 = each['proba']\n",
    "                    probas.append(p1)\n",
    "                    nr_events.append(each['nr_events'])\n",
    "                    input_ = each['input']\n",
    "\n",
    "                    test_x_group = feature_combiner.fit_transform(input_) \n",
    "                    test_x=np.transpose(test_x_group[0])\n",
    "                    #print(test_x)\n",
    "                    #print(p1)\n",
    "\n",
    "                    lime_exp = generate_lime_explanations(lime_explainer, test_x, cls, input_, max_feat = 10)\n",
    "                    explanation = lime_exp.as_list()\n",
    "\n",
    "                    parts = explanation[0][0].split(' ')\n",
    "\n",
    "                    if len(parts) > 3:\n",
    "                        two_bounds = True\n",
    "                    else:\n",
    "                        two_bounds = False\n",
    "                        \n",
    "                    for n in range(len(feat_list)):\n",
    "                        if feat_list[n] in explanation[0][0]:\n",
    "                            feat_loc = n\n",
    "\n",
    "                    #feat_name = parts[-3]\n",
    "                    #if feat_name not in feat_list:\n",
    "                    #    feat_name = parts[-4]+\" \"+parts[-3]\n",
    "                    #feat_loc = feat_list.index(feat_name)\n",
    "\n",
    "                    #if two_bounds:\n",
    "                    #    l_bound = float(parts[0])\n",
    "                    #    u_bound = float(parts[-1])\n",
    "                    #else:\n",
    "                    bound = float(parts[-1])\n",
    "\n",
    "                    if parts[-2] == \"<=\" or parts[-2] == \"<\":\n",
    "                        new_val = bound + 3\n",
    "                    else:\n",
    "                        new_val = bound - 3\n",
    "\n",
    "                    alt_x = test_x_group\n",
    "                    alt_x[0][feat_loc] = new_val\n",
    "\n",
    "                    p2 = cls.predict_proba(alt_x)[0][ind]\n",
    "                    #print(p2)\n",
    "\n",
    "                    diff = p2-p1\n",
    "                    changes.append(abs(diff))\n",
    "                    \n",
    "                    if ind == 0:\n",
    "                        pos_lime_changes.append(abs(diff))\n",
    "                        pos_probas.append(p1)\n",
    "                        pos_nr_events.append(each['nr_events'])\n",
    "                        pos_case_ids.append(each['caseID'])\n",
    "                    else:\n",
    "                        neg_lime_changes.append(abs(diff))\n",
    "                        neg_probas.append(p1)\n",
    "                        neg_nr_events.append(each['nr_events'])\n",
    "                        neg_case_ids.append(each['caseID'])\n",
    "\n",
    "                fig, ax = plt.subplots()\n",
    "                ax.plot(nr_events, changes, 'bo', label = \"LIME\")\n",
    "                ax.set_xlabel(\"Prefix Length\")\n",
    "                ax.set_ylabel(\"Change in prediction probability\")\n",
    "                #ax.legend(frameon = False, bbox_to_anchor=(1, 1), loc = 'upper left')\n",
    "                #plt.yticks(np.arange(0,1, 0.1))\n",
    "                plt.title(\"Prefix length and change in prediction probability - %s (Bucket %s)\" %(type_list[i], bucketID))\n",
    "                plt.show()\n",
    "\n",
    "                fig2, ax2 = plt.subplots()\n",
    "                ax2.plot(probas, changes, 'bo', label = \"LIME\")\n",
    "                ax2.set_xlabel(\"Prediction Probability\")\n",
    "                ax2.set_ylabel(\"Change in prediction probability\")\n",
    "                #ax2.legend(frameon = False, bbox_to_anchor=(1, 1), loc = 'upper left')\n",
    "                plt.yticks(np.arange(0,1, 0.1))\n",
    "                plt.title(\"Prediction probability and change in prediction probability - %s (Bucket %s)\" %(type_list[i], bucketID))\n",
    "                plt.show()\n",
    "\n",
    "                all_lime_changes.extend(changes)\n",
    "                all_lens.extend(nr_events)\n",
    "                all_probas.extend(probas)\n",
    "                all_case_ids.extend(case_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(all_lens, all_lime_changes, 'bo', label = \"LIME\")\n",
    "ax.set_xlabel(\"Prefix Length\")\n",
    "ax.set_ylabel(\"Change in prediction probability\")\n",
    "#ax.legend(frameon = False, bbox_to_anchor=(1, 1), loc = 'upper left')\n",
    "#plt.yticks(np.arange(0,1, 0.1))\n",
    "plt.title(\"Prefix length and change in prediction probability - All\")\n",
    "plt.show()\n",
    "\n",
    "fig2, ax2 = plt.subplots()\n",
    "ax2.plot(all_probas, all_lime_changes, 'bo', label = \"LIME\")\n",
    "ax2.set_xlabel(\"Prediction Probability\")\n",
    "ax2.set_ylabel(\"Change in prediction probability\")\n",
    "#ax2.legend(frameon = False, bbox_to_anchor=(1, 1), loc = 'upper left')\n",
    "#plt.yticks(np.arange(0,1, 0.1))\n",
    "plt.title(\"Prediction probability and change in prediction probability - All\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2, ax2 = plt.subplots()\n",
    "ax2.plot(pos_probas, pos_lime_changes, 'bo', label = \"LIME\")\n",
    "ax2.set_xlabel(\"Prediction Probability\")\n",
    "ax2.set_ylabel(\"Change in prediction probability\")\n",
    "#ax2.legend(frameon = False, bbox_to_anchor=(1, 1), loc = 'upper left')\n",
    "plt.yticks(np.arange(0,1, 0.1))\n",
    "plt.title(\"Prediction probability and change in prediction probability - Positives\")\n",
    "plt.show()\n",
    "\n",
    "fig2, ax2 = plt.subplots()\n",
    "ax2.plot(neg_probas, neg_lime_changes, 'bo', label = \"LIME\")\n",
    "ax2.set_xlabel(\"Prediction Probability\")\n",
    "ax2.set_ylabel(\"Change in prediction probability\")\n",
    "#ax2.legend(frameon = False, bbox_to_anchor=(1, 1), loc = 'upper left')\n",
    "plt.yticks(np.arange(0,1, 0.1))\n",
    "plt.title(\"Prediction probability and change in prediction probability - Negatives\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lime_fid_score = sum(all_lime_changes)/len(all_lime_changes)\n",
    "shap_fid_score = sum(all_shap_changes)/len(all_shap_changes)\n",
    "\n",
    "print(\"LIME Fidelity Score: %s \\nSHAP Fidelity Score: %s\" %(lime_fid_score, shap_fid_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lime_fid_score = sum(pos_lime_changes)/len(pos_lime_changes)\n",
    "shap_fid_score = sum(pos_shap_changes)/len(pos_shap_changes)\n",
    "\n",
    "print(\"Positive predictions: \\nLIME Fidelity Score: %s \\nSHAP Fidelity Score: %s\" %(lime_fid_score, shap_fid_score))\n",
    "\n",
    "lime_fid_score = sum(neg_lime_changes)/len(neg_lime_changes)\n",
    "shap_fid_score = sum(neg_shap_changes)/len(neg_shap_changes)\n",
    "\n",
    "print(\"Negative predictions: \\nLIME Fidelity Score: %s \\nSHAP Fidelity Score: %s\" %(lime_fid_score, shap_fid_score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
