{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "import sys\n",
    "#PATH = '/content/drive/My Drive/PPM_Stability/'\n",
    "#PATH = \"C:/Users/velmurug/Documents/Stability Experiments/benchmark_interpretability/PPM_Stability/\"\n",
    "#PATH = \"C:/Users/Mythreyi/Documents/GitHub/Stability-Experiments/benchmark_interpretability/PPM_Stability/\"\n",
    "PATH = \"C:/Users/mythr/Documents/GitHub/Stability-Experiments/benchmark_interpretability/PPM_Stability/\"\n",
    "sys.path.append(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DatasetManager import DatasetManager\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from sys import argv\n",
    "import pickle\n",
    "import random\n",
    "import joblib\n",
    "\n",
    "import statistics\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bpic2012_accepted']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_ref = \"bpic2012\"\n",
    "params_dir = PATH + \"params\"\n",
    "results_dir = \"results\"\n",
    "bucket_method = \"single\"\n",
    "cls_encoding = \"agg\"\n",
    "cls_method = \"xgboost\"\n",
    "\n",
    "gap = 1\n",
    "n_iter = 1\n",
    "\n",
    "method_name = \"%s_%s\"%(bucket_method, cls_encoding)\n",
    "\n",
    "generate_samples = False\n",
    "generate_lime = True\n",
    "generate_kernel_shap = False\n",
    "generate_model_shap = True\n",
    "\n",
    "sample_size = 2\n",
    "exp_iter = 10\n",
    "max_feat = 10\n",
    "max_prefix = 25\n",
    "\n",
    "dataset_ref_to_datasets = {\n",
    "    #\"bpic2011\": [\"bpic2011_f%s\"%formula for formula in range(1,5)],\n",
    "    \"bpic2015\": [\"bpic2015_%s_f2\"%(municipality) for municipality in range(5,6)],\n",
    "    \"bpic2017\" : [\"bpic2017_accepted\"],\n",
    "    \"bpic2012\" : [\"bpic2012_accepted\"]\n",
    "    #\"insurance\": [\"insurance_activity\", \"insurance_followup\"],\n",
    "    #\"sepsis_cases\": [\"sepsis_cases_1\", \"sepsis_cases_2\", \"sepsis_cases_4\"]\n",
    "}\n",
    "\n",
    "datasets = [dataset_ref] if dataset_ref not in dataset_ref_to_datasets else dataset_ref_to_datasets[dataset_ref]\n",
    "\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket 1\n"
     ]
    }
   ],
   "source": [
    "for dataset_name in datasets:\n",
    "\n",
    "    dataset_manager = DatasetManager(dataset_name)\n",
    "\n",
    "    for ii in range(n_iter):\n",
    "        num_buckets = len([name for name in os.listdir(os.path.join(PATH,'%s/%s_%s/models'% (dataset_ref, cls_method, method_name)))])\n",
    "\n",
    "        for bucket in range(num_buckets):\n",
    "            bucketID = bucket+1\n",
    "            print ('Bucket', bucketID)\n",
    "\n",
    "            #import everything needed to sort and predict\n",
    "            feat_comb_path = os.path.join(PATH, \"%s/%s_%s/bucketers_and_encoders/feature_combiner_bucket_%s.joblib\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "            cls_path = os.path.join(PATH, \"%s/%s_%s/models/cls_bucket_%s.joblib\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "            cls = joblib.load(cls_path)\n",
    "            feature_combiner = joblib.load(feat_comb_path)\n",
    "\n",
    "            #import previously identified samples\n",
    "            tn_path = os.path.join(PATH, \"%s/%s_%s/samples/true_neg_bucket_%s_.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "            tp_path = os.path.join(PATH, \"%s/%s_%s/samples/true_pos_bucket_%s_.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "            fn_path = os.path.join(PATH, \"%s/%s_%s/samples/false_neg_bucket_%s_.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "            fp_path = os.path.join(PATH, \"%s/%s_%s/samples/false_pos_bucket_%s_.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "\n",
    "            sample_instances = []\n",
    "\n",
    "            with open (tn_path, 'rb') as f:\n",
    "                tn_list = pickle.load(f)\n",
    "            with open (tp_path, 'rb') as f:\n",
    "                tp_list = pickle.load(f)\n",
    "            with open (fn_path, 'rb') as f:\n",
    "                fn_list = pickle.load(f)\n",
    "            with open (fp_path, 'rb') as f:\n",
    "                fp_list = pickle.load(f)\n",
    "\n",
    "            #save results to a list\n",
    "            sample_instances.append(tn_list)\n",
    "            sample_instances.append(tp_list)\n",
    "            sample_instances.append(fn_list)\n",
    "            sample_instances.append(fp_list)\n",
    "            \n",
    "            \n",
    "            for lis in sample_instances:\n",
    "                for each in lis:\n",
    "                    dispersal_lime = each['lime_weights_dispersal']\n",
    "                    dispersal_shap = each['shap_weights_dispersal']\n",
    "                    adj_dispersal_lime = each['adjusted_lime_weights_dispersal']\n",
    "                    adj_dispersal_shap = each['adjusted_shap_weights_dispersal']\n",
    "                    \n",
    "                    lime_stability = np.mean(dispersal_lime)\n",
    "                    shap_stability = np.mean(dispersal_shap)\n",
    "                    adj_lime_stability = np.mean(adj_dispersal_lime)\n",
    "                    adj_shap_stability = np.mean(adj_dispersal_shap)\n",
    "                    \n",
    "                    each['lime_importance_stability'] = 1-lime_stability\n",
    "                    each['shap_importance_stability'] = 1-shap_stability\n",
    "                    each['adjusted_lime_importance_stability'] = 1-adj_lime_stability\n",
    "                    each['adjusted_shap_importance_stability'] = 1-adj_shap_stability\n",
    "\n",
    "            with open (tn_path, 'wb') as f:\n",
    "                pickle.dump(sample_instances[0], f)\n",
    "            with open (tp_path, 'wb') as f:\n",
    "                pickle.dump(sample_instances[1], f)\n",
    "            with open (fn_path, 'wb') as f:\n",
    "                pickle.dump(sample_instances[2], f)\n",
    "            with open (fp_path, 'wb') as f:\n",
    "                pickle.dump(sample_instances[3], f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(each['lime_weights_dispersal'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
