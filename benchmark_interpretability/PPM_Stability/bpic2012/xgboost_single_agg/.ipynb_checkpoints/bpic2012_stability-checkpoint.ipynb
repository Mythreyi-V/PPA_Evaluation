{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 683,
     "status": "ok",
     "timestamp": 1597835061991,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh6DP4Hn-qpp593Bc6lrayXXcdQv42KAS3zJ-Ay=s64",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "vTbNLZGY9Zjr",
    "outputId": "8f89e413-7d2c-49e0-8040-3f8cd3b3048d"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "import sys\n",
    "#PATH = '/content/drive/My Drive/PPM_Stability/'\n",
    "PATH = \"C:/Users/velmurug/Documents/Stability Experiments/benchmark_interpretability/PPM_Stability/\"\n",
    "#PATH = \"C:/Users/Mythreyi/Documents/GitHub/Stability-Experiments/benchmark_interpretability/PPM_Stability/\"\n",
    "sys.path.append(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 454
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 10776,
     "status": "ok",
     "timestamp": 1597835076495,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh6DP4Hn-qpp593Bc6lrayXXcdQv42KAS3zJ-Ay=s64",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "NOTE47wJ9v8E",
    "outputId": "616caafa-afb8-48e2-bcbb-794684297715"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost==1.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (1.0.0)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from xgboost==1.0.0) (1.18.5)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (from xgboost==1.0.0) (1.4.1)\n"
     ]
    }
   ],
   "source": [
    "#!pip install lime\n",
    "#!pip install shap\n",
    "#!pip install pandas==0.19.2\n",
    "!pip install xgboost==1.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 14587,
     "status": "ok",
     "timestamp": 1597835083465,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh6DP4Hn-qpp593Bc6lrayXXcdQv42KAS3zJ-Ay=s64",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "-wVsWqvt9zzb"
   },
   "outputs": [],
   "source": [
    "import EncoderFactory\n",
    "from DatasetManager import DatasetManager\n",
    "import BucketFactory\n",
    "import stability as st #Nogueira, Sechidis, Brown.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "from sys import argv\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import joblib\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "from lime import submodular_pick;\n",
    "\n",
    "import shap\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11333,
     "status": "ok",
     "timestamp": 1597835083468,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh6DP4Hn-qpp593Bc6lrayXXcdQv42KAS3zJ-Ay=s64",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "NgQh__fq9_xK"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def imp_df(column_names, importances):\n",
    "        df = pd.DataFrame({'feature': column_names,\n",
    "                       'feature_importance': importances}) \\\n",
    "           .sort_values('feature_importance', ascending = False) \\\n",
    "           .reset_index(drop = True)\n",
    "        return df\n",
    "\n",
    "# plotting a feature importance dataframe (horizontal barchart)\n",
    "def var_imp_plot(imp_df, title, num_feat):\n",
    "        imp_df.columns = ['feature', 'feature_importance']\n",
    "        b= sns.barplot(x = 'feature_importance', y = 'feature', data = imp_df.head(num_feat), orient = 'h', palette=\"Blues_r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9499,
     "status": "ok",
     "timestamp": 1597835083469,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh6DP4Hn-qpp593Bc6lrayXXcdQv42KAS3zJ-Ay=s64",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "1X1WzmXc-Hoa"
   },
   "outputs": [],
   "source": [
    "def generate_global_explanations(train_X,train_Y, cls, feature_combiner):\n",
    "    \n",
    "    print(\"The number of testing instances is \",len(train_Y))\n",
    "    print(\"The total number of columns is\",train_X.shape[1]);\n",
    "    print(\"The total accuracy is \",cls.score(train_X,train_Y));\n",
    "       \n",
    "    sns.set(rc={'figure.figsize':(10,10), \"font.size\":18,\"axes.titlesize\":18,\"axes.labelsize\":18})\n",
    "    sns.set\n",
    "    feat_names = feature_combiner.get_feature_names()\n",
    "    base_imp = imp_df(feat_names, cls.feature_importances_)\n",
    "    base_imp.head(15)\n",
    "    var_imp_plot(base_imp, 'Feature importance using XGBoost', 15)\n",
    "    return base_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 827,
     "status": "ok",
     "timestamp": 1597835084306,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh6DP4Hn-qpp593Bc6lrayXXcdQv42KAS3zJ-Ay=s64",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "ss0KwacD-MaC"
   },
   "outputs": [],
   "source": [
    "from lime import submodular_pick\n",
    "def generate_lime_explanations(explainer,test_xi, cls,test_y, submod=False, test_all_data=None, max_feat = 10):\n",
    "    \n",
    "    #print(\"Actual value \", test_y)\n",
    "    exp = explainer.explain_instance(test_xi, \n",
    "                                 cls.predict_proba, num_features=max_feat, labels=[0,1])\n",
    "    \n",
    "    return exp\n",
    "        \n",
    "    if submod==True:\n",
    "        sp_obj=submodular_pick.SubmodularPick(explainer, test_all_data, cls.predict_proba, \n",
    "                                      sample_size=20, num_features=num_features,num_exps_desired=4)\n",
    "        [exp.as_pyplot_figure(label=exp.available_labels()[0]) for exp in sp_obj.sp_explanations];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_samples(shap_explainer, iterations, row, features, top = 10):\n",
    "    length = len(features)\n",
    "    \n",
    "    exp = []\n",
    "    rel_exp = []\n",
    "    \n",
    "    for j in range(iterations):\n",
    "        \n",
    "        #if shap_type == \"kernel\":\n",
    "        #    shap_explainer = shap.KernelExplainer(cls.predict, trainingsample)\n",
    "        #elif shap_type == \"tree\":\n",
    "        #    shap_explainer = shap.TreeExplainer(cls)\n",
    "        #elif shap_type == \"deep\":\n",
    "        #    shap_explainer = shap.DeepExplainer(cls, background)\n",
    "        \n",
    "        #print(X_test_frame.loc[row])\n",
    "        shap_values = shap_explainer.shap_values(row)\n",
    "        #print(shap_values)\n",
    "\n",
    "        importances = []\n",
    "        \n",
    "        if type(shap_explainer) == shap.explainers.kernel.KernelExplainer:\n",
    "            for i in range(length):\n",
    "                feat = features[i]\n",
    "                shap_val = shap_values[0][i]\n",
    "                abs_val = abs(shap_values[0][i])\n",
    "                entry = (feat, shap_val, abs_val)\n",
    "                importances.append(entry)\n",
    "                \n",
    "        elif type(shap_explainer) == shap.explainers.tree.TreeExplainer:\n",
    "            for i in range(length):\n",
    "                feat = features[i]\n",
    "                shap_val = shap_values[0][i]\n",
    "                abs_val = abs(shap_values[0][i])\n",
    "                entry = (feat, shap_val, abs_val)\n",
    "                importances.append(entry)\n",
    "        \n",
    "        elif type(shap_explainer) == shap.explainers.deep.DeepExplainer:\n",
    "            for i in range(length):\n",
    "                feat = features[i]\n",
    "                shap_val = shap_values[0][0][i]\n",
    "                abs_val = abs(shap_values[0][0][i])\n",
    "                entry = (feat, shap_val, abs_val)\n",
    "                importances.append(entry)\n",
    "        \n",
    "        importances.sort(key=lambda tup: tup[2], reverse = True)\n",
    "        \n",
    "        exp.append(importances)\n",
    "\n",
    "        rel_feat = []\n",
    "\n",
    "        for i in range(top):\n",
    "            feat = importances[i]\n",
    "            if feat[2] > 0:\n",
    "                rel_feat.append(feat)\n",
    "                \n",
    "        rel_exp.append(rel_feat)\n",
    "        \n",
    "    return exp, rel_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dispersal(weights, features):\n",
    "    feat_len = len(features)\n",
    "    #print(feat_len)\n",
    "    weights_by_feat = []\n",
    "       \n",
    "    for i in list(range(feat_len)):\n",
    "        feat_weight = []\n",
    "        for iteration in weights:\n",
    "            feat_weight.append(iteration[i])\n",
    "        weights_by_feat.append(feat_weight)\n",
    "        \n",
    "    #for iteration in weights:\n",
    "     #   for val in iteration:\n",
    "      #      idx = iteration.index(val)\n",
    "       #     print(idx)\n",
    "        #    weights_by_feat[idx].append(val)\n",
    "    \n",
    "    dispersal = []\n",
    "    dispersal_no_outlier = []\n",
    "    \n",
    "    for each in weights_by_feat:\n",
    "        mean = np.mean(each)\n",
    "        std_dev = np.std(each)\n",
    "        var = std_dev**2\n",
    "        \n",
    "        if mean == 0:\n",
    "            dispersal.append(0)\n",
    "            dispersal_no_outlier.append(0)\n",
    "        #print(each)\n",
    "        else:\n",
    "            #dispersal with outliers\n",
    "            rel_var = var/abs(mean)\n",
    "            dispersal.append(rel_var)\n",
    "            \n",
    "            #dispersal without outliers - remove anything with a z-score higher\n",
    "            #than 3 (more than 3 standard deviations away from the mean)\n",
    "            rem_outlier = []\n",
    "            z_scores = stats.zscore(each)\n",
    "            #print(z_scores)\n",
    "            for i in range(len(z_scores)):\n",
    "                #print(each[i],\":\",z_scores[i])\n",
    "                if -3 < z_scores[i] < 3:\n",
    "                    rem_outlier.append(each[i])\n",
    "                #print(rem_outlier)\n",
    "            if rem_outlier != []:\n",
    "                new_mean = np.mean(rem_outlier)\n",
    "                new_std = np.std(rem_outlier)\n",
    "                new_var = new_std**2\n",
    "                new_rel_var = new_var/abs(new_mean)\n",
    "                dispersal_no_outlier.append(new_rel_var)\n",
    "            else:\n",
    "                dispersal_no_outlier.append(rel_var)\n",
    "    \n",
    "    return dispersal, dispersal_no_outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 828,
     "status": "ok",
     "timestamp": 1597835084311,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh6DP4Hn-qpp593Bc6lrayXXcdQv42KAS3zJ-Ay=s64",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "SynFz2rV-arK"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "['bpic2012_accepted']"
      ],
      "text/plain": [
       "['bpic2012_accepted']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_ref = \"bpic2012\"\n",
    "params_dir = PATH + \"params\"\n",
    "results_dir = \"results\"\n",
    "bucket_method = \"single\"\n",
    "cls_encoding = \"agg\"\n",
    "cls_method = \"xgboost\"\n",
    "\n",
    "gap = 1\n",
    "n_iter = 1\n",
    "\n",
    "method_name = \"%s_%s\"%(bucket_method, cls_encoding)\n",
    "\n",
    "generate_samples = False\n",
    "generate_lime = True\n",
    "generate_kernel_shap = False\n",
    "generate_model_shap = True\n",
    "\n",
    "sample_size = 2\n",
    "exp_iter = 10\n",
    "max_feat = 10\n",
    "\n",
    "dataset_ref_to_datasets = {\n",
    "    #\"bpic2011\": [\"bpic2011_f%s\"%formula for formula in range(1,5)],\n",
    "    \"bpic2015\": [\"bpic2015_%s_f2\"%(municipality) for municipality in range(5,6)],\n",
    "    \"bpic2017\" : [\"bpic2017_accepted\"],\n",
    "    \"bpic2012\" : [\"bpic2012_accepted\"]\n",
    "    #\"insurance\": [\"insurance_activity\", \"insurance_followup\"],\n",
    "    #\"sepsis_cases\": [\"sepsis_cases_1\", \"sepsis_cases_2\", \"sepsis_cases_4\"]\n",
    "}\n",
    "\n",
    "datasets = [dataset_ref] if dataset_ref not in dataset_ref_to_datasets else dataset_ref_to_datasets[dataset_ref]\n",
    "\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "hYO7FF3t-wUI",
    "outputId": "bbe62c34-0eaf-440f-83c4-42f496943ed4"
   },
   "outputs": [],
   "source": [
    "if generate_samples:\n",
    "\n",
    "    for dataset_name in datasets:\n",
    "        \n",
    "        dataset_manager = DatasetManager(dataset_name)\n",
    "        \n",
    "        for ii in range(n_iter):\n",
    "            num_buckets = range(len([name for name in os.listdir(os.path.join(PATH,'%s/%s_%s/models'% (dataset_ref, cls_method, method_name)))]))\n",
    "            \n",
    "            for bucket in list(num_buckets):\n",
    "                bucketID = bucket+1\n",
    "                print ('Bucket', bucketID)\n",
    "\n",
    "                #import everything needed to sort and predict\n",
    "                pipeline_path = os.path.join(PATH, \"%s/%s_%s/pipelines/pipeline_bucket_%s.joblib\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "                feat_comb_path = os.path.join(PATH, \"%s/%s_%s/bucketers_and_encoders/feature_combiner_bucket_%s.joblib\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "                bucketer_path = os.path.join(PATH, \"%s/%s_%s/bucketers_and_encoders/bucketer_bucket_%s.joblib\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "                cls_path = os.path.join(PATH, \"%s/%s_%s/models/cls_bucket_%s.joblib\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "\n",
    "                predictor = joblib.load(pipeline_path)\n",
    "                cls = joblib.load(cls_path)\n",
    "                feature_combiner = joblib.load(feat_comb_path)\n",
    "                bucketer = joblib.load(bucketer_path)\n",
    "\n",
    "                #import data for bucket\n",
    "                X_train_path = os.path.join(PATH, \"%s/%s_%s/train_data/bucket_%s_prefixes.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "                Y_train_path = os.path.join(PATH, \"%s/%s_%s/train_data/bucket_%s_labels.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "                X_test_path = os.path.join(PATH, \"%s/%s_%s/test_data/bucket_%s_prefixes.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "                Y_test_path = os.path.join(PATH, \"%s/%s_%s/test_data/bucket_%s_labels.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "\n",
    "                with open (X_train_path, 'rb') as f:\n",
    "                    dt_train_bucket = pickle.load(f)\n",
    "                with open (Y_train_path, 'rb') as f:\n",
    "                    train_y = pickle.load(f)\n",
    "                with open (X_test_path, 'rb') as f:\n",
    "                    dt_test_bucket = pickle.load(f)\n",
    "                with open (Y_test_path, 'rb') as f:\n",
    "                    test_y = pickle.load(f)\n",
    "\n",
    "                #import previous results from predictions\n",
    "                tn_path = os.path.join(PATH, \"%s/%s_%s/instances/true_neg_bucket_%s_.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "                tp_path = os.path.join(PATH, \"%s/%s_%s/instances/true_pos_bucket_%s_.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "                fn_path = os.path.join(PATH, \"%s/%s_%s/instances/false_neg_bucket_%s_.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "                fp_path = os.path.join(PATH, \"%s/%s_%s/instances/false_pos_bucket_%s_.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "\n",
    "                instances = []\n",
    "\n",
    "                with open (tn_path, 'rb') as f:\n",
    "                    tn_list = pickle.load(f)\n",
    "                with open (tp_path, 'rb') as f:\n",
    "                    tp_list = pickle.load(f)\n",
    "                with open (fn_path, 'rb') as f:\n",
    "                    fn_list = pickle.load(f)\n",
    "                with open (fp_path, 'rb') as f:\n",
    "                    fp_list = pickle.load(f)\n",
    "\n",
    "                #save results to a list\n",
    "                instances.append(tn_list)\n",
    "                instances.append(tp_list)\n",
    "                instances.append(fn_list)\n",
    "                instances.append(fp_list)\n",
    "\n",
    "                #choose instances from the event log to explain, based on different prefix lengths\n",
    "                sample_instances = []\n",
    "                \n",
    "                for each in instances:\n",
    "                    prefixes = set([instance['nr_events'] for instance in each])\n",
    "                    sample = []\n",
    "                    for length in prefixes:\n",
    "                    \n",
    "                        #Find instances of relevant length\n",
    "                        relevant = [d for d in each if (d['nr_events'] == length)]\n",
    "                        #Find instances of different prediction probabilities\n",
    "                        prs = [0.5, 0.6, 0.7, 0.8, 0.9, 1.1]\n",
    "                        for i in list(range(len(prs)-1)):\n",
    "                            low = prs[i]\n",
    "                            high = prs[i+1]\n",
    "                            ins = [d for d in relevant if (d['proba'] >= low) & (d['proba'] < high)]\n",
    "                            if len(ins) >= sample_size:\n",
    "                                sample.extend(random.sample(ins, k=sample_size))\n",
    "                            else:\n",
    "                                sample.extend(ins)\n",
    "                    sample = sorted(sample, key = lambda i: (i['proba'], i['nr_events']))\n",
    "                    sample_instances.append(sample)\n",
    "\n",
    "                tn_path = os.path.join(PATH, \"%s/%s_%s/samples/true_neg_bucket_%s_.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "                tp_path = os.path.join(PATH, \"%s/%s_%s/samples/true_pos_bucket_%s_.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "                fn_path = os.path.join(PATH, \"%s/%s_%s/samples/false_neg_bucket_%s_.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "                fp_path = os.path.join(PATH, \"%s/%s_%s/samples/false_pos_bucket_%s_.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "\n",
    "                with open(tn_path, 'wb') as f:\n",
    "                    pickle.dump(sample_instances[0], f)\n",
    "                with open(tp_path, 'wb') as f:\n",
    "                    pickle.dump(sample_instances[1], f)\n",
    "                with open(fn_path, 'wb') as f:\n",
    "                    pickle.dump(sample_instances[2], f)\n",
    "                with open(fp_path, 'wb') as f:\n",
    "                    pickle.dump(sample_instances[3], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket 1\n",
      "Category 1 of 2 . Testing 1 of 400 .\n",
      "(30, 18) 0 0\n",
      "Generating local Explanations for 204712_30\n",
      "Computing feature presence for iteration 0\n",
      "Computing feature presence for iteration 0\n",
      "Computing feature presence for iteration 0\n",
      "Computing feature presence for iteration 0\n",
      "Computing feature presence for iteration 0\n",
      "Computing feature presence for iteration 0\n",
      "Computing feature presence for iteration 0\n",
      "Computing feature presence for iteration 0\n",
      "Computing feature presence for iteration 0\n",
      "Computing feature presence for iteration 0\n",
      "Compiling feature weights for iteration 0\n",
      "Compiling feature weights for iteration 0\n",
      "Compiling feature weights for iteration 0\n",
      "Compiling feature weights for iteration 0\n",
      "Compiling feature weights for iteration 0\n",
      "Compiling feature weights for iteration 0\n",
      "Compiling feature weights for iteration 0\n",
      "Compiling feature weights for iteration 0\n",
      "Compiling feature weights for iteration 0\n",
      "Compiling feature weights for iteration 0\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 0.0\n",
      "Dispersal with no outliers: 0.0\n",
      "Category 1 of 2 . Testing 2 of 400 .\n",
      "(19, 18) 0 0\n",
      "Generating local Explanations for 209078_19\n",
      "Computing feature presence for iteration 0\n",
      "Computing feature presence for iteration 0\n",
      "Computing feature presence for iteration 0\n",
      "Computing feature presence for iteration 0\n",
      "Computing feature presence for iteration 0\n",
      "Computing feature presence for iteration 0\n",
      "Computing feature presence for iteration 0\n",
      "Computing feature presence for iteration 0\n",
      "Computing feature presence for iteration 0\n",
      "Computing feature presence for iteration 0\n",
      "Compiling feature weights for iteration 0\n",
      "Compiling feature weights for iteration 0\n",
      "Compiling feature weights for iteration 0\n",
      "Compiling feature weights for iteration 0\n",
      "Compiling feature weights for iteration 0\n",
      "Compiling feature weights for iteration 0\n",
      "Compiling feature weights for iteration 0\n",
      "Compiling feature weights for iteration 0\n",
      "Compiling feature weights for iteration 0\n",
      "Compiling feature weights for iteration 0\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 0.0\n",
      "Dispersal with no outliers: 0.0\n",
      "Category 1 of 2 . Testing 3 of 400 .\n",
      "(22, 18) 0 0\n",
      "Generating local Explanations for 206727_22\n",
      "Computing feature presence for iteration 0\n",
      "Computing feature presence for iteration 0\n",
      "Computing feature presence for iteration 0\n",
      "Computing feature presence for iteration 0\n",
      "Computing feature presence for iteration 0\n",
      "Computing feature presence for iteration 0\n",
      "Computing feature presence for iteration 0\n",
      "Computing feature presence for iteration 0\n",
      "Computing feature presence for iteration 0\n",
      "Computing feature presence for iteration 0\n",
      "Compiling feature weights for iteration 0\n",
      "Compiling feature weights for iteration 0\n",
      "Compiling feature weights for iteration 0\n",
      "Compiling feature weights for iteration 0\n",
      "Compiling feature weights for iteration 0\n",
      "Compiling feature weights for iteration 0\n",
      "Compiling feature weights for iteration 0\n",
      "Compiling feature weights for iteration 0\n",
      "Compiling feature weights for iteration 0\n",
      "Compiling feature weights for iteration 0\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 0.0\n",
      "Dispersal with no outliers: 0.0\n",
      "Category 1 of 2 . Testing 4 of 400 .\n",
      "(28, 18) 0 0\n",
      "Generating local Explanations for 207212_28\n",
      "Computing feature presence for iteration 0\n",
      "Computing feature presence for iteration 0\n",
      "Computing feature presence for iteration 0\n",
      "Computing feature presence for iteration 0\n",
      "Computing feature presence for iteration 0\n",
      "Computing feature presence for iteration 0\n",
      "Computing feature presence for iteration 0\n",
      "Computing feature presence for iteration 0\n",
      "Computing feature presence for iteration 0\n",
      "Computing feature presence for iteration 0\n",
      "Compiling feature weights for iteration 0\n",
      "Compiling feature weights for iteration 0\n",
      "Compiling feature weights for iteration 0\n",
      "Compiling feature weights for iteration 0\n",
      "Compiling feature weights for iteration 0\n",
      "Compiling feature weights for iteration 0\n",
      "Compiling feature weights for iteration 0\n",
      "Compiling feature weights for iteration 0\n",
      "Compiling feature weights for iteration 0\n",
      "Compiling feature weights for iteration 0\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 0.0\n",
      "Dispersal with no outliers: 0.0\n",
      "Category 1 of 2 . Testing 5 of 400 .\n",
      "(31, 18) 0 0\n",
      "Generating local Explanations for 206333_31\n",
      "Computing feature presence for iteration 0\n",
      "Computing feature presence for iteration 0\n",
      "Computing feature presence for iteration 0\n",
      "Computing feature presence for iteration 0\n",
      "Computing feature presence for iteration 0\n",
      "Computing feature presence for iteration 0\n",
      "Computing feature presence for iteration 0\n",
      "Computing feature presence for iteration 0\n",
      "Computing feature presence for iteration 0\n",
      "Computing feature presence for iteration 0\n",
      "Compiling feature weights for iteration 0\n",
      "Compiling feature weights for iteration 0\n",
      "Compiling feature weights for iteration 0\n",
      "Compiling feature weights for iteration 0\n",
      "Compiling feature weights for iteration 0\n",
      "Compiling feature weights for iteration 0\n",
      "Compiling feature weights for iteration 0\n",
      "Compiling feature weights for iteration 0\n",
      "Compiling feature weights for iteration 0\n",
      "Compiling feature weights for iteration 0\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 0.0\n",
      "Dispersal with no outliers: 0.0\n",
      "Category 1 of 2 . Testing 6 of 400 .\n",
      "(15, 18) 0 0\n",
      "Generating local Explanations for 208871_15\n",
      "Computing feature presence for iteration 0\n",
      "Computing feature presence for iteration 0\n",
      "Computing feature presence for iteration 0\n",
      "Computing feature presence for iteration 0\n",
      "Computing feature presence for iteration 0\n",
      "Computing feature presence for iteration 0\n",
      "Computing feature presence for iteration 0\n",
      "Computing feature presence for iteration 0\n",
      "Computing feature presence for iteration 0\n",
      "Computing feature presence for iteration 0\n",
      "Compiling feature weights for iteration 0\n",
      "Compiling feature weights for iteration 0\n",
      "Compiling feature weights for iteration 0\n",
      "Compiling feature weights for iteration 0\n",
      "Compiling feature weights for iteration 0\n",
      "Compiling feature weights for iteration 0\n",
      "Compiling feature weights for iteration 0\n",
      "Compiling feature weights for iteration 0\n",
      "Compiling feature weights for iteration 0\n",
      "Compiling feature weights for iteration 0\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 0.0\n",
      "Dispersal with no outliers: 0.0\n",
      "Category 1 of 2 . Testing 7 of 400 .\n",
      "(26, 18) 0 0\n",
      "Generating local Explanations for 205247_26\n",
      "Computing feature presence for iteration 0\n",
      "Computing feature presence for iteration 0\n",
      "Computing feature presence for iteration 0\n",
      "Computing feature presence for iteration 0\n",
      "Computing feature presence for iteration 0\n",
      "Computing feature presence for iteration 0\n",
      "Computing feature presence for iteration 0\n",
      "Computing feature presence for iteration 0\n",
      "Computing feature presence for iteration 0\n",
      "Computing feature presence for iteration 0\n",
      "Compiling feature weights for iteration 0\n",
      "Compiling feature weights for iteration 0\n",
      "Compiling feature weights for iteration 0\n",
      "Compiling feature weights for iteration 0\n",
      "Compiling feature weights for iteration 0\n",
      "Compiling feature weights for iteration 0\n",
      "Compiling feature weights for iteration 0\n",
      "Compiling feature weights for iteration 0\n",
      "Compiling feature weights for iteration 0\n",
      "Compiling feature weights for iteration 0\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 0.0\n",
      "Dispersal with no outliers: 0.0\n",
      "Category 1 of 2 . Testing 8 of 400 .\n",
      "(5, 18) 0 0\n",
      "Generating local Explanations for 204613_5\n",
      "Computing feature presence for iteration 0\n",
      "Computing feature presence for iteration 0\n",
      "Computing feature presence for iteration 0\n",
      "Computing feature presence for iteration 0\n",
      "Computing feature presence for iteration 0\n",
      "Computing feature presence for iteration 0\n",
      "Computing feature presence for iteration 0\n",
      "Computing feature presence for iteration 0\n",
      "Computing feature presence for iteration 0\n",
      "Computing feature presence for iteration 0\n",
      "Compiling feature weights for iteration 0\n",
      "Compiling feature weights for iteration 0\n",
      "Compiling feature weights for iteration 0\n",
      "Compiling feature weights for iteration 0\n",
      "Compiling feature weights for iteration 0\n",
      "Compiling feature weights for iteration 0\n",
      "Compiling feature weights for iteration 0\n",
      "Compiling feature weights for iteration 0\n",
      "Compiling feature weights for iteration 0\n",
      "Compiling feature weights for iteration 0\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 0.0\n",
      "Dispersal with no outliers: 0.0\n",
      "Category 1 of 2 . Testing 9 of 400 .\n",
      "(20, 18) 0 0\n",
      "Generating local Explanations for 209841_20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing feature presence for iteration 0\n",
      "Computing feature presence for iteration 0\n",
      "Computing feature presence for iteration 0\n",
      "Computing feature presence for iteration 0\n",
      "Computing feature presence for iteration 0\n",
      "Computing feature presence for iteration 0\n",
      "Computing feature presence for iteration 0\n",
      "Computing feature presence for iteration 0\n",
      "Computing feature presence for iteration 0\n",
      "Computing feature presence for iteration 0\n",
      "Compiling feature weights for iteration 0\n",
      "Compiling feature weights for iteration 0\n",
      "Compiling feature weights for iteration 0\n",
      "Compiling feature weights for iteration 0\n",
      "Compiling feature weights for iteration 0\n",
      "Compiling feature weights for iteration 0\n",
      "Compiling feature weights for iteration 0\n",
      "Compiling feature weights for iteration 0\n",
      "Compiling feature weights for iteration 0\n",
      "Compiling feature weights for iteration 0\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 0.0\n",
      "Dispersal with no outliers: 0.0\n",
      "Category 1 of 2 . Testing 10 of 400 .\n",
      "(12, 18) 0 0\n",
      "Generating local Explanations for 204721_12\n",
      "Computing feature presence for iteration 0\n",
      "Computing feature presence for iteration 0\n",
      "Computing feature presence for iteration 0\n",
      "Computing feature presence for iteration 0\n",
      "Computing feature presence for iteration 0\n",
      "Computing feature presence for iteration 0\n",
      "Computing feature presence for iteration 0\n",
      "Computing feature presence for iteration 0\n",
      "Computing feature presence for iteration 0\n",
      "Computing feature presence for iteration 0\n",
      "Compiling feature weights for iteration 0\n",
      "Compiling feature weights for iteration 0\n",
      "Compiling feature weights for iteration 0\n",
      "Compiling feature weights for iteration 0\n",
      "Compiling feature weights for iteration 0\n",
      "Compiling feature weights for iteration 0\n",
      "Compiling feature weights for iteration 0\n",
      "Compiling feature weights for iteration 0\n",
      "Compiling feature weights for iteration 0\n",
      "Compiling feature weights for iteration 0\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 0.0\n",
      "Dispersal with no outliers: 0.0\n",
      "Category 1 of 2 . Testing 11 of 400 .\n",
      "(33, 18) 0 0\n",
      "Generating local Explanations for 208961_33\n"
     ]
    }
   ],
   "source": [
    "if generate_model_shap:\n",
    "\n",
    "    for dataset_name in datasets:\n",
    "        \n",
    "        dataset_manager = DatasetManager(dataset_name)\n",
    "        \n",
    "        for ii in range(n_iter):\n",
    "            num_buckets = range(len([name for name in os.listdir(os.path.join(PATH,'%s/%s_%s/models'% (dataset_ref, cls_method, method_name)))]))\n",
    "            \n",
    "            for bucket in list(num_buckets):\n",
    "                bucketID = bucket+1\n",
    "                print ('Bucket', bucketID)\n",
    "\n",
    "                #import everything needed to sort and predict\n",
    "                feat_comb_path = os.path.join(PATH, \"%s/%s_%s/bucketers_and_encoders/feature_combiner_bucket_%s.joblib\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "                cls_path = os.path.join(PATH, \"%s/%s_%s/models/cls_bucket_%s.joblib\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "                cls = joblib.load(cls_path)\n",
    "                feature_combiner = joblib.load(feat_comb_path)\n",
    "                \n",
    "                #import data for bucket\n",
    "                #X_train_path = os.path.join(PATH, \"%s/%s_%s/train_data/bucket_%s_prefixes.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "                #Y_train_path = os.path.join(PATH, \"%s/%s_%s/train_data/bucket_%s_labels.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "                \n",
    "                #with open (X_train_path, 'rb') as f:\n",
    "                #    dt_train_bucket = pickle.load(f)\n",
    "                #with open (Y_train_path, 'rb') as f:\n",
    "                #    train_y = pickle.load(f)\n",
    "                #with open (X_test_path, 'rb') as f:\n",
    "                #    dt_test_bucket = pickle.load(f)\n",
    "                #with open (Y_test_path, 'rb') as f:\n",
    "                #    test_y = pickle.load(f)\n",
    "\n",
    "                #import previously identified samples\n",
    "                tn_path = os.path.join(PATH, \"%s/%s_%s/samples/true_neg_bucket_%s_.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "                tp_path = os.path.join(PATH, \"%s/%s_%s/samples/true_pos_bucket_%s_.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "                #fn_path = os.path.join(PATH, \"%s/%s_%s/samples/false_neg_bucket_%s_.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "                #fp_path = os.path.join(PATH, \"%s/%s_%s/samples/false_pos_bucket_%s_.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "\n",
    "                sample_instances = []\n",
    "\n",
    "                with open (tn_path, 'rb') as f:\n",
    "                    tn_list = pickle.load(f)\n",
    "                with open (tp_path, 'rb') as f:\n",
    "                    tp_list = pickle.load(f)\n",
    "                #with open (fn_path, 'rb') as f:\n",
    "                #    fn_list = pickle.load(f)\n",
    "                #with open (fp_path, 'rb') as f:\n",
    "                #    fp_list = pickle.load(f)\n",
    "\n",
    "                #save results to a list\n",
    "                sample_instances.append(tn_list)\n",
    "                sample_instances.append(tp_list)\n",
    "                #sample_instances.append(fn_list)\n",
    "                #sample_instances.append(fp_list)\n",
    "\n",
    "                #create explainers now that can be passed later\n",
    "                tree_explainer = shap.TreeExplainer(cls)\n",
    "\n",
    "                #explain the chosen instances and find the stability score\n",
    "                cat_no = 0\n",
    "                for category in sample_instances:\n",
    "                    cat_no += 1\n",
    "                    instance_no = 0\n",
    "                    \n",
    "                    for instance in category:\n",
    "                        instance_no += 1    \n",
    "                        print(\"Category\", cat_no, \"of\", len(sample_instances), \". Testing\", instance_no, \"of\", len(category), \".\")\n",
    "                \n",
    "                        group = instance['input']\n",
    "\n",
    "                        print(group.shape,instance['actual'], instance['predicted'])\n",
    "                        test_x_group= feature_combiner.fit_transform(group) \n",
    "                        test_x=np.transpose(test_x_group[0])\n",
    "\n",
    "                        print('Generating local Explanations for', instance['caseID'])\n",
    "\n",
    "                        feat_list = feature_combiner.get_feature_names()\n",
    "\n",
    "                        #Get Tree SHAP explanations for instance\n",
    "                        exp, rel_exp = create_samples(tree_explainer, exp_iter, test_x.reshape(-1, len(test_x)), feat_list, top = max_feat)\n",
    "\n",
    "                        feat_pres = []\n",
    "                        feat_weights = []\n",
    "\n",
    "                        for iteration in rel_exp:\n",
    "                            print(\"Computing feature presence for iteration\", rel_exp.index(iteration))\n",
    "                            #print(iteration)\n",
    "\n",
    "                            #Stability by index\n",
    "                            presence_list = [0]*len(feat_list)\n",
    "                            #stability by weight\n",
    "                            #weights = [0]*len(feat_list)\n",
    "\n",
    "                            for each in feat_list:\n",
    "                                list_idx = feat_list.index(each)\n",
    "                                #print (\"Feature\", list_idx)\n",
    "                                for explanation in iteration:\n",
    "                                    if each in explanation[0]:\n",
    "                                        presence_list[list_idx] = 1\n",
    "                                        #weights[list_idx] = explanation[1]\n",
    "\n",
    "                            feat_pres.append(presence_list)\n",
    "                            \n",
    "                        for iteration in exp:\n",
    "                            print(\"Compiling feature weights for iteration\", exp.index(iteration))\n",
    "                            #print(iteration)\n",
    "\n",
    "                            #Stability by index\n",
    "                            #presence_list = [0]*len(feat_list)\n",
    "                            #stability by weight\n",
    "                            weights = [0]*len(feat_list)\n",
    "\n",
    "                            for each in feat_list:\n",
    "                                list_idx = feat_list.index(each)\n",
    "                                #print (\"Feature\", list_idx)\n",
    "                                for explanation in iteration:\n",
    "                                    if each in explanation[0]:\n",
    "                                        #presence_list[list_idx] = 1\n",
    "                                        weights[list_idx] = explanation[1]\n",
    "                            feat_weights.append(weights)\n",
    "\n",
    "                        stability = st.getStability(feat_pres)\n",
    "                        print (\"Stability:\", round(stability,2))\n",
    "                        instance['tree_shap_stability'] = stability\n",
    "                        \n",
    "                        rel_var, second_var = dispersal(feat_weights, feat_list)\n",
    "                        avg_dispersal = np.mean(rel_var)\n",
    "                        print (\"Dispersal of feature importance:\", round(avg_dispersal, 2))\n",
    "                        instance['shap_weights_dispersal'] = rel_var\n",
    "                        adj_dispersal = np.mean(second_var)\n",
    "                        print (\"Dispersal with no outliers:\", round(adj_dispersal, 2))\n",
    "                        instance['adjusted_shap_weights_dispersal'] = second_var\n",
    "                        \n",
    "                with open(tn_path, 'wb') as f:\n",
    "                    pickle.dump(sample_instances[0], f)\n",
    "                with open(tp_path, 'wb') as f:\n",
    "                    pickle.dump(sample_instances[1], f)\n",
    "                #with open(fn_path, 'wb') as f:\n",
    "                #    pickle.dump(sample_instances[2], f)\n",
    "                #with open(fp_path, 'wb') as f:\n",
    "                #    pickle.dump(sample_instances[3], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if generate_lime:\n",
    "\n",
    "    for dataset_name in datasets:\n",
    "        \n",
    "        dataset_manager = DatasetManager(dataset_name)\n",
    "        \n",
    "        for ii in range(n_iter):\n",
    "            num_buckets = range(len([name for name in os.listdir(os.path.join(PATH,'%s/%s_%s/models'% (dataset_ref, cls_method, method_name)))]))\n",
    "            \n",
    "            for bucket in list(num_buckets):\n",
    "                bucketID = bucket+1\n",
    "                print ('Bucket', bucketID)\n",
    "\n",
    "                #import everything needed to sort and predict\n",
    "                feat_comb_path = os.path.join(PATH, \"%s/%s_%s/bucketers_and_encoders/feature_combiner_bucket_%s.joblib\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "                cls_path = os.path.join(PATH, \"%s/%s_%s/models/cls_bucket_%s.joblib\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "                cls = joblib.load(cls_path)\n",
    "                feature_combiner = joblib.load(feat_comb_path)\n",
    "                \n",
    "                #import data for bucket\n",
    "                X_train_path = os.path.join(PATH, \"%s/%s_%s/train_data/bucket_%s_prefixes.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "                Y_train_path = os.path.join(PATH, \"%s/%s_%s/train_data/bucket_%s_labels.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "                \n",
    "                with open (X_train_path, 'rb') as f:\n",
    "                    dt_train_bucket = pickle.load(f)\n",
    "                with open (Y_train_path, 'rb') as f:\n",
    "                    train_y = pickle.load(f)\n",
    "                #with open (X_test_path, 'rb') as f:\n",
    "                #    dt_test_bucket = pickle.load(f)\n",
    "                #with open (Y_test_path, 'rb') as f:\n",
    "                #    test_y = pickle.load(f)\n",
    "\n",
    "                #import previously identified samples\n",
    "                tn_path = os.path.join(PATH, \"%s/%s_%s/samples/true_neg_bucket_%s_.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "                tp_path = os.path.join(PATH, \"%s/%s_%s/samples/true_pos_bucket_%s_.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "                #fn_path = os.path.join(PATH, \"%s/%s_%s/samples/false_neg_bucket_%s_.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "                #fp_path = os.path.join(PATH, \"%s/%s_%s/samples/false_pos_bucket_%s_.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "\n",
    "                sample_instances = []\n",
    "\n",
    "                with open (tn_path, 'rb') as f:\n",
    "                    tn_list = pickle.load(f)\n",
    "                with open (tp_path, 'rb') as f:\n",
    "                    tp_list = pickle.load(f)\n",
    "                #with open (fn_path, 'rb') as f:\n",
    "                #    fn_list = pickle.load(f)\n",
    "                #with open (fp_path, 'rb') as f:\n",
    "                #    fp_list = pickle.load(f)\n",
    "\n",
    "                #save results to a list\n",
    "                sample_instances.append(tn_list)\n",
    "                sample_instances.append(tp_list)\n",
    "                #sample_instances.append(fn_list)\n",
    "                #sample_instances.append(fp_list)\n",
    "\n",
    "                #get the training data as a matrix\n",
    "                trainingdata = feature_combiner.fit_transform(dt_train_bucket);\n",
    "                #importance = generate_global_explanations(trainingdata,train_y, cls, feature_combiner)\n",
    "\n",
    "                #explain the chosen instances and find the stability score\n",
    "                cat_no = 0\n",
    "                for category in sample_instances:\n",
    "                    cat_no += 1\n",
    "                    instance_no = 0\n",
    "                    for instance in category:\n",
    "                        instance_no += 1\n",
    "                        \n",
    "                        print(\"Category\", cat_no, \"of\", len(sample_instances), \". Testing\", instance_no, \"of\", len(category), \".\")\n",
    "                        \n",
    "                        group = instance['input']\n",
    "                        \n",
    "                        #create explainer now that can be passed later\n",
    "                        class_names=['regular','deviant']# regular is 0, deviant is 1, 0 is left, 1 is right\n",
    "                        lime_explainer = lime.lime_tabular.LimeTabularExplainer(trainingdata,\n",
    "                                          feature_names = feature_combiner.get_feature_names(),\n",
    "                                          class_names=class_names, discretize_continuous=True)\n",
    "\n",
    "                        #print(group.shape,instance['actual'], instance['predicted'])\n",
    "                        test_x_group= feature_combiner.fit_transform(group) \n",
    "                        test_x=np.transpose(test_x_group[0])\n",
    "\n",
    "                        print('Generating local Explanations for', instance['caseID'])\n",
    "\n",
    "                        feat_list = feature_combiner.get_feature_names()\n",
    "\n",
    "                        #Get lime explanations for instance\n",
    "                        feat_pres = []\n",
    "                        feat_weights = []\n",
    "\n",
    "                        for iteration in list(range(exp_iter)):\n",
    "                            print(\"Run\", iteration)\n",
    "                            \n",
    "                            lime_exp = generate_lime_explanations(lime_explainer, test_x, cls, instance['actual'], max_feat = 300)\n",
    "\n",
    "                            presence_list = [0]*len(feat_list)\n",
    "                            weights = [0]*len(feat_list)\n",
    "\n",
    "                            for each in feat_list:\n",
    "                                list_idx = feat_list.index(each)\n",
    "                                #print (\"Feature\", list_idx)\n",
    "                                for explanation in lime_exp.as_list():\n",
    "                                    if each in explanation[0]:\n",
    "                                        if lime_exp.as_list().index(explanation) < max_feat:\n",
    "                                            presence_list[list_idx] = 1\n",
    "                                        weights[list_idx] = explanation[1]\n",
    "\n",
    "                            feat_pres.append(presence_list)\n",
    "                            feat_weights.append(weights)\n",
    "\n",
    "                        stability = st.getStability(feat_pres)\n",
    "                        print (\"Stability:\", round(stability,2))\n",
    "                        instance['lime_stability'] = stability\n",
    "                        \n",
    "                        rel_var, second_var = dispersal(feat_weights, feat_list)\n",
    "                        avg_dispersal = np.mean(rel_var)\n",
    "                        print (\"Dispersal of feature importance:\", round(avg_dispersal, 2))\n",
    "                        instance['lime_weights_dispersal'] = rel_var\n",
    "                        adj_dispersal = np.mean(second_var)\n",
    "                        print (\"Dispersal with no outliers:\", round(adj_dispersal, 2))\n",
    "                        instance['adjusted_lime_weights_dispersal'] = second_var\n",
    "                        \n",
    "                #Save dictionaries updated with stability scores\n",
    "                with open(tn_path, 'wb') as f:\n",
    "                    pickle.dump(sample_instances[0], f)\n",
    "                with open(tp_path, 'wb') as f:\n",
    "                    pickle.dump(sample_instances[1], f)\n",
    "                #with open(fn_path, 'wb') as f:\n",
    "                #    pickle.dump(sample_instances[2], f)\n",
    "                #with open(fp_path, 'wb') as f:\n",
    "                #    pickle.dump(sample_instances[3], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPLd3CKoi62ghquFtMVq2SC",
   "collapsed_sections": [],
   "name": "bpic2012_lime_stability.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
