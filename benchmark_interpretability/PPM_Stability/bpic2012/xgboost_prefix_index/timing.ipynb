{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vTbNLZGY9Zjr",
    "outputId": "ad98f449-8962-40fd-bea2-8e585067a421"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "import sys\n",
    "#PATH = '/content/drive/My Drive/PPM_Stability/'\n",
    "PATH = \"C:/Users/velmurug/Documents/Stability Experiments/benchmark_interpretability/PPM_Stability/\"\n",
    "#PATH = \"C:/Users/Mythreyi/Documents/GitHub/Stability-Experiments/benchmark_interpretability/PPM_Stability/\"\n",
    "sys.path.append(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NOTE47wJ9v8E",
    "outputId": "0e69dc67-1419-4609-e176-e3e8fd1dc9c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lime==0.2.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (0.2.0.1)\n",
      "Requirement already satisfied: scikit-image>=0.12 in c:\\programdata\\anaconda3\\lib\\site-packages (from lime==0.2.0.1) (0.17.2)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from lime==0.2.0.1) (1.18.5)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from lime==0.2.0.1) (4.49.0)\n",
      "Requirement already satisfied: matplotlib in c:\\programdata\\anaconda3\\lib\\site-packages (from lime==0.2.0.1) (3.3.2)\n",
      "Requirement already satisfied: scikit-learn>=0.18 in c:\\programdata\\anaconda3\\lib\\site-packages (from lime==0.2.0.1) (0.23.2)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (from lime==0.2.0.1) (1.4.1)\n",
      "Requirement already satisfied: networkx>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-image>=0.12->lime==0.2.0.1) (2.5)\n",
      "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-image>=0.12->lime==0.2.0.1) (7.2.0)\n",
      "Requirement already satisfied: imageio>=2.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-image>=0.12->lime==0.2.0.1) (2.9.0)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-image>=0.12->lime==0.2.0.1) (2020.9.3)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-image>=0.12->lime==0.2.0.1) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->lime==0.2.0.1) (0.10.0)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->lime==0.2.0.1) (2020.6.20)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->lime==0.2.0.1) (1.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->lime==0.2.0.1) (2.8.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->lime==0.2.0.1) (2.4.7)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn>=0.18->lime==0.2.0.1) (0.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn>=0.18->lime==0.2.0.1) (2.1.0)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from networkx>=2.0->scikit-image>=0.12->lime==0.2.0.1) (4.4.2)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from cycler>=0.10->matplotlib->lime==0.2.0.1) (1.15.0)\n",
      "Requirement already satisfied: shap==0.35.0 in c:\\programdata\\anaconda3\\lib\\site-packages (0.35.0)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (from shap==0.35.0) (1.4.1)\n",
      "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\lib\\site-packages (from shap==0.35.0) (1.1.2)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from shap==0.35.0) (1.18.5)\n",
      "Requirement already satisfied: scikit-learn in c:\\programdata\\anaconda3\\lib\\site-packages (from shap==0.35.0) (0.23.2)\n",
      "Requirement already satisfied: tqdm>4.25.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from shap==0.35.0) (4.49.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas->shap==0.35.0) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas->shap==0.35.0) (2020.1)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn->shap==0.35.0) (0.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn->shap==0.35.0) (2.1.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7.3->pandas->shap==0.35.0) (1.15.0)\n",
      "Requirement already satisfied: xgboost==1.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (1.0.0)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (from xgboost==1.0.0) (1.4.1)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from xgboost==1.0.0) (1.18.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install lime==0.2.0.1\n",
    "!pip install shap==0.35.0\n",
    "#!pip install pandas==0.19.2\n",
    "!pip install xgboost==1.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-wVsWqvt9zzb",
    "outputId": "c5dc50ef-5eba-4b9d-ede2-c4c23600c064"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import EncoderFactory\n",
    "from DatasetManager_for_colab import DatasetManager\n",
    "import BucketFactory\n",
    "import stability as st #Nogueira, Sechidis, Brown.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "from sys import argv\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import joblib\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import Dense, Embedding, Flatten, Input, LSTM\n",
    "from keras.optimizers import Nadam, RMSprop\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "from tensorflow.keras.backend import print_tensor\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.compat.v1 import disable_v2_behavior#, ConfigProto, Session\n",
    "from tensorflow.compat.v1.keras.backend import get_session\n",
    "disable_v2_behavior()\n",
    "\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "from lime import submodular_pick;\n",
    "\n",
    "import shap\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "NgQh__fq9_xK"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "lime_pal = sns.diverging_palette(100, 200, s=150, as_cmap=True)\n",
    "shap_pal = sns.diverging_palette(0, 240, s=150, as_cmap=True)\n",
    "\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "SynFz2rV-arK",
    "outputId": "26de37b4-9690-48e8-90a1-bc6889559dc4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "['bpic2012_accepted']"
      ],
      "text/plain": [
       "['bpic2012_accepted']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_ref = \"bpic2012\"\n",
    "params_dir = PATH + \"params\"\n",
    "results_dir = \"results\"\n",
    "bucket_method = \"prefix\"\n",
    "cls_encoding = \"index\"\n",
    "cls_method = \"xgboost\"\n",
    "\n",
    "gap = 1\n",
    "n_iter = 1\n",
    "\n",
    "method_name = \"%s_%s\"%(bucket_method, cls_encoding)\n",
    "\n",
    "generate_samples = True\n",
    "generate_lime = True\n",
    "generate_kernel_shap = False\n",
    "generate_model_shap = True\n",
    "\n",
    "sample_size = 0.25\n",
    "exp_iter = 1\n",
    "max_feat = 10\n",
    "\n",
    "dataset_ref_to_datasets = {\n",
    "    #\"bpic2011\": [\"bpic2011_f%s\"%formula for formula in range(1,5)],\n",
    "    \"bpic2015\": [\"bpic2015_%s_f2\"%(municipality) for municipality in range(5,6)],\n",
    "    \"bpic2017\" : [\"bpic2017_accepted\"],\n",
    "    \"bpic2012\" : [\"bpic2012_accepted\"],\n",
    "    #\"insurance\": [\"insurance_activity\", \"insurance_followup\"],\n",
    "    \"sepsis_cases\": [\"sepsis_cases_1\"],# \"sepsis_cases_2\", \"sepsis_cases_4\"]\n",
    "    \"production\" : [\"production\"]\n",
    "}\n",
    "\n",
    "datasets = [dataset_ref] if dataset_ref not in dataset_ref_to_datasets else dataset_ref_to_datasets[dataset_ref]\n",
    "\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9BPaNPbQo39O",
    "outputId": "c49d5458-664e-4912-f5bb-cc1a065d403a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket 1\n",
      "Time taken to create SHAP explainer: 1.219465970993042 seconds.\n",
      "Time taken to create LIME explainer: 0.04586672782897949 seconds.\n",
      "Bucket 2\n",
      "Time taken to create SHAP explainer: 1.194061040878296 seconds.\n",
      "Time taken to create LIME explainer: 0.06640744209289551 seconds.\n",
      "Bucket 3\n",
      "Time taken to create SHAP explainer: 1.1198768615722656 seconds.\n",
      "Time taken to create LIME explainer: 0.09275269508361816 seconds.\n",
      "Bucket 4\n",
      "Time taken to create SHAP explainer: 1.0759241580963135 seconds.\n",
      "Time taken to create LIME explainer: 0.17185592651367188 seconds.\n",
      "Bucket 5\n",
      "Time taken to create SHAP explainer: 1.1569807529449463 seconds.\n",
      "Time taken to create LIME explainer: 0.2675161361694336 seconds.\n",
      "Bucket 6\n",
      "Time taken to create SHAP explainer: 1.033940315246582 seconds.\n",
      "Time taken to create LIME explainer: 0.3807716369628906 seconds.\n",
      "Bucket 7\n",
      "Time taken to create SHAP explainer: 0.9568262100219727 seconds.\n",
      "Time taken to create LIME explainer: 0.48133420944213867 seconds.\n",
      "Bucket 8\n",
      "Time taken to create SHAP explainer: 0.9382603168487549 seconds.\n",
      "Time taken to create LIME explainer: 0.599470853805542 seconds.\n",
      "Bucket 9\n",
      "Time taken to create SHAP explainer: 0.9226398468017578 seconds.\n",
      "Time taken to create LIME explainer: 0.6961259841918945 seconds.\n",
      "Bucket 10\n",
      "Time taken to create SHAP explainer: 0.925570011138916 seconds.\n",
      "Time taken to create LIME explainer: 0.8328211307525635 seconds.\n",
      "Bucket 11\n",
      "Time taken to create SHAP explainer: 0.9197099208831787 seconds.\n",
      "Time taken to create LIME explainer: 0.9402337074279785 seconds.\n",
      "Bucket 12\n",
      "Time taken to create SHAP explainer: 0.9040892124176025 seconds.\n",
      "Time taken to create LIME explainer: 1.0573692321777344 seconds.\n",
      "Bucket 13\n",
      "Time taken to create SHAP explainer: 0.883589506149292 seconds.\n",
      "Time taken to create LIME explainer: 1.1686968803405762 seconds.\n",
      "Bucket 14\n",
      "Time taken to create SHAP explainer: 0.8787052631378174 seconds.\n",
      "Time taken to create LIME explainer: 1.2936513423919678 seconds.\n",
      "Bucket 15\n",
      "Time taken to create SHAP explainer: 0.840609073638916 seconds.\n",
      "Time taken to create LIME explainer: 1.409850835800171 seconds.\n",
      "Bucket 16\n",
      "Time taken to create SHAP explainer: 0.833794355392456 seconds.\n",
      "Time taken to create LIME explainer: 1.5308966636657715 seconds.\n",
      "Bucket 17\n",
      "Time taken to create SHAP explainer: 0.7986505031585693 seconds.\n",
      "Time taken to create LIME explainer: 1.6529393196105957 seconds.\n",
      "Bucket 18\n",
      "Time taken to create SHAP explainer: 0.7683801651000977 seconds.\n",
      "Time taken to create LIME explainer: 1.7945077419281006 seconds.\n",
      "Bucket 19\n",
      "Time taken to create SHAP explainer: 0.7390906810760498 seconds.\n",
      "Time taken to create LIME explainer: 1.9204347133636475 seconds.\n",
      "Bucket 20\n",
      "Time taken to create SHAP explainer: 0.7156558036804199 seconds.\n",
      "Time taken to create LIME explainer: 2.0327367782592773 seconds.\n",
      "Bucket 21\n",
      "Time taken to create SHAP explainer: 0.6834192276000977 seconds.\n",
      "Time taken to create LIME explainer: 2.0669057369232178 seconds.\n",
      "Bucket 22\n",
      "Time taken to create SHAP explainer: 0.7771701812744141 seconds.\n",
      "Time taken to create LIME explainer: 2.188953161239624 seconds.\n",
      "Bucket 23\n",
      "Time taken to create SHAP explainer: 0.6346232891082764 seconds.\n",
      "Time taken to create LIME explainer: 2.237788677215576 seconds.\n"
     ]
    }
   ],
   "source": [
    "  for dataset_name in datasets:\n",
    "      \n",
    "      dataset_manager = DatasetManager(dataset_name)\n",
    "      \n",
    "      for ii in range(n_iter):\n",
    "          if cls_method == \"lstm\":\n",
    "              num_buckets = 1\n",
    "          else:\n",
    "              num_buckets = len([name for name in os.listdir(os.path.join(PATH,'%s/%s_%s/models'% (dataset_ref, cls_method, method_name)))])\n",
    "\n",
    "          all_pref_len = []\n",
    "          all_feat_len = []\n",
    "          all_shap_times = []\n",
    "          all_lime_times = []\n",
    "          sep_pref_len = []\n",
    "          sep_feat_len = []\n",
    "          all_times = []\n",
    "          all_types = []\n",
    "\n",
    "          sep_data_dict = {'Prefix Length': sep_pref_len, 'Feature Vector Length': sep_feat_len, 'SHAP Running Time': all_shap_times, 'LIME Running Time': all_lime_times}\n",
    "          all_data_dict = {'Prefix Length': all_pref_len, 'Feature Vector Length': all_feat_len, 'Explainer': all_types, 'Running Time': all_times}\n",
    "          sep_timing_path = os.path.join(PATH, \"%s/%s_%s/sep_timing.csv\" % (dataset_ref, cls_method, method_name))\n",
    "          all_timing_path = os.path.join(PATH, \"%s/%s_%s/all_timing.csv\" % (dataset_ref, cls_method, method_name))\n",
    "\n",
    "\n",
    "          for bucket in range(num_buckets):\n",
    "              if cls_method == \"lstm\":\n",
    "                bucketID = \"all\"\n",
    "              else:\n",
    "                bucketID = bucket+1\n",
    "              print ('Bucket', bucketID)\n",
    "\n",
    "              if cls_method == \"lstm\":\n",
    "                  print(\"get everything to create model\")\n",
    "                  params_path = os.path.join(PATH, \"%s/%s_%s/cls/params.pickle\" % (dataset_ref, cls_method, method_name))\n",
    "                  with open(params_path, 'rb') as f:\n",
    "                      args = pickle.load(f)\n",
    "\n",
    "                  max_len = args['max_len']\n",
    "                  data_dim = args['data_dim']\n",
    "                  print(\"Parameters loaded\")\n",
    "\n",
    "                  #create model\n",
    "                  print(\"defining input layer\")\n",
    "                  main_input = Input(shape=(max_len, data_dim), name='main_input')\n",
    "                  \n",
    "                  print(\"adding lstm layers\")\n",
    "                  if args[\"lstm_layers\"][\"layers\"] == \"one\":\n",
    "                      l2_3 = LSTM(args['lstm1_nodes'], input_shape=(max_len, data_dim), implementation=2, \n",
    "                                  kernel_initializer='glorot_uniform', return_sequences=False, \n",
    "                                  recurrent_dropout=args['lstm1_dropouts'], stateful = False)(main_input)\n",
    "                      b2_3 = BatchNormalization()(l2_3)\n",
    "\n",
    "                  if args[\"lstm_layers\"][\"layers\"] == \"two\":\n",
    "                      l1 = LSTM(args['lstm1_nodes'], input_shape=(max_len, data_dim), implementation=2, \n",
    "                              kernel_initializer='glorot_uniform', return_sequences=True, \n",
    "                              recurrent_dropout=args['lstm1_dropouts'], stateful = False)(main_input)\n",
    "                      b1 = BatchNormalization()(l1)\n",
    "                      l2_3 = LSTM(args[\"lstm_layers\"][\"lstm2_nodes\"], activation=\"sigmoid\", \n",
    "                                  implementation=2, kernel_initializer='glorot_uniform', return_sequences=False, \n",
    "                                  recurrent_dropout=args[\"lstm_layers\"][\"lstm2_dropouts\"], stateful = False)(b1)\n",
    "                      b2_3 = BatchNormalization()(l2_3)\n",
    "                      \n",
    "                  if args[\"lstm_layers\"][\"layers\"] == \"three\":\n",
    "                      l1 = LSTM(args['lstm1_nodes'], input_shape=(max_len, data_dim),implementation=2, \n",
    "                              kernel_initializer='glorot_uniform', return_sequences=True, \n",
    "                              recurrent_dropout=args['lstm1_dropouts'], stateful = False)(main_input)\n",
    "                      b1 = BatchNormalization()(l1)\n",
    "                      l2 = LSTM(args[\"lstm_layers\"][\"lstm2_nodes\"], activation=\"sigmoid\", \n",
    "                                  implementation=2, kernel_initializer='glorot_uniform', return_sequences=True, \n",
    "                                  recurrent_dropout=args[\"lstm_layers\"][\"lstm2_dropouts\"], stateful = False)(b1)\n",
    "                      b2 = BatchNormalization()(l2)\n",
    "                      l2_3 = LSTM(args[\"lstm_layers\"][\"lstm3_nodes\"], activation=\"sigmoid\", \n",
    "                                  implementation=2, kernel_initializer='glorot_uniform', return_sequences=False, \n",
    "                                  recurrent_dropout=args[\"lstm_layers\"][\"lstm3_dropouts\"], stateful = False)(b2)\n",
    "                      b2_3 = BatchNormalization()(l2_3)\n",
    "                  \n",
    "                  print(\"adding dense layers\")\n",
    "                  if args['dense_layers']['layers'] == \"two\":\n",
    "                      d1 = Dense(args['dense_layers']['dense2_nodes'], activation = \"relu\")(b2_3)\n",
    "                      outcome_output = Dense(2, activation='sigmoid', kernel_initializer='glorot_uniform', name='outcome_output')(d1)\n",
    "\n",
    "                  else:\n",
    "                      outcome_output = Dense(2, activation='sigmoid', kernel_initializer='glorot_uniform', name='outcome_output')(b2_3)\n",
    "                  \n",
    "                  print(\"putting together layers\")\n",
    "                  cls = Model(inputs=[main_input], outputs=[outcome_output])\n",
    "                  \n",
    "                  print(\"choosing optimiser\")\n",
    "                  if args['optimizer'] == \"adam\":\n",
    "                      opt = Nadam(lr=args['learning_rate'], beta_1=0.9, beta_2=0.999, epsilon=1e-08, schedule_decay=0.004, clipvalue=3)\n",
    "                  elif args['optimizer'] == \"rmsprop\":\n",
    "                      opt = RMSprop(lr=args['learning_rate'], rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "                      \n",
    "                  print(\"adding weights to model\")\n",
    "                  checkpoint_path = os.path.join(PATH, \"%s/%s_%s/cls/checkpoint.cpt\" % (dataset_ref, cls_method, method_name))\n",
    "                  weights = cls.load_weights(checkpoint_path)\n",
    "\n",
    "                  print(\"compiling model\")\n",
    "                  cls.compile(loss='binary_crossentropy', optimizer=opt)\n",
    "\n",
    "                  X_train_path = os.path.join(PATH, \"%s/%s_%s/train_data/bucket_%s_prefixes.pickle\" % (dataset_ref, cls_method, method_name, bucketID))                  \n",
    "                  with open (X_train_path, 'rb') as f:\n",
    "                      dt_train_bucket = pickle.load(f)\n",
    "\n",
    "                  X_test_path = os.path.join(PATH, \"%s/%s_%s/test_data/bucket_%s_prefixes.pickle\" % (dataset_ref, cls_method, method_name, bucketID))                  \n",
    "                  with open (X_test_path, 'rb') as f:\n",
    "                      dt_test_bucket = pickle.load(f)\n",
    "\n",
    "                  sizes = []\n",
    "\n",
    "                  indices = list(range(len(dt_test_bucket)))\n",
    "                  sample_indices = random.sample(indices, len(dt_test_bucket)*sample_size)\n",
    "                  dt_testing_sample = [dt_test_bucket[i] for i in indices]\n",
    "\n",
    "                  for instance in dt_testing_sample:\n",
    "                    filled = [np.any(ts != 0) for ts in instance]\n",
    "                    prefs = filled.count(True)\n",
    "                    sizes.append(prefs)\n",
    "                  \n",
    "                  feat_len = [vec.shape for vec in dt_testing_sample]\n",
    "\n",
    "                  # samples = []\n",
    "                  # tn_path = os.path.join(PATH, \"%s/%s_%s/instances/true_neg_bucket_%s_.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "                  # tp_path = os.path.join(PATH, \"%s/%s_%s/instances/true_pos_bucket_%s_.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "                  # fn_path = os.path.join(PATH, \"%s/%s_%s/instances/false_neg_bucket_%s_.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "                  # fp_path = os.path.join(PATH, \"%s/%s_%s/instances/false_pos_bucket_%s_.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "\n",
    "                  # with open (tn_path, 'rb') as f:\n",
    "                  #     tn_list = pickle.load(f)\n",
    "                  # with open (tp_path, 'rb') as f:\n",
    "                  #     tp_list = pickle.load(f)\n",
    "                  # with open (fn_path, 'rb') as f:\n",
    "                  #     fn_list = pickle.load(f)\n",
    "                  # with open (fp_path, 'rb') as f:\n",
    "                  #     fp_list = pickle.load(f)\n",
    "                  \n",
    "                  # sa,\n",
    "\n",
    "              else:\n",
    "                  pipeline_path = os.path.join(PATH, \"%s/%s_%s/pipelines/pipeline_bucket_%s.joblib\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "                  feat_comb_path = os.path.join(PATH, \"%s/%s_%s/bucketers_and_encoders/feature_combiner_bucket_%s.joblib\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "                  bucketer_path = os.path.join(PATH, \"%s/%s_%s/bucketers_and_encoders/bucketer_bucket_%s.joblib\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "                  cls_path = os.path.join(PATH, \"%s/%s_%s/models/cls_bucket_%s.joblib\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "\n",
    "                  predictor = joblib.load(pipeline_path)\n",
    "                  cls = joblib.load(cls_path)\n",
    "                  feature_combiner = joblib.load(feat_comb_path)\n",
    "                  bucketer = joblib.load(bucketer_path)\n",
    "\n",
    "                  #import data for bucket\n",
    "                  X_train_path = os.path.join(PATH, \"%s/%s_%s/train_data/bucket_%s_prefixes.pickle\" % (dataset_ref, cls_method, method_name, bucketID))                  \n",
    "                  with open (X_train_path, 'rb') as f:\n",
    "                      dt_train_bucket = pickle.load(f)\n",
    "\n",
    "                  X_test_path = os.path.join(PATH, \"%s/%s_%s/test_data/bucket_%s_prefixes.pickle\" % (dataset_ref, cls_method, method_name, bucketID))                  \n",
    "                  with open (X_test_path, 'rb') as f:\n",
    "                      dt_test_bucket = pickle.load(f)\n",
    "\n",
    "                  dt_test_bucket = dt_test_bucket.dropna()\n",
    "                  dt_test_sample = dt_test_bucket.sample(frac = sample_size)\n",
    "                  \n",
    "                  dt_testing_sample = feature_combiner.fit_transform(dt_test_sample)\n",
    "                  lens = dataset_manager.get_prefix_lengths(dt_test_sample)\n",
    "\n",
    "                  feat_len = [len(vec) for vec in dt_testing_sample]\n",
    "\n",
    "              #Get a list of feature names\n",
    "              if cls_method == \"lstm\":\n",
    "                feat_list_path = os.path.join(PATH, \"%s/%s_%s/cls/feature_names.pickle\" % (dataset_ref, cls_method, method_name))\n",
    "                with open (feat_list_path, 'rb') as f:\n",
    "                    file = f.read()\n",
    "                    orig_list = np.array(pickle.loads(file))\n",
    "                    feat_list = orig_list[0]\n",
    "              else:\n",
    "                feat_list = feature_combiner.get_feature_names()\n",
    "\n",
    "              #create explainers now that can be passed later\n",
    "              start_time = time.time()\n",
    "              if cls_method == \"lstm\":\n",
    "                  if len(dt_train_bucket) > 10000:\n",
    "                      training_sample = shap.sample(dt_train_bucket, 10000)\n",
    "                  else:\n",
    "                      training_sample = dt_train_bucket\n",
    "                  shap_explainer = shap.DeepExplainer(cls, training_sample)\n",
    "              else:\n",
    "                  shap_explainer = shap.TreeExplainer(cls)\n",
    "              duration = time.time() - start_time\n",
    "              \n",
    "              print(\"Time taken to create SHAP explainer:\", duration, \"seconds.\")\n",
    "\n",
    "              start_time = time.time()\n",
    "              class_names=['regular','deviant']# regular is 0, deviant is 1, 0 is left, 1 is right\n",
    "              if cls_method == \"lstm\":\n",
    "                  trainingdata = dt_train_bucket\n",
    "                  lime_explainer = lime.lime_tabular.RecurrentTabularExplainer(trainingdata,\n",
    "                                feature_names = feat_list,\n",
    "                                class_names=class_names, discretize_continuous=True)\n",
    "              else:\n",
    "                  trainingdata = feature_combiner.fit_transform(dt_train_bucket)\n",
    "                  lime_explainer = lime.lime_tabular.LimeTabularExplainer(trainingdata,\n",
    "                                feature_names = feat_list,\n",
    "                                class_names=class_names, discretize_continuous=True)\n",
    "              duration = time.time() - start_time\n",
    "              print(\"Time taken to create LIME explainer:\", duration, \"seconds.\")\n",
    "\n",
    "\n",
    "              shap_times = []\n",
    "              lime_times = []\n",
    "\n",
    "              for instance in dt_testing_sample:\n",
    "                #generate data for SHAP\n",
    "                start_time = time.time()\n",
    "                if cls_method != \"lstm\":\n",
    "                  shap_explainer.shap_values(np.array([instance]), check_additivity = False)\n",
    "                else:\n",
    "                  shap_explainer.shap_values(instance)\n",
    "                duration = time.time() - start_time\n",
    "                shap_times.append(duration)\n",
    "\n",
    "                #generate data for LIME\n",
    "                start_time = time.time()\n",
    "                if cls_method == \"lstm\":\n",
    "                  lime_explainer.explain_instance(instance, cls.predict)\n",
    "                else:\n",
    "                  lime_explainer.explain_instance(instance, cls.predict_proba)\n",
    "                duration = time.time() - start_time\n",
    "                lime_times.append(duration)\n",
    "\n",
    "              sep_pref_len.extend(list(lens))\n",
    "              sep_feat_len.extend(list(feat_len))\n",
    "              all_shap_times.extend(shap_times)\n",
    "              all_lime_times.extend(lime_times)\n",
    "\n",
    "              for i in range(2):\n",
    "                all_pref_len.extend(sep_pref_len)\n",
    "                all_feat_len.extend(sep_feat_len)\n",
    "              all_times.extend(all_shap_times)\n",
    "              all_times.extend(all_lime_times)\n",
    "              all_types.extend([\"SHAP\"]*len(all_shap_times))\n",
    "              all_types.extend([\"LIME\"]*len(all_lime_times))\n",
    "\n",
    "          sep_data = pd.DataFrame(data = sep_data_dict)\n",
    "          sep_data.to_csv(sep_timing_path, index = False)\n",
    "\n",
    "          all_data = pd.DataFrame(data = all_data_dict)\n",
    "          all_data.to_csv(all_timing_path, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tuJCUX69yiAO"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(sep_data[\"Prefix Length\"], sep_data[\"SHAP Running Time\"], 'ro')#, y = ['SHAP Running Time', 'LIME Running Time'], x = ['Prefix Length'] )#, hue = data['Feature Vector Length'])\n",
    "ax.plot(sep_data[\"Prefix Length\"], sep_data[\"LIME Running Time\"], 'bo')#, y = ['SHAP Running Time', 'LIME Running Time'], x = ['Prefix Length'] )#, hue = data['Feature Vector Length'])\n",
    "#grid.map(sns.scatterplot, color=\".3\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UORiSY7W90Ow"
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(x = all_data['Prefix Length'], y = all_data['Running Time'], hue = all_data['Explainer'], size = all_data['Feature Vector Length'])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "timing.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
