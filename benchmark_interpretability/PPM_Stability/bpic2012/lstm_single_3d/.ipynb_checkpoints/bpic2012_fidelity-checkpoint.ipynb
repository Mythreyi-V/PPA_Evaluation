{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "import sys\n",
    "#PATH = '/content/drive/My Drive/PPM_Stability/'\n",
    "PATH = \"C:/Users/velmurug/Documents/Stability Experiments/benchmark_interpretability/PPM_Stability/\"\n",
    "#PATH = \"C:/Users/Mythreyi/Documents/GitHub/Stability-Experiments/benchmark_interpretability/PPM_Stability/\"\n",
    "#PATH = \"C:/Users/mythr/Documents/GitHub/Stability-Experiments/benchmark_interpretability/PPM_Stability/\"\n",
    "#PATH = \"C:/Users/n9455647/Documents/GitHub/Stability-Experiments/benchmark_interpretability/PPM_Stability/\"\n",
    "sys.path.append(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost==1.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (1.0.0)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (from xgboost==1.0.0) (1.4.1)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from xgboost==1.0.0) (1.18.5)\n"
     ]
    }
   ],
   "source": [
    "#!pip install lime\n",
    "#!pip install shap\n",
    "#!pip install pandas==0.19.2\n",
    "!pip install xgboost==1.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import EncoderFactory\n",
    "from DatasetManager import DatasetManager\n",
    "import BucketFactory\n",
    "import stability as st #Nogueira, Sechidis, Brown.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import math\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "from sys import argv\n",
    "import pickle\n",
    "from collections import defaultdict, Counter\n",
    "import random\n",
    "import joblib\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import Dense, Embedding, Flatten, Input, LSTM\n",
    "from keras.optimizers import Nadam, RMSprop\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "from tensorflow.keras.backend import print_tensor\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.compat.v1 import disable_v2_behavior#, ConfigProto, Session\n",
    "from tensorflow.compat.v1.keras.backend import get_session\n",
    "disable_v2_behavior()\n",
    "\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "from lime import submodular_pick;\n",
    "\n",
    "import shap\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def imp_df(column_names, importances):\n",
    "        df = pd.DataFrame({'feature': column_names,\n",
    "                       'feature_importance': importances}) \\\n",
    "           .sort_values('feature_importance', ascending = False) \\\n",
    "           .reset_index(drop = True)\n",
    "        return df\n",
    "\n",
    "# plotting a feature importance dataframe (horizontal barchart)\n",
    "def var_imp_plot(imp_df, title, num_feat):\n",
    "        imp_df.columns = ['feature', 'feature_importance']\n",
    "        b= sns.barplot(x = 'feature_importance', y = 'feature', data = imp_df.head(num_feat), orient = 'h', palette=\"Blues_r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_global_explanations(train_X,train_Y, cls, feature_combiner):\n",
    "    \n",
    "    print(\"The number of testing instances is \",len(train_Y))\n",
    "    print(\"The total number of columns is\",train_X.shape[1]);\n",
    "    print(\"The total accuracy is \",cls.score(train_X,train_Y));\n",
    "       \n",
    "    sns.set(rc={'figure.figsize':(10,10), \"font.size\":18,\"axes.titlesize\":18,\"axes.labelsize\":18})\n",
    "    sns.set\n",
    "    feat_names = feature_combiner.get_feature_names()\n",
    "    base_imp = imp_df(feat_names, cls.feature_importances_)\n",
    "    base_imp.head(15)\n",
    "    var_imp_plot(base_imp, 'Feature importance using XGBoost', 15)\n",
    "    return base_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime import submodular_pick\n",
    "def generate_lime_explanations(explainer,test_xi, cls,test_y, submod=False, test_all_data=None, max_feat = 10):\n",
    "    \n",
    "    #print(\"Actual value \", test_y)\n",
    "    exp = explainer.explain_instance(test_xi, \n",
    "                                 cls.predict_proba, num_features=max_feat, labels=[0,1])\n",
    "    \n",
    "    return exp\n",
    "        \n",
    "    if submod==True:\n",
    "        sp_obj=submodular_pick.SubmodularPick(explainer, test_all_data, cls.predict_proba, \n",
    "                                      sample_size=20, num_features=num_features,num_exps_desired=4)\n",
    "        [exp.as_pyplot_figure(label=exp.available_labels()[0]) for exp in sp_obj.sp_explanations];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_samples(shap_explainer, iterations, row, features, top = None):\n",
    "    length = len(features)\n",
    "    \n",
    "    exp = []\n",
    "    rel_exp = []\n",
    "    \n",
    "    for j in range(iterations):\n",
    "        \n",
    "        shap_values = shap_explainer.shap_values(row)\n",
    "        \n",
    "        importances = []\n",
    "        \n",
    "        if type(shap_explainer) == shap.explainers.kernel.KernelExplainer:\n",
    "            for i in range(length):\n",
    "                feat = features[i]\n",
    "                shap_val = shap_values[0][i]\n",
    "                abs_val = abs(shap_values[0][i])\n",
    "                entry = (feat, shap_val, abs_val)\n",
    "                importances.append(entry)\n",
    "                \n",
    "        elif type(shap_explainer) == shap.explainers.tree.TreeExplainer:\n",
    "            for i in range(length):\n",
    "                feat = features[i]\n",
    "                shap_val = shap_values[0][i]\n",
    "                abs_val = abs(shap_values[0][i])\n",
    "                entry = (feat, shap_val, abs_val)\n",
    "                importances.append(entry)\n",
    "        \n",
    "        elif type(shap_explainer) == shap.explainers.deep.DeepExplainer:\n",
    "            for i in range(length):\n",
    "                if len(features.shape) == 2:\n",
    "                    for j in range(len(features[i])):\n",
    "                        feat = features[i][j]\n",
    "                        shap_val = shap_values[0][0][i][j]\n",
    "                        abs_val = abs(shap_values[0][0][i][j])\n",
    "                        entry = (feat, shap_val, abs_val)\n",
    "                        importances.append(entry)\n",
    "                else:\n",
    "                    feat = features[i]\n",
    "                    shap_val = shap_values[0][0][i]\n",
    "                    abs_val = abs(shap_values[0][0][i])\n",
    "                    entry = (feat, shap_val, abs_val)\n",
    "                    importances.append(entry)\n",
    "    \n",
    "        importances.sort(key=lambda tup: tup[2], reverse = True)\n",
    "        \n",
    "        exp.append(importances)\n",
    "\n",
    "        rel_feat = []\n",
    "\n",
    "        if top != None:\n",
    "            for i in range(top):\n",
    "                feat = importances[i]\n",
    "                if feat[2] > 0:\n",
    "                    rel_feat.append(feat)\n",
    "\n",
    "            rel_exp.append(rel_feat)\n",
    "        else:\n",
    "            rel_exp = exp\n",
    "        \n",
    "    return exp, rel_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_distributions(explainer, features, test_x, bin_min = -1, bin_max = 1, bin_width = 0.05):\n",
    "    \n",
    "    if type(explainer) == shap.explainers.tree.TreeExplainer:\n",
    "        #generate shap values for entire test set\n",
    "        shap_values = explainer.shap_values(test_x, check_additivity = False)\n",
    "    #    print(shap_values)\n",
    "        shap_val_feat = np.transpose(shap_values)\n",
    "    #    print(shap_val_feat)\n",
    "        feats = np.transpose(test_x)\n",
    "\n",
    "        shap_distribs = []\n",
    "\n",
    "        #For each feature\n",
    "        for i in range(len(features)):\n",
    "            print (i+1, \"of\", len(features), \"features\")\n",
    "            shap_vals = shap_val_feat[i]\n",
    "    #        print(shap_vals)\n",
    "\n",
    "            #create bins based on shap value ranges\n",
    "            bins = np.arange(bin_min, bin_max, bin_width)\n",
    "\n",
    "            feat_vals = []\n",
    "            for sbin in range(len(bins)):\n",
    "                nl = []\n",
    "                feat_vals.append(nl)\n",
    "\n",
    "            #place relevant feature values into each bin\n",
    "            for j in range(len(shap_vals)):\n",
    "                val = shap_vals[j]\n",
    "                b = 0\n",
    "                cur_bin = bins[b]\n",
    "                idx = b\n",
    "\n",
    "                while val > cur_bin and b < len(bins)-1:\n",
    "                    #print(cur_bin)\n",
    "                    idx = b\n",
    "                    b+=1\n",
    "                    #print(b)\n",
    "                    cur_bin = bins[b]\n",
    "\n",
    "                #print(val, idx)\n",
    "                feat_vals[idx].append(feats[i][j])\n",
    "\n",
    "            #Remove feature values that are outliers\n",
    "            #for each in feat_vals:\n",
    "            #    zscore = stats.zscore(each)\n",
    "                #print(each)\n",
    "            #    for n in range(len(zscore)):\n",
    "            #        if zscore[n] > 3 or zscore[n] < -3:\n",
    "            #            np.delete(zscore, n)\n",
    "            #            del each[n]\n",
    "                #print(each)\n",
    "\n",
    "            #Find min and max values for each shap value bin\n",
    "            mins = []\n",
    "            maxes = []\n",
    "            #width = []\n",
    "            #print(feat_vals)\n",
    "            #n = 0\n",
    "            for each in feat_vals:\n",
    "                if each != []:\n",
    "                    mins.append(min(each))\n",
    "                    maxes.append(max(each))\n",
    "             #       width.append(\"Bin \"+str(n))\n",
    "             #       n+=1\n",
    "            #plt.bar(width, maxes, bottom = mins)\n",
    "            #plt.show()\n",
    "\n",
    "            #Create dictionary with list of bins and max and min feature values for each bin\n",
    "            feat_name = features[i]\n",
    "\n",
    "            feat_dict = {'Feature Name': feat_name}\n",
    "            for each in feat_vals:\n",
    "                if each != []:\n",
    "                    mins.append(min(each))\n",
    "                    maxes.append(max(each))\n",
    "                else:\n",
    "                    mins.append(None)\n",
    "                    maxes.append(None)\n",
    "\n",
    "            feat_dict['bins'] = bins\n",
    "            feat_dict['mins'] = mins\n",
    "            feat_dict['maxes'] = maxes\n",
    "\n",
    "            shap_distribs.append(feat_dict)\n",
    "        \n",
    "    elif type(explainer) == shap.explainers.deep.DeepExplainer:\n",
    "        #generate shap values for entire test set\n",
    "        shap_values = explainer.shap_values(test_x, check_additivity = False)\n",
    "        \n",
    "        for l in range(len(test_x)):\n",
    "    #    print(shap_values)\n",
    "            shap_val_feat = np.transpose(shap_values[l])\n",
    "        #    print(shap_val_feat)\n",
    "            feats = np.transpose(test_x[l])\n",
    "\n",
    "            shap_distribs = []\n",
    "\n",
    "            #For each feature\n",
    "            for i in range(len(features[l])):\n",
    "                print (i+1, \"of\", len(features[l]), \"features\")\n",
    "                shap_vals = shap_val_feat[i]\n",
    "        #        print(shap_vals)\n",
    "\n",
    "                #create bins based on shap value ranges\n",
    "                bins = np.arange(bin_min, bin_max, bin_width)\n",
    "\n",
    "                feat_vals = []\n",
    "                for sbin in range(len(bins)):\n",
    "                    nl = []\n",
    "                    feat_vals.append(nl)\n",
    "\n",
    "                #place relevant feature values into each bin\n",
    "                for j in range(len(shap_vals)):\n",
    "                    val = shap_vals[j]\n",
    "                    b = 0\n",
    "                    cur_bin = bins[b]\n",
    "                    idx = b\n",
    "\n",
    "                    while val > cur_bin and b < len(bins)-1:\n",
    "                        #print(cur_bin)\n",
    "                        idx = b\n",
    "                        b+=1\n",
    "                        #print(b)\n",
    "                        cur_bin = bins[b]\n",
    "\n",
    "                    #print(val, idx)\n",
    "                    feat_vals[idx].append(feats[i][j])\n",
    "\n",
    "                #Remove feature values that are outliers\n",
    "                #for each in feat_vals:\n",
    "                #    zscore = stats.zscore(each)\n",
    "                    #print(each)\n",
    "                #    for n in range(len(zscore)):\n",
    "                #        if zscore[n] > 3 or zscore[n] < -3:\n",
    "                #            np.delete(zscore, n)\n",
    "                #            del each[n]\n",
    "                    #print(each)\n",
    "\n",
    "                #Find min and max values for each shap value bin\n",
    "                mins = []\n",
    "                maxes = []\n",
    "                #width = []\n",
    "                #print(feat_vals)\n",
    "                #n = 0\n",
    "                for each in feat_vals:\n",
    "                    if each != []:\n",
    "                        mins.append(min(each))\n",
    "                        maxes.append(max(each))\n",
    "                 #       width.append(\"Bin \"+str(n))\n",
    "                 #       n+=1\n",
    "                #plt.bar(width, maxes, bottom = mins)\n",
    "                #plt.show()\n",
    "\n",
    "                #Create dictionary with list of bins and max and min feature values for each bin\n",
    "                feat_name = features[i]\n",
    "\n",
    "                feat_dict = {'Feature Name': feat_name}\n",
    "                for each in feat_vals:\n",
    "                    if each != []:\n",
    "                        mins.append(min(each))\n",
    "                        maxes.append(max(each))\n",
    "                    else:\n",
    "                        mins.append(None)\n",
    "                        maxes.append(None)\n",
    "\n",
    "                feat_dict['bins'] = bins\n",
    "                feat_dict['mins'] = mins\n",
    "                feat_dict['maxes'] = maxes\n",
    "\n",
    "                shap_distribs.append(feat_dict)\n",
    "        \n",
    "    return shap_distribs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "['bpic2012_accepted']"
      ],
      "text/plain": [
       "['bpic2012_accepted']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_ref = \"bpic2012\"\n",
    "params_dir = PATH + \"params\"\n",
    "results_dir = \"results\"\n",
    "bucket_method = \"single\"\n",
    "cls_encoding = \"3d\"\n",
    "cls_method = \"lstm\"\n",
    "\n",
    "gap = 1\n",
    "n_iter = 1\n",
    "\n",
    "method_name = \"%s_%s\"%(bucket_method, cls_encoding)\n",
    "\n",
    "generate_samples = False\n",
    "generate_lime = True\n",
    "generate_kernel_shap = False\n",
    "generate_model_shap = True\n",
    "\n",
    "sample_size = 2\n",
    "exp_iter = 10\n",
    "#max_feat = 10\n",
    "\n",
    "dataset_ref_to_datasets = {\n",
    "    #\"bpic2011\": [\"bpic2011_f%s\"%formula for formula in range(1,5)],\n",
    "    \"bpic2015\": [\"bpic2015_%s_f2\"%(municipality) for municipality in range(5,6)],\n",
    "    \"bpic2017\" : [\"bpic2017_accepted\"],\n",
    "    \"bpic2012\" : [\"bpic2012_accepted\"],\n",
    "    #\"insurance\": [\"insurance_activity\", \"insurance_followup\"],\n",
    "    \"sepsis_cases\": [\"sepsis_cases_1\"],# \"sepsis_cases_2\", \"sepsis_cases_4\"]\n",
    "    \"production\" : [\"production\"]\n",
    "}\n",
    "\n",
    "datasets = [dataset_ref] if dataset_ref not in dataset_ref_to_datasets else dataset_ref_to_datasets[dataset_ref]\n",
    "\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------SHAP----------------------------------------------\n",
      "Bucket all\n",
      "get everything to create model\n",
      "Parameters loaded\n",
      "defining input layer\n",
      "adding lstm layers\n",
      "adding dense layers\n",
      "putting together layers\n",
      "choosing optimiser\n",
      "adding weights to model\n",
      "compiling model\n"
     ]
    }
   ],
   "source": [
    "#Try SHAP\n",
    "print(\"----------------------------------------------SHAP----------------------------------------------\")\n",
    "\n",
    "if generate_model_shap:\n",
    "    for dataset_name in datasets:\n",
    "\n",
    "        dataset_manager = DatasetManager(dataset_name)\n",
    "\n",
    "        for ii in range(n_iter):\n",
    "            if cls_method == \"lstm\":\n",
    "                num_buckets = range(1)\n",
    "            else:\n",
    "                num_buckets = range(len([name for name in os.listdir(os.path.join(PATH,'%s/%s_%s/models'% (dataset_ref, cls_method, method_name)))]))\n",
    "\n",
    "            all_shap_changes = []\n",
    "            all_lens = []\n",
    "            all_probas = []\n",
    "            all_case_ids = []\n",
    "\n",
    "            pos_shap_changes = []\n",
    "            pos_probas = []\n",
    "            pos_nr_events = []\n",
    "            pos_case_ids = []\n",
    "\n",
    "            neg_shap_changes = []\n",
    "            neg_probas = []\n",
    "            neg_nr_events = []\n",
    "            neg_case_ids = []\n",
    "\n",
    "            for bucket in list(num_buckets):\n",
    "                bucketID = \"all\"\n",
    "                print ('Bucket', bucketID)\n",
    "\n",
    "                #import everything needed to sort and predict\n",
    "                if cls_method == \"lstm\":\n",
    "                    print(\"get everything to create model\")\n",
    "                    params_path = os.path.join(PATH, \"%s/%s_%s/cls/params.pickle\" % (dataset_ref, cls_method, method_name))\n",
    "                    with open(params_path, 'rb') as f:\n",
    "                        args = pickle.load(f)\n",
    "\n",
    "                    max_len = args['max_len']\n",
    "                    data_dim = args['data_dim']\n",
    "                    print(\"Parameters loaded\")\n",
    "\n",
    "                    #create model\n",
    "                    print(\"defining input layer\")\n",
    "                    main_input = Input(shape=(max_len, data_dim), name='main_input')\n",
    "                    \n",
    "                    print(\"adding lstm layers\")\n",
    "                    if args[\"lstm_layers\"][\"layers\"] == \"one\":\n",
    "                        l2_3 = LSTM(args['lstm1_nodes'], input_shape=(max_len, data_dim), implementation=2, \n",
    "                                    kernel_initializer='glorot_uniform', return_sequences=False, \n",
    "                                    recurrent_dropout=args['lstm1_dropouts'], stateful = False)(main_input)\n",
    "                        b2_3 = BatchNormalization()(l2_3)\n",
    "\n",
    "                    if args[\"lstm_layers\"][\"layers\"] == \"two\":\n",
    "                        l1 = LSTM(args['lstm1_nodes'], input_shape=(max_len, data_dim), implementation=2, \n",
    "                                kernel_initializer='glorot_uniform', return_sequences=True, \n",
    "                                recurrent_dropout=args['lstm1_dropouts'], stateful = False)(main_input)\n",
    "                        b1 = BatchNormalization()(l1)\n",
    "                        l2_3 = LSTM(args[\"lstm_layers\"][\"lstm2_nodes\"], activation=\"sigmoid\", \n",
    "                                    implementation=2, kernel_initializer='glorot_uniform', return_sequences=False, \n",
    "                                    recurrent_dropout=args[\"lstm_layers\"][\"lstm2_dropouts\"], stateful = False)(b1)\n",
    "                        b2_3 = BatchNormalization()(l2_3)\n",
    "                        \n",
    "                    if args[\"lstm_layers\"][\"layers\"] == \"three\":\n",
    "                        l1 = LSTM(args['lstm1_nodes'], input_shape=(max_len, data_dim),implementation=2, \n",
    "                                kernel_initializer='glorot_uniform', return_sequences=True, \n",
    "                                recurrent_dropout=args['lstm1_dropouts'], stateful = False)(main_input)\n",
    "                        b1 = BatchNormalization()(l1)\n",
    "                        l2 = LSTM(args[\"lstm_layers\"][\"lstm2_nodes\"], activation=\"sigmoid\", \n",
    "                                    implementation=2, kernel_initializer='glorot_uniform', return_sequences=True, \n",
    "                                    recurrent_dropout=args[\"lstm_layers\"][\"lstm2_dropouts\"], stateful = False)(b1)\n",
    "                        b2 = BatchNormalization()(l2)\n",
    "                        l2_3 = LSTM(args[\"lstm_layers\"][\"lstm3_nodes\"], activation=\"sigmoid\", \n",
    "                                    implementation=2, kernel_initializer='glorot_uniform', return_sequences=False, \n",
    "                                    recurrent_dropout=args[\"lstm_layers\"][\"lstm3_dropouts\"], stateful = False)(b2)\n",
    "                        b2_3 = BatchNormalization()(l2_3)\n",
    "                    \n",
    "                    print(\"adding dense layers\")\n",
    "                    if args['dense_layers']['layers'] == \"two\":\n",
    "                        d1 = Dense(args['dense_layers']['dense2_nodes'], activation = \"relu\")(b2_3)\n",
    "                        outcome_output = Dense(2, activation='sigmoid', kernel_initializer='glorot_uniform', name='outcome_output')(d1)\n",
    "\n",
    "                    else:\n",
    "                        outcome_output = Dense(2, activation='sigmoid', kernel_initializer='glorot_uniform', name='outcome_output')(b2_3)\n",
    "                    \n",
    "                    print(\"putting together layers\")\n",
    "                    cls = Model(inputs=[main_input], outputs=[outcome_output])\n",
    "                    \n",
    "                    print(\"choosing optimiser\")\n",
    "                    if args['optimizer'] == \"adam\":\n",
    "                        opt = Nadam(lr=args['learning_rate'], beta_1=0.9, beta_2=0.999, epsilon=1e-08, schedule_decay=0.004, clipvalue=3)\n",
    "                    elif args['optimizer'] == \"rmsprop\":\n",
    "                        opt = RMSprop(lr=args['learning_rate'], rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "                        \n",
    "                    print(\"adding weights to model\")\n",
    "                    checkpoint_path = os.path.join(PATH, \"%s/%s_%s/cls/checkpoint.cpt\" % (dataset_ref, cls_method, method_name))\n",
    "                    weights = cls.load_weights(checkpoint_path)\n",
    "                    #print(weights.assert_consumed())\n",
    "                     \n",
    "                    print(\"compiling model\")\n",
    "                    cls.compile(loss='binary_crossentropy', optimizer=opt)\n",
    "                else:\n",
    "                    pipeline_path = os.path.join(PATH, \"%s/%s_%s/pipelines/pipeline_bucket_%s.joblib\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "                    feat_comb_path = os.path.join(PATH, \"%s/%s_%s/bucketers_and_encoders/feature_combiner_bucket_%s.joblib\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "                    bucketer_path = os.path.join(PATH, \"%s/%s_%s/bucketers_and_encoders/bucketer_bucket_%s.joblib\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "                    cls_path = os.path.join(PATH, \"%s/%s_%s/models/cls_bucket_%s.joblib\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "\n",
    "                    predictor = joblib.load(pipeline_path)\n",
    "                    cls = joblib.load(cls_path)\n",
    "                    feature_combiner = joblib.load(feat_comb_path)\n",
    "                    bucketer = joblib.load(bucketer_path)\n",
    "\n",
    "                #import data for bucket\n",
    "                X_train_path = os.path.join(PATH, \"%s/%s_%s/train_data/bucket_%s_prefixes.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "                Y_train_path = os.path.join(PATH, \"%s/%s_%s/train_data/bucket_%s_labels.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "                X_test_path = os.path.join(PATH, \"%s/%s_%s/test_data/bucket_%s_prefixes.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "                Y_test_path = os.path.join(PATH, \"%s/%s_%s/test_data/bucket_%s_labels.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "\n",
    "                with open (X_train_path, 'rb') as f:\n",
    "                    dt_train_bucket = pickle.load(f)\n",
    "                with open (Y_train_path, 'rb') as f:\n",
    "                    train_y = pickle.load(f)\n",
    "                with open (X_test_path, 'rb') as f:\n",
    "                    dt_test_bucket = pickle.load(f)\n",
    "                with open (Y_test_path, 'rb') as f:\n",
    "                    test_y = pickle.load(f)\n",
    "\n",
    "                #import previously identified samples\n",
    "                tn_path = os.path.join(PATH, \"%s/%s_%s/samples/true_neg_bucket_%s_.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "                tp_path = os.path.join(PATH, \"%s/%s_%s/samples/true_pos_bucket_%s_.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "                fn_path = os.path.join(PATH, \"%s/%s_%s/samples/false_neg_bucket_%s_.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "                fp_path = os.path.join(PATH, \"%s/%s_%s/samples/false_pos_bucket_%s_.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "\n",
    "                sample_instances = []\n",
    "\n",
    "                with open (tn_path, 'rb') as f:\n",
    "                    tn_list = pickle.load(f)\n",
    "                with open (tp_path, 'rb') as f:\n",
    "                    tp_list = pickle.load(f)\n",
    "                with open (fn_path, 'rb') as f:\n",
    "                    fn_list = pickle.load(f)\n",
    "                with open (fp_path, 'rb') as f:\n",
    "                    fp_list = pickle.load(f)\n",
    "\n",
    "                #save results to a list\n",
    "                sample_instances.append(tn_list)\n",
    "                sample_instances.append(tp_list)\n",
    "                sample_instances.append(fn_list)\n",
    "                sample_instances.append(fp_list)\n",
    "                \n",
    "                #break\n",
    "\n",
    "                if cls_method == \"xgboost\":\n",
    "                    tree_explainer = shap.TreeExplainer(cls)\n",
    "                    test_x = feature_combiner.fit_transform(dt_test_bucket)\n",
    "                    feat_list = feature_combiner.get_feature_names()\n",
    "                elif cls_method == \"lstm\":\n",
    "                    if len(dt_train_bucket) >10000:\n",
    "                        training_sample = shap.sample(dt_train_bucket, 10000)\n",
    "                    else:\n",
    "                        training_sample = dt_train_bucket\n",
    "                    deep_explainer = shap.DeepExplainer(cls, training_sample)\n",
    "                    test_x = dt_test_bucket\n",
    "                    feat_list_path = os.path.join(PATH, \"%s/%s_%s/cls/feature_names.pickle\" % (dataset_ref, cls_method, method_name))\n",
    "                    with open(feat_list_path, 'rb') as f:\n",
    "                        file = f.read()\n",
    "                        feat_list = np.array(pickle.loads(file))\n",
    "                #break\n",
    "                type_list = ['True Negatives', 'True Positives', 'False Negatives', 'False Positives']\n",
    "                max_feat = round(len(feat_list)*0.1)\n",
    "                #print(max_feat)\n",
    "                \n",
    "                print(\"Generating distributions for bucket\")\n",
    "                start_time = time.time()\n",
    "                if cls_method == \"lstm\":\n",
    "                    distribs = generate_distributions(deep_explainer, feat_list, test_x)\n",
    "                if cls_method == \"xgboost\":\n",
    "                    distribs = generate_distributions(tree_explainer, feat_list, test_x)\n",
    "                elapsed = time.time()-start_time\n",
    "                print(\"Time taken to generate distribution:\", elapsed)\n",
    "                \n",
    "                start_time = time.time()\n",
    "                for i_type in range(len(sample_instances[:1])):\n",
    "                    changes = []\n",
    "                    probas = []\n",
    "                    nr_events = []\n",
    "                    case_ids = []\n",
    "\n",
    "                    for n in range(len(sample_instances[i_type][:1])):\n",
    "                        print(\"Category %s of %s. Instance %s of %s\" %(i_type+1, len(sample_instances), n+1, len(sample_instances[i_type])))\n",
    "                        instance = sample_instances[i_type][n]\n",
    "\n",
    "                        ind = instance['predicted']\n",
    "                        case_ids.append(instance['caseID'])\n",
    "                        p1 = instance['proba']\n",
    "                        probas.append(p1)\n",
    "                        nr_events.append(instance['nr_events'])\n",
    "                        input_ = instance['input']\n",
    "\n",
    "                        if cls_method != \"lstm\":\n",
    "                            test_x_group = feature_combiner.fit_transform(input_) \n",
    "                        else:\n",
    "                            test_x_group = input_\n",
    "                        #test_x=np.transpose(test_x_group[0])\n",
    "                        #print(test_x)\n",
    "                        #print(p1)\n",
    "\n",
    "                        print(\"Creating explanations\")\n",
    "                        if cls_method == \"xgboost\":\n",
    "                            exp, rel_exp = create_samples(tree_explainer, exp_iter, test_x_group, feat_list, top = max_feat)\n",
    "                        elif cls_method == \"lstm\":\n",
    "                            exp, rel_exp = create_samples(deep_explainer, exp_iter, test_x_group, test_x_group, top = max_feat)\n",
    "\n",
    "                        features = []\n",
    "                        shap_vals = []\n",
    "                        \n",
    "                        print(\"Identifying relevant features\")\n",
    "                        for explanation in rel_exp:\n",
    "                            features.extend([feat[0] for feat in explanation])\n",
    "                            shap_vals.extend([feat for feat in explanation])\n",
    "\n",
    "                        counter = Counter(features).most_common(max_feat)\n",
    "\n",
    "                        feats = [feat[0] for feat in counter]\n",
    "\n",
    "                        rel_feats = []\n",
    "                        for feat in feats:\n",
    "                            vals = [i[1] for i in shap_vals if i[0] == feat]\n",
    "                            #print(feat, vals)\n",
    "                            val = np.mean(vals)\n",
    "                            rel_feats.append((feat, val))\n",
    "\n",
    "                        intervals = []\n",
    "                        for item in rel_feats:\n",
    "                            feat = item [0]\n",
    "                            val = item[1]\n",
    "\n",
    "                            print(\"Creating distribution for feature\", rel_feats.index(item)+1, \"of\", len(rel_feats))\n",
    "\n",
    "                            n = feat_list.index(feat)\n",
    "                            feat_dict = distribs[n]\n",
    "\n",
    "                            if feat_dict['Feature Name'] != feat:\n",
    "                                for each in distribs:\n",
    "                                    if feat_dict['Feature Name'] == feat:\n",
    "                                        feat_dict = each\n",
    "\n",
    "                            bins = feat_dict['bins']\n",
    "                            mins = feat_dict['mins']\n",
    "                            maxes = feat_dict['maxes']\n",
    "                            #print (feat, val, bins, mins, maxes)\n",
    "\n",
    "                            i = 0\n",
    "                            while val > bins[i] and i < len(bins)-1:\n",
    "                                idx = i\n",
    "                                i+=1\n",
    "                            #print (i)\n",
    "                            if mins[i] != None:\n",
    "                                min_val = mins[i]\n",
    "                                max_val = maxes[i]\n",
    "                            else:\n",
    "                                j = i\n",
    "                                while mins[j] == None and j > 0:\n",
    "                                    min_val = mins[j-1]\n",
    "                                    max_val = maxes[j-1]\n",
    "                                    j = j-1\n",
    "\n",
    "                            interval = max_val - min_val\n",
    "                            \n",
    "                            if type(feat_list) == list:\n",
    "                                index = feat_list.index(feat)\n",
    "                            elif type(feat_list) == numpy.ndarray:\n",
    "                                index = [int(np.where(feat_list==feat)[0]),int(np.where(feat_list==feat)[1])]\n",
    "                            int_min = max_val\n",
    "                            int_max = max_val + interval\n",
    "                            intervals.append((feat, index, int_min, int_max))\n",
    "\n",
    "\n",
    "                        diffs = []\n",
    "\n",
    "                        for iteration in range(exp_iter):\n",
    "                            print(\"Pertubing - Run\", iteration+1)\n",
    "                            alt_x = np.copy(test_x_group)\n",
    "                            #print(\"original:\", alt_x)\n",
    "                            for each in intervals:\n",
    "                                new_val = random.uniform(each[2], each[3])\n",
    "                                if cls_encoding == \"3d\":\n",
    "                                    alt_x[0][each[1][0]][each[1][1]] = new_val\n",
    "                                else:\n",
    "                                    alt_x[0][each[1]] = new_val\n",
    "                            if cls_method != \"lstm\":\n",
    "                                p2 = cls.predict_proba(alt_x)[0][ind]\n",
    "                            else:\n",
    "                                p2 = cls.predict(alt_x)[0][ind]\n",
    "                            diff = p1-p2\n",
    "                            diffs.append(diff)\n",
    "\n",
    "                        changes.append(np.mean(diffs))\n",
    "\n",
    "                        instance['shap_fid_change'] = diffs\n",
    "                        #print(\"RMSE for instance:\", np.std(diffs))\n",
    "\n",
    "\n",
    "                        if ind == 0:\n",
    "                            pos_shap_changes.append(abs(diff))#np.std(diffs))\n",
    "                            pos_probas.append(p1)\n",
    "                            pos_nr_events.append(instance['nr_events'])\n",
    "                            pos_case_ids.append(instance['caseID'])\n",
    "                        else:\n",
    "                            neg_shap_changes.append(abs(diff))#np.std(diffs))\n",
    "                            neg_probas.append(p1)\n",
    "                            neg_nr_events.append(instance['nr_events'])\n",
    "                            neg_case_ids.append(instance['caseID'])\n",
    "\n",
    "#                     fig, ax = plt.subplots()\n",
    "#                     ax.plot(probas, changes, 'ro', label = \"SHAP\")\n",
    "#                     ax.set_xlabel(\"Prefix Length\")\n",
    "#                     ax.set_ylabel(\"Change in prediction probability\")\n",
    "#                     #ax.legend(frameon = False, bbox_to_anchor=(1, 1), loc = 'upper left')\n",
    "#                     plt.yticks(np.arange(0,1.1, 0.1))\n",
    "#                     plt.title(\"Prefix length and change in prediction probability - %s (Bucket %s)\" %(type_list[i_type], bucketID))\n",
    "#                     plt.show()\n",
    "\n",
    "#                     fig2, ax2 = plt.subplots()\n",
    "#                     ax2.plot(nr_events, changes, 'ro', label = \"SHAP\")\n",
    "#                     ax2.set_xlabel(\"Prediction Probability\")\n",
    "#                     ax2.set_ylabel(\"Change in prediction probability\")\n",
    "#                     #ax2.legend(frameon = False, bbox_to_anchor=(1, 1), loc = 'upper left')\n",
    "#                     plt.yticks(np.arange(0,1.1, 0.1))\n",
    "#                     plt.title(\"Prediction probability and change in prediction probability - %s (Bucket %s)\" %(type_list[i_type], bucketID))\n",
    "#                     plt.show()\n",
    "\n",
    "#                     all_shap_changes.extend(changes)\n",
    "#                     all_lens.extend(nr_events)\n",
    "#                     all_probas.extend(probas)\n",
    "#                     all_case_ids.extend(case_ids)\n",
    "\n",
    "                #Save dictionaries updated with scores\n",
    "                with open(tn_path, 'wb') as f:\n",
    "                    pickle.dump(sample_instances[0], f)\n",
    "                with open(tp_path, 'wb') as f:\n",
    "                    pickle.dump(sample_instances[1], f)\n",
    "                with open(fn_path, 'wb') as f:\n",
    "                    pickle.dump(sample_instances[2], f)\n",
    "                with open(fp_path, 'wb') as f:\n",
    "                    pickle.dump(sample_instances[3], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Functional' object has no attribute 'predict_proba'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-1a67d1fb82ef>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0malt_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0malt_x\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Functional' object has no attribute 'predict_proba'"
     ]
    }
   ],
   "source": [
    "print(cls.predict_proba(np.array([test_x])))\n",
    "\n",
    "alt_x = np.copy(test_x)\n",
    "alt_x[0] = 0\n",
    "\n",
    "print(cls.predict_proba(np.array([alt_x])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "array = np.array([[1,3,5,6],[7,4,5,6]])\n",
    "\n",
    "np.where(array==7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots()\n",
    "# ax.plot(all_lens, all_shap_changes, 'ro', label = \"SHAP\")\n",
    "# ax.set_xlabel(\"Prefix Length\")\n",
    "# ax.set_ylabel(\"Change in prediction probability\")\n",
    "# #ax.legend(frameon = False, bbox_to_anchor=(1, 1), loc = 'upper left')\n",
    "# plt.yticks(np.arange(0,1.1, 0.1))\n",
    "# plt.title(\"Prefix length and change in prediction probability - All\")\n",
    "# plt.show()\n",
    "\n",
    "# fig2, ax2 = plt.subplots()\n",
    "# ax2.plot(all_probas, all_shap_changes, 'ro', label = \"SHAP\")\n",
    "# ax2.set_xlabel(\"Prediction Probability\")\n",
    "# ax2.set_ylabel(\"Change in prediction probability\")\n",
    "# #ax2.legend(frameon = False, bbox_to_anchor=(1, 1), loc = 'upper left')\n",
    "# plt.yticks(np.arange(0,1.1, 0.1))\n",
    "# plt.title(\"Prediction probability and change in prediction probability - All\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig2, ax2 = plt.subplots()\n",
    "# ax2.plot(pos_probas, pos_shap_changes, 'ro', label = \"SHAP\")\n",
    "# ax2.set_xlabel(\"Prediction Probability\")\n",
    "# ax2.set_ylabel(\"Change in prediction probability\")\n",
    "# #ax2.legend(frameon = False, bbox_to_anchor=(1, 1), loc = 'upper left')\n",
    "# plt.yticks(np.arange(0,1.1, 0.1))\n",
    "# plt.title(\"Prediction probability and change in prediction probability - Positives\")\n",
    "# plt.show()\n",
    "\n",
    "# fig2, ax2 = plt.subplots()\n",
    "# ax2.plot(neg_probas, neg_shap_changes, 'ro', label = \"SHAP\")\n",
    "# ax2.set_xlabel(\"Prediction Probability\")\n",
    "# ax2.set_ylabel(\"Change in prediction probability\")\n",
    "# #ax2.legend(frameon = False, bbox_to_anchor=(1, 1), loc = 'upper left')\n",
    "# plt.yticks(np.arange(0,1.1, 0.1))\n",
    "# plt.title(\"Prediction probability and change in prediction probability - Negatives\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if generate_lime:\n",
    "    for dataset_name in datasets:\n",
    "\n",
    "        dataset_manager = DatasetManager(dataset_name)\n",
    "\n",
    "        for ii in range(n_iter):\n",
    "            num_buckets = range(len([name for name in os.listdir(os.path.join(PATH,'%s/%s_%s/models'% (dataset_ref, cls_method, method_name)))]))\n",
    "\n",
    "            all_lime_changes = []\n",
    "            all_lens = []\n",
    "            all_probas = []\n",
    "            all_case_ids = []\n",
    "\n",
    "            pos_lime_changes = []\n",
    "            pos_probas = []\n",
    "            pos_nr_events = []\n",
    "            pos_case_ids = []\n",
    "\n",
    "            neg_lime_changes = []\n",
    "            neg_probas = []\n",
    "            neg_nr_events = []\n",
    "            neg_case_ids = []\n",
    "\n",
    "            for bucket in list(num_buckets):\n",
    "                bucketID = bucket+1\n",
    "                print ('Bucket', bucketID)\n",
    "\n",
    "                #import everything needed to sort and predict\n",
    "                feat_comb_path = os.path.join(PATH, \"%s/%s_%s/bucketers_and_encoders/feature_combiner_bucket_%s.joblib\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "                cls_path = os.path.join(PATH, \"%s/%s_%s/models/cls_bucket_%s.joblib\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "                cls = joblib.load(cls_path)\n",
    "                feature_combiner = joblib.load(feat_comb_path)\n",
    "\n",
    "                #import data for bucket\n",
    "                X_train_path = os.path.join(PATH, \"%s/%s_%s/train_data/bucket_%s_prefixes.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "                Y_train_path = os.path.join(PATH, \"%s/%s_%s/train_data/bucket_%s_labels.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "\n",
    "                with open (X_train_path, 'rb') as f:\n",
    "                    dt_train_bucket = pickle.load(f)\n",
    "                with open (Y_train_path, 'rb') as f:\n",
    "                    train_y = pickle.load(f)\n",
    "                #with open (X_test_path, 'rb') as f:\n",
    "                #    dt_test_bucket = pickle.load(f)\n",
    "                #with open (Y_test_path, 'rb') as f:\n",
    "                #    test_y = pickle.load(f)\n",
    "\n",
    "                #import previously identified samples\n",
    "                tn_path = os.path.join(PATH, \"%s/%s_%s/samples/true_neg_bucket_%s_.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "                tp_path = os.path.join(PATH, \"%s/%s_%s/samples/true_pos_bucket_%s_.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "                fn_path = os.path.join(PATH, \"%s/%s_%s/samples/false_neg_bucket_%s_.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "                fp_path = os.path.join(PATH, \"%s/%s_%s/samples/false_pos_bucket_%s_.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "\n",
    "                sample_instances = []\n",
    "\n",
    "                with open (tn_path, 'rb') as f:\n",
    "                    tn_list = pickle.load(f)\n",
    "                with open (tp_path, 'rb') as f:\n",
    "                    tp_list = pickle.load(f)\n",
    "                with open (fn_path, 'rb') as f:\n",
    "                    fn_list = pickle.load(f)\n",
    "                with open (fp_path, 'rb') as f:\n",
    "                    fp_list = pickle.load(f)\n",
    "\n",
    "                #save results to a list\n",
    "                sample_instances.append(tn_list)\n",
    "                sample_instances.append(tp_list)\n",
    "                sample_instances.append(fn_list)\n",
    "                sample_instances.append(fp_list)\n",
    "                \n",
    "                #get the training data as a matrix\n",
    "                trainingdata = feature_combiner.fit_transform(dt_train_bucket);\n",
    "                #importance = generate_global_explanations(trainingdata,train_y, cls, feature_combiner)\n",
    "\n",
    "                feat_list = feature_combiner.get_feature_names()\n",
    "                max_feat = round(len(feat_list)*0.1)\n",
    "                class_names=['regular','deviant']# regular is 0, deviant is 1, 0 is left, 1 is right\n",
    "                lime_explainer = lime.lime_tabular.LimeTabularExplainer(trainingdata, feature_names = feat_list, \n",
    "                                                                        class_names=class_names, discretize_continuous=True)\n",
    "                type_list = ['True Negatives', 'True Positives', 'False Negatives', 'False Positives']\n",
    "\n",
    "                for i in list(range(len(sample_instances))):\n",
    "                    changes = []\n",
    "                    probas = []\n",
    "                    nr_events = []\n",
    "                    case_ids = []\n",
    "\n",
    "                    for j in list(range(len(sample_instances[i]))):\n",
    "                        print(\"Category %s of %s. Instance %s of %s\" %(i+1, len(sample_instances), j+1, len(sample_instances[i])))\n",
    "                        instance = sample_instances[i][j]\n",
    "\n",
    "                        ind = instance['predicted']\n",
    "                        case_ids.append(instance['caseID'])\n",
    "                        p1 = instance['proba']\n",
    "                        probas.append(p1)\n",
    "                        #print(\"proba:\", p1)\n",
    "                        nr_events.append(instance['nr_events'])\n",
    "                        input_ = instance['input']\n",
    "\n",
    "                        test_x_group = feature_combiner.fit_transform(input_) \n",
    "                        test_x=np.transpose(test_x_group[0])\n",
    "                        #print(test_x)\n",
    "                        #print(p1)\n",
    "\n",
    "                        explanations = []\n",
    "                        for iteration in range(exp_iter):\n",
    "                            lime_exp = generate_lime_explanations(lime_explainer, test_x, cls, input_, max_feat = max_feat)\n",
    "                            explanation = lime_exp.as_list()\n",
    "                            explanations.extend(explanation)\n",
    "\n",
    "                        features = []\n",
    "                        for explanation in explanations:\n",
    "                            features.append(explanation[0])\n",
    "\n",
    "                        counter = Counter(features)\n",
    "                        check_dup = []\n",
    "                        for feat in feat_list:\n",
    "                            for feature in counter:\n",
    "                                if feat in feature:\n",
    "                                    check_dup.append(feat)\n",
    "\n",
    "                        dup_counter = Counter(check_dup)\n",
    "                        duplicated = [feat for feat in dup_counter if dup_counter[feat] > 1]\n",
    "\n",
    "                        for each in duplicated:\n",
    "                            dpls = []\n",
    "                            vals = []\n",
    "                            for feat in counter.keys():\n",
    "                                if each in feat:\n",
    "                                    dpls.append(feat)\n",
    "                                    vals.append(counter[feat])\n",
    "                            keepval = vals.index(max(vals))\n",
    "                            for n in range(len(dpls)):\n",
    "                                if n != keepval:\n",
    "                                    del counter[dpls[n]]\n",
    "\n",
    "                        rel_feat = counter.most_common(max_feat)\n",
    "\n",
    "                        intervals = []\n",
    "\n",
    "                        for item in rel_feat:\n",
    "                            print(\"Creating distribution for feature\", rel_feat.index(item))\n",
    "                            feat = item[0]\n",
    "                            #print(item)\n",
    "                            #print(feat)\n",
    "                            for n in range(len(feat_list)):\n",
    "                                if feat_list[n] in feat:\n",
    "                                    if (\"<\" or \"<=\") in feat and (\">\" or \">=\") in feat:\n",
    "                                        two_sided = True\n",
    "                                        parts = feat.split(' ')\n",
    "                                        l_bound = float(parts[0])\n",
    "                                        u_bound = float(parts[-1])\n",
    "                                        interval = u_bound - l_bound\n",
    "                                        new_min = u_bound\n",
    "                                        new_max = u_bound + interval\n",
    "                                    else:\n",
    "                                        two_sided = False\n",
    "                                        parts = feat.split(' ')\n",
    "                                        if parts[-2] == \"<=\" or parts[-2] == \"<\":\n",
    "                                            u_bound = float(parts[-1])\n",
    "                                            if u_bound != 0:\n",
    "                                                interval = math.ceil(u_bound*1.1)\n",
    "                                            else:\n",
    "                                                interval = 5\n",
    "                                            new_min = u_bound\n",
    "                                            new_max = u_bound + interval\n",
    "                                        elif parts[-2] == \">=\" or parts[-2] == \">\":\n",
    "                                            l_bound = float(parts[-1])\n",
    "                                            if l_bound != 0:\n",
    "                                                interval = math.ceil(l_bound*1.1)\n",
    "                                            else:\n",
    "                                                interval = 5\n",
    "                                            new_max = l_bound\n",
    "                                            new_min = l_bound - interval\n",
    "                                        else:\n",
    "                                            bound = float(parts[-1])\n",
    "                                            interval = math.ceil((bound*1.1)/2)\n",
    "                                            new_min = bound\n",
    "                                            new_max = bound+interval\n",
    "                                    feature_name = feat_list[n]\n",
    "                                    index = n\n",
    "                                    int_min = new_min\n",
    "                                    int_max = new_max\n",
    "                                    intervals.append((feature_name, index, int_min, int_max))\n",
    "\n",
    "                        diffs = []\n",
    "                        for iteration in range(exp_iter):\n",
    "                            print(\"Pertubing - Run\", iteration+1)\n",
    "                            alt_x = np.copy(test_x_group)\n",
    "                            #print(\"original:\", alt_x)\n",
    "                            for each in intervals:\n",
    "                                new_val = random.uniform(each[2], each[3])\n",
    "                                alt_x[0][each[1]] = new_val\n",
    "                            p2 = cls.predict_proba(alt_x)[0][ind]\n",
    "                            diff = p1-p2\n",
    "                            diffs.append(diff)\n",
    "\n",
    "                        changes.append(np.mean(diffs))\n",
    "                        \n",
    "                        instance['lime_fid_change'] = diffs\n",
    "                        #print(\"RMSE for instance:\", np.std(diffs))\n",
    "\n",
    "\n",
    "                        if ind == 0:\n",
    "                            pos_lime_changes.append(abs(diff))#np.std(diffs))\n",
    "                            pos_probas.append(p1)\n",
    "                            pos_nr_events.append(instance['nr_events'])\n",
    "                            pos_case_ids.append(instance['caseID'])\n",
    "                        else:\n",
    "                            neg_lime_changes.append(abs(diff))#np.std(diffs))\n",
    "                            neg_probas.append(p1)\n",
    "                            neg_nr_events.append(instance['nr_events'])\n",
    "                            neg_case_ids.append(instance['caseID'])\n",
    "\n",
    "#                     fig, ax = plt.subplots()\n",
    "#                     ax.plot(nr_events, changes, 'bo', label = \"LIME\")\n",
    "#                     ax.set_xlabel(\"Prefix Length\")\n",
    "#                     ax.set_ylabel(\"Change in prediction probability\")\n",
    "#                     #ax.legend(frameon = False, bbox_to_anchor=(1, 1), loc = 'upper left')\n",
    "#                     #plt.yticks(np.arange(0,1.1, 0.1))\n",
    "#                     plt.title(\"Prefix length and change in prediction probability - %s (Bucket %s)\" %(type_list[i], bucketID))\n",
    "#                     plt.show()\n",
    "\n",
    "#                     fig2, ax2 = plt.subplots()\n",
    "#                     ax2.plot(probas, changes, 'bo', label = \"LIME\")\n",
    "#                     ax2.set_xlabel(\"Prediction Probability\")\n",
    "#                     ax2.set_ylabel(\"Change in prediction probability\")\n",
    "#                     #ax2.legend(frameon = False, bbox_to_anchor=(1, 1), loc = 'upper left')\n",
    "#                     plt.yticks(np.arange(0,1.1, 0.1))\n",
    "#                     plt.title(\"Prediction probability and change in prediction probability - %s (Bucket %s)\" %(type_list[i], bucketID))\n",
    "#                     plt.show()\n",
    "\n",
    "                    all_lime_changes.extend(changes)\n",
    "                    all_lens.extend(nr_events)\n",
    "                    all_probas.extend(probas)\n",
    "                    all_case_ids.extend(case_ids)\n",
    "\n",
    "                #Save dictionaries updated with scores\n",
    "                with open(tn_path, 'wb') as f:\n",
    "                    pickle.dump(sample_instances[0], f)\n",
    "                with open(tp_path, 'wb') as f:\n",
    "                    pickle.dump(sample_instances[1], f)\n",
    "                with open(fn_path, 'wb') as f:\n",
    "                    pickle.dump(sample_instances[2], f)\n",
    "                with open(fp_path, 'wb') as f:\n",
    "                    pickle.dump(sample_instances[3], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots()\n",
    "# ax.plot(all_lens, all_lime_changes, 'bo', label = \"LIME\")\n",
    "# ax.set_xlabel(\"Prefix Length\")\n",
    "# ax.set_ylabel(\"Change in prediction probability\")\n",
    "# #ax.legend(frameon = False, bbox_to_anchor=(1, 1), loc = 'upper left')\n",
    "# #plt.yticks(np.arange(0,1.1, 0.1))\n",
    "# plt.title(\"Prefix length and change in prediction probability - All\")\n",
    "# plt.show()\n",
    "\n",
    "# fig2, ax2 = plt.subplots()\n",
    "# ax2.plot(all_probas, all_lime_changes, 'bo', label = \"LIME\")\n",
    "# ax2.set_xlabel(\"Prediction Probability\")\n",
    "# ax2.set_ylabel(\"Change in prediction probability\")\n",
    "# #ax2.legend(frameon = False, bbox_to_anchor=(1, 1), loc = 'upper left')\n",
    "# #plt.yticks(np.arange(0,1.1, 0.1))\n",
    "# plt.title(\"Prediction probability and change in prediction probability - All\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig2, ax2 = plt.subplots()\n",
    "# ax2.plot(pos_probas, pos_lime_changes, 'bo', label = \"LIME\")\n",
    "# ax2.set_xlabel(\"Prediction Probability\")\n",
    "# ax2.set_ylabel(\"Change in prediction probability\")\n",
    "# #ax2.legend(frameon = False, bbox_to_anchor=(1, 1), loc = 'upper left')\n",
    "# plt.yticks(np.arange(0,1.1, 0.1))\n",
    "# plt.title(\"Prediction probability and change in prediction probability - Negatives\")\n",
    "# plt.show()\n",
    "\n",
    "# fig2, ax2 = plt.subplots()\n",
    "# ax2.plot(neg_probas, neg_lime_changes, 'bo', label = \"LIME\")\n",
    "# ax2.set_xlabel(\"Prediction Probability\")\n",
    "# ax2.set_ylabel(\"Change in prediction probability\")\n",
    "# #ax2.legend(frameon = False, bbox_to_anchor=(1, 1), loc = 'upper left')\n",
    "# plt.yticks(np.arange(0,1.1, 0.1))\n",
    "# plt.title(\"Prediction probability and change in prediction probability - Positives\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lime_fid_score = sum(all_lime_changes)/len(all_lime_changes)\n",
    "# shap_fid_score = sum(all_shap_changes)/len(all_shap_changes)\n",
    "\n",
    "# print(\"LIME Fidelity Score: %s \\nSHAP Fidelity Score: %s\" %(lime_fid_score, shap_fid_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lime_fid_score = sum(pos_lime_changes)/len(pos_lime_changes)\n",
    "# shap_fid_score = sum(pos_shap_changes)/len(pos_shap_changes)\n",
    "\n",
    "# print(\"Positive predictions: \\nLIME Fidelity Score: %s \\nSHAP Fidelity Score: %s\" %(lime_fid_score, shap_fid_score))\n",
    "\n",
    "# lime_fid_score = sum(neg_lime_changes)/len(neg_lime_changes)\n",
    "# shap_fid_score = sum(neg_shap_changes)/len(neg_shap_changes)\n",
    "\n",
    "# print(\"Negative predictions: \\nLIME Fidelity Score: %s \\nSHAP Fidelity Score: %s\" %(lime_fid_score, shap_fid_score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
