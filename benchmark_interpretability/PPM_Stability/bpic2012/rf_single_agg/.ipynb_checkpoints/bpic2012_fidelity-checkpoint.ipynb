{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "import sys\n",
    "#PATH = '/content/drive/My Drive/PPM_Stability/'\n",
    "PATH = \"C:/Users/velmurug/Documents/Stability Experiments/benchmark_interpretability/PPM_Stability/\"\n",
    "#PATH = \"C:/Users/Mythreyi/Documents/GitHub/Stability-Experiments/benchmark_interpretability/PPM_Stability/\"\n",
    "#PATH = \"C:/Users/mythr/Documents/GitHub/Stability-Experiments/benchmark_interpretability/PPM_Stability/\"\n",
    "sys.path.append(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost==1.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (1.0.0)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from xgboost==1.0.0) (1.18.5)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (from xgboost==1.0.0) (1.4.1)\n"
     ]
    }
   ],
   "source": [
    "#!pip install lime\n",
    "#!pip install shap\n",
    "#!pip install pandas==0.19.2\n",
    "!pip install xgboost==1.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import EncoderFactory\n",
    "from DatasetManager import DatasetManager\n",
    "import BucketFactory\n",
    "import stability as st #Nogueira, Sechidis, Brown.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import math\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "from sys import argv\n",
    "import pickle\n",
    "from collections import defaultdict, Counter\n",
    "import random\n",
    "import joblib\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "from lime import submodular_pick;\n",
    "\n",
    "import shap\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def imp_df(column_names, importances):\n",
    "        df = pd.DataFrame({'feature': column_names,\n",
    "                       'feature_importance': importances}) \\\n",
    "           .sort_values('feature_importance', ascending = False) \\\n",
    "           .reset_index(drop = True)\n",
    "        return df\n",
    "\n",
    "# plotting a feature importance dataframe (horizontal barchart)\n",
    "def var_imp_plot(imp_df, title, num_feat):\n",
    "        imp_df.columns = ['feature', 'feature_importance']\n",
    "        b= sns.barplot(x = 'feature_importance', y = 'feature', data = imp_df.head(num_feat), orient = 'h', palette=\"Blues_r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_global_explanations(train_X,train_Y, cls, feature_combiner):\n",
    "    \n",
    "    print(\"The number of testing instances is \",len(train_Y))\n",
    "    print(\"The total number of columns is\",train_X.shape[1]);\n",
    "    print(\"The total accuracy is \",cls.score(train_X,train_Y));\n",
    "       \n",
    "    sns.set(rc={'figure.figsize':(10,10), \"font.size\":18,\"axes.titlesize\":18,\"axes.labelsize\":18})\n",
    "    sns.set\n",
    "    feat_names = feature_combiner.get_feature_names()\n",
    "    base_imp = imp_df(feat_names, cls.feature_importances_)\n",
    "    base_imp.head(15)\n",
    "    var_imp_plot(base_imp, 'Feature importance using XGBoost', 15)\n",
    "    return base_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime import submodular_pick\n",
    "def generate_lime_explanations(explainer,test_xi, cls,test_y, submod=False, test_all_data=None, max_feat = 10):\n",
    "    \n",
    "    #print(\"Actual value \", test_y)\n",
    "    exp = explainer.explain_instance(test_xi, \n",
    "                                 cls.predict_proba, num_features=max_feat, labels=[0,1])\n",
    "    \n",
    "    return exp\n",
    "        \n",
    "    if submod==True:\n",
    "        sp_obj=submodular_pick.SubmodularPick(explainer, test_all_data, cls.predict_proba, \n",
    "                                      sample_size=20, num_features=num_features,num_exps_desired=4)\n",
    "        [exp.as_pyplot_figure(label=exp.available_labels()[0]) for exp in sp_obj.sp_explanations];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_samples(shap_explainer, iterations, row, features, top = None):\n",
    "    length = len(features)\n",
    "    \n",
    "    exp = []\n",
    "    rel_exp = []\n",
    "    \n",
    "    for j in range(iterations):\n",
    "        \n",
    "        #if shap_type == \"kernel\":\n",
    "        #    shap_explainer = shap.KernelExplainer(cls.predict, trainingsample)\n",
    "        #elif shap_type == \"tree\":\n",
    "        #    shap_explainer = shap.TreeExplainer(cls)\n",
    "        #elif shap_type == \"deep\":\n",
    "        #    shap_explainer = shap.DeepExplainer(cls, background)\n",
    "        \n",
    "        #print(X_test_frame.loc[row])\n",
    "        shap_values = shap_explainer.shap_values(row)\n",
    "        #print(shap_values)\n",
    "\n",
    "        importances = []\n",
    "        \n",
    "        if type(shap_explainer) == shap.explainers.kernel.KernelExplainer:\n",
    "            for i in range(length):\n",
    "                feat = features[i]\n",
    "                shap_val = shap_values[0][i]\n",
    "                abs_val = abs(shap_values[0][i])\n",
    "                entry = (feat, shap_val, abs_val)\n",
    "                importances.append(entry)\n",
    "                \n",
    "        elif type(shap_explainer) == shap.explainers.tree.TreeExplainer:\n",
    "            for i in range(length):\n",
    "                feat = features[i]\n",
    "                shap_val = shap_values[0][i]\n",
    "                abs_val = abs(shap_values[0][i])\n",
    "                entry = (feat, shap_val, abs_val)\n",
    "                importances.append(entry)\n",
    "        \n",
    "        elif type(shap_explainer) == shap.explainers.deep.DeepExplainer:\n",
    "            for i in range(length):\n",
    "                feat = features[i]\n",
    "                shap_val = shap_values[0][0][i]\n",
    "                abs_val = abs(shap_values[0][0][i])\n",
    "                entry = (feat, shap_val, abs_val)\n",
    "                importances.append(entry)\n",
    "        \n",
    "        importances.sort(key=lambda tup: tup[2], reverse = True)\n",
    "        \n",
    "        exp.append(importances)\n",
    "\n",
    "        rel_feat = []\n",
    "\n",
    "        if top != None:\n",
    "            for i in range(top):\n",
    "                feat = importances[i]\n",
    "                if feat[2] > 0:\n",
    "                    rel_feat.append(feat)\n",
    "\n",
    "            rel_exp.append(rel_feat)\n",
    "        else:\n",
    "            rel_exp = exp\n",
    "        \n",
    "    return exp, rel_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_distributions(explainer, features, test_x, bin_min = -1, bin_max = 1, bin_width = 0.05):\n",
    "    \n",
    "    #generate shap values for entire test set\n",
    "    shap_values = explainer.shap_values(test_x, check_additivity = False)\n",
    "#    print(shap_values)\n",
    "    shap_val_feat = np.transpose(shap_values)\n",
    "#    print(shap_val_feat)\n",
    "    feats = np.transpose(test_x)\n",
    "    \n",
    "    shap_distribs = []\n",
    "    \n",
    "    #For each feature\n",
    "    for i in range(len(features)):\n",
    "        print (i+1, \"of\", len(features), \"features\")\n",
    "        shap_vals = shap_val_feat[i]\n",
    "#        print(shap_vals)\n",
    "\n",
    "        #create bins based on shap value ranges\n",
    "        bins = np.arange(bin_min, bin_max, bin_width)\n",
    "\n",
    "        feat_vals = []\n",
    "        for sbin in range(len(bins)):\n",
    "            nl = []\n",
    "            feat_vals.append(nl)\n",
    "\n",
    "        #place relevant feature values into each bin\n",
    "        for j in range(len(shap_vals)):\n",
    "            val = shap_vals[j]\n",
    "            b = 0\n",
    "            cur_bin = bins[b]\n",
    "            idx = b\n",
    "\n",
    "            while val > cur_bin and b < len(bins)-1:\n",
    "                #print(cur_bin)\n",
    "                idx = b\n",
    "                b+=1\n",
    "                #print(b)\n",
    "                cur_bin = bins[b]\n",
    "\n",
    "            #print(val, idx)\n",
    "            feat_vals[idx].append(feats[i][j])\n",
    "\n",
    "        #Remove feature values that are outliers\n",
    "        #for each in feat_vals:\n",
    "        #    zscore = stats.zscore(each)\n",
    "            #print(each)\n",
    "        #    for n in range(len(zscore)):\n",
    "        #        if zscore[n] > 3 or zscore[n] < -3:\n",
    "        #            np.delete(zscore, n)\n",
    "        #            del each[n]\n",
    "            #print(each)\n",
    "            \n",
    "        #Find min and max values for each shap value bin\n",
    "        mins = []\n",
    "        maxes = []\n",
    "        #width = []\n",
    "        #print(feat_vals)\n",
    "        #n = 0\n",
    "        for each in feat_vals:\n",
    "            if each != []:\n",
    "                mins.append(min(each))\n",
    "                maxes.append(max(each))\n",
    "         #       width.append(\"Bin \"+str(n))\n",
    "         #       n+=1\n",
    "        #plt.bar(width, maxes, bottom = mins)\n",
    "        #plt.show()\n",
    "\n",
    "        #Create dictionary with list of bins and max and min feature values for each bin\n",
    "        feat_name = features[i]\n",
    "\n",
    "        feat_dict = {'Feature Name': feat_name}\n",
    "        for each in feat_vals:\n",
    "            if each != []:\n",
    "                mins.append(min(each))\n",
    "                maxes.append(max(each))\n",
    "            else:\n",
    "                mins.append(None)\n",
    "                maxes.append(None)\n",
    "\n",
    "        feat_dict['bins'] = bins\n",
    "        feat_dict['mins'] = mins\n",
    "        feat_dict['maxes'] = maxes\n",
    "       \n",
    "        shap_distribs.append(feat_dict)\n",
    "        \n",
    "    return shap_distribs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "['bpic2012_accepted']"
      ],
      "text/plain": [
       "['bpic2012_accepted']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_ref = \"bpic2012\"\n",
    "params_dir = PATH + \"params\"\n",
    "results_dir = \"results\"\n",
    "bucket_method = \"single\"\n",
    "cls_encoding = \"agg\"\n",
    "cls_method = \"xgboost\"\n",
    "\n",
    "gap = 1\n",
    "n_iter = 1\n",
    "\n",
    "method_name = \"%s_%s\"%(bucket_method, cls_encoding)\n",
    "\n",
    "generate_samples = False\n",
    "generate_lime = True\n",
    "generate_kernel_shap = False\n",
    "generate_model_shap = True\n",
    "\n",
    "sample_size = 2\n",
    "exp_iter = 10\n",
    "#max_feat = 10\n",
    "\n",
    "dataset_ref_to_datasets = {\n",
    "    #\"bpic2011\": [\"bpic2011_f%s\"%formula for formula in range(1,5)],\n",
    "    \"bpic2015\": [\"bpic2015_%s_f2\"%(municipality) for municipality in range(5,6)],\n",
    "    \"bpic2017\" : [\"bpic2017_accepted\"],\n",
    "    \"bpic2012\" : [\"bpic2012_accepted\"]\n",
    "    #\"insurance\": [\"insurance_activity\", \"insurance_followup\"],\n",
    "    #\"sepsis_cases\": [\"sepsis_cases_1\", \"sepsis_cases_2\", \"sepsis_cases_4\"]\n",
    "}\n",
    "\n",
    "datasets = [dataset_ref] if dataset_ref not in dataset_ref_to_datasets else dataset_ref_to_datasets[dataset_ref]\n",
    "\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket 1\n",
      "Generating distributions for bucket\n",
      "Category 1 of 4. Instance 1 of 250\n",
      "Creating explanations\n",
      "Identifying relevant features\n",
      "Pertubing - Run 1\n",
      "Pertubing - Run 2\n",
      "Pertubing - Run 3\n",
      "Pertubing - Run 4\n",
      "Pertubing - Run 5\n",
      "Pertubing - Run 6\n",
      "Pertubing - Run 7\n",
      "Pertubing - Run 8\n",
      "Pertubing - Run 9\n",
      "Pertubing - Run 10\n",
      "Category 1 of 4. Instance 2 of 250\n",
      "Creating explanations\n",
      "Identifying relevant features\n",
      "Pertubing - Run 1\n",
      "Pertubing - Run 2\n",
      "Pertubing - Run 3\n",
      "Pertubing - Run 4\n",
      "Pertubing - Run 5\n",
      "Pertubing - Run 6\n",
      "Pertubing - Run 7\n",
      "Pertubing - Run 8\n",
      "Pertubing - Run 9\n",
      "Pertubing - Run 10\n",
      "Category 1 of 4. Instance 3 of 250\n",
      "Creating explanations\n",
      "Identifying relevant features\n",
      "Pertubing - Run 1\n",
      "Pertubing - Run 2\n",
      "Pertubing - Run 3\n",
      "Pertubing - Run 4\n",
      "Pertubing - Run 5\n",
      "Pertubing - Run 6\n",
      "Pertubing - Run 7\n",
      "Pertubing - Run 8\n",
      "Pertubing - Run 9\n",
      "Pertubing - Run 10\n",
      "Category 1 of 4. Instance 4 of 250\n",
      "Creating explanations\n",
      "Identifying relevant features\n",
      "Pertubing - Run 1\n",
      "Pertubing - Run 2\n",
      "Pertubing - Run 3\n",
      "Pertubing - Run 4\n",
      "Pertubing - Run 5\n",
      "Pertubing - Run 6\n",
      "Pertubing - Run 7\n",
      "Pertubing - Run 8\n",
      "Pertubing - Run 9\n",
      "Pertubing - Run 10\n",
      "Category 1 of 4. Instance 5 of 250\n",
      "Creating explanations\n",
      "Identifying relevant features\n",
      "Pertubing - Run 1\n",
      "Pertubing - Run 2\n",
      "Pertubing - Run 3\n",
      "Pertubing - Run 4\n",
      "Pertubing - Run 5\n",
      "Pertubing - Run 6\n",
      "Pertubing - Run 7\n",
      "Pertubing - Run 8\n",
      "Pertubing - Run 9\n",
      "Pertubing - Run 10\n",
      "Category 1 of 4. Instance 6 of 250\n",
      "Creating explanations\n",
      "Identifying relevant features\n",
      "Pertubing - Run 1\n",
      "Pertubing - Run 2\n",
      "Pertubing - Run 3\n",
      "Pertubing - Run 4\n",
      "Pertubing - Run 5\n",
      "Pertubing - Run 6\n",
      "Pertubing - Run 7\n",
      "Pertubing - Run 8\n",
      "Pertubing - Run 9\n",
      "Pertubing - Run 10\n",
      "Category 1 of 4. Instance 7 of 250\n",
      "Creating explanations\n",
      "Identifying relevant features\n",
      "Pertubing - Run 1\n",
      "Pertubing - Run 2\n",
      "Pertubing - Run 3\n",
      "Pertubing - Run 4\n",
      "Pertubing - Run 5\n",
      "Pertubing - Run 6\n",
      "Pertubing - Run 7\n",
      "Pertubing - Run 8\n",
      "Pertubing - Run 9\n",
      "Pertubing - Run 10\n",
      "Category 1 of 4. Instance 8 of 250\n",
      "Creating explanations\n",
      "Identifying relevant features\n",
      "Pertubing - Run 1\n",
      "Pertubing - Run 2\n",
      "Pertubing - Run 3\n",
      "Pertubing - Run 4\n",
      "Pertubing - Run 5\n",
      "Pertubing - Run 6\n",
      "Pertubing - Run 7\n",
      "Pertubing - Run 8\n",
      "Pertubing - Run 9\n",
      "Pertubing - Run 10\n",
      "Category 1 of 4. Instance 9 of 250\n",
      "Creating explanations\n",
      "Identifying relevant features\n",
      "Pertubing - Run 1\n",
      "Pertubing - Run 2\n",
      "Pertubing - Run 3\n",
      "Pertubing - Run 4\n",
      "Pertubing - Run 5\n",
      "Pertubing - Run 6\n",
      "Pertubing - Run 7\n",
      "Pertubing - Run 8\n",
      "Pertubing - Run 9\n",
      "Pertubing - Run 10\n",
      "Category 1 of 4. Instance 10 of 250\n",
      "Creating explanations\n",
      "Identifying relevant features\n",
      "Pertubing - Run 1\n",
      "Pertubing - Run 2\n",
      "Pertubing - Run 3\n",
      "Pertubing - Run 4\n",
      "Pertubing - Run 5\n",
      "Pertubing - Run 6\n",
      "Pertubing - Run 7\n",
      "Pertubing - Run 8\n",
      "Pertubing - Run 9\n",
      "Pertubing - Run 10\n",
      "Category 1 of 4. Instance 11 of 250\n",
      "Creating explanations\n",
      "Identifying relevant features\n",
      "Pertubing - Run 1\n",
      "Pertubing - Run 2\n",
      "Pertubing - Run 3\n",
      "Pertubing - Run 4\n",
      "Pertubing - Run 5\n",
      "Pertubing - Run 6\n",
      "Pertubing - Run 7\n",
      "Pertubing - Run 8\n",
      "Pertubing - Run 9\n",
      "Pertubing - Run 10\n",
      "Category 1 of 4. Instance 12 of 250\n",
      "Creating explanations\n",
      "Identifying relevant features\n",
      "Pertubing - Run 1\n",
      "Pertubing - Run 2\n",
      "Pertubing - Run 3\n",
      "Pertubing - Run 4\n",
      "Pertubing - Run 5\n",
      "Pertubing - Run 6\n",
      "Pertubing - Run 7\n",
      "Pertubing - Run 8\n",
      "Pertubing - Run 9\n",
      "Pertubing - Run 10\n",
      "Category 1 of 4. Instance 13 of 250\n",
      "Creating explanations\n",
      "Identifying relevant features\n",
      "Pertubing - Run 1\n",
      "Pertubing - Run 2\n",
      "Pertubing - Run 3\n",
      "Pertubing - Run 4\n",
      "Pertubing - Run 5\n",
      "Pertubing - Run 6\n",
      "Pertubing - Run 7\n",
      "Pertubing - Run 8\n",
      "Pertubing - Run 9\n",
      "Pertubing - Run 10\n",
      "Category 1 of 4. Instance 14 of 250\n",
      "Creating explanations\n",
      "Identifying relevant features\n",
      "Pertubing - Run 1\n",
      "Pertubing - Run 2\n",
      "Pertubing - Run 3\n",
      "Pertubing - Run 4\n",
      "Pertubing - Run 5\n",
      "Pertubing - Run 6\n",
      "Pertubing - Run 7\n",
      "Pertubing - Run 8\n",
      "Pertubing - Run 9\n",
      "Pertubing - Run 10\n",
      "Category 1 of 4. Instance 15 of 250\n",
      "Creating explanations\n",
      "Identifying relevant features\n",
      "Pertubing - Run 1\n",
      "Pertubing - Run 2\n",
      "Pertubing - Run 3\n",
      "Pertubing - Run 4\n",
      "Pertubing - Run 5\n",
      "Pertubing - Run 6\n",
      "Pertubing - Run 7\n",
      "Pertubing - Run 8\n",
      "Pertubing - Run 9\n",
      "Pertubing - Run 10\n",
      "Category 1 of 4. Instance 16 of 250\n",
      "Creating explanations\n",
      "Identifying relevant features\n",
      "Pertubing - Run 1\n",
      "Pertubing - Run 2\n",
      "Pertubing - Run 3\n",
      "Pertubing - Run 4\n",
      "Pertubing - Run 5\n",
      "Pertubing - Run 6\n",
      "Pertubing - Run 7\n",
      "Pertubing - Run 8\n",
      "Pertubing - Run 9\n",
      "Pertubing - Run 10\n",
      "Category 1 of 4. Instance 17 of 250\n",
      "Creating explanations\n",
      "Identifying relevant features\n",
      "Pertubing - Run 1\n",
      "Pertubing - Run 2\n",
      "Pertubing - Run 3\n",
      "Pertubing - Run 4\n",
      "Pertubing - Run 5\n",
      "Pertubing - Run 6\n",
      "Pertubing - Run 7\n",
      "Pertubing - Run 8\n",
      "Pertubing - Run 9\n",
      "Pertubing - Run 10\n",
      "Category 1 of 4. Instance 18 of 250\n",
      "Creating explanations\n",
      "Identifying relevant features\n",
      "Pertubing - Run 1\n",
      "Pertubing - Run 2\n",
      "Pertubing - Run 3\n",
      "Pertubing - Run 4\n",
      "Pertubing - Run 5\n",
      "Pertubing - Run 6\n",
      "Pertubing - Run 7\n",
      "Pertubing - Run 8\n",
      "Pertubing - Run 9\n",
      "Pertubing - Run 10\n",
      "Category 1 of 4. Instance 19 of 250\n",
      "Creating explanations\n",
      "Identifying relevant features\n",
      "Pertubing - Run 1\n",
      "Pertubing - Run 2\n",
      "Pertubing - Run 3\n",
      "Pertubing - Run 4\n",
      "Pertubing - Run 5\n",
      "Pertubing - Run 6\n",
      "Pertubing - Run 7\n",
      "Pertubing - Run 8\n",
      "Pertubing - Run 9\n",
      "Pertubing - Run 10\n",
      "Category 1 of 4. Instance 20 of 250\n",
      "Creating explanations\n",
      "Identifying relevant features\n",
      "Pertubing - Run 1\n",
      "Pertubing - Run 2\n",
      "Pertubing - Run 3\n",
      "Pertubing - Run 4\n",
      "Pertubing - Run 5\n",
      "Pertubing - Run 6\n",
      "Pertubing - Run 7\n",
      "Pertubing - Run 8\n",
      "Pertubing - Run 9\n",
      "Pertubing - Run 10\n",
      "Category 1 of 4. Instance 21 of 250\n",
      "Creating explanations\n",
      "Identifying relevant features\n",
      "Pertubing - Run 1\n",
      "Pertubing - Run 2\n",
      "Pertubing - Run 3\n",
      "Pertubing - Run 4\n",
      "Pertubing - Run 5\n",
      "Pertubing - Run 6\n",
      "Pertubing - Run 7\n",
      "Pertubing - Run 8\n",
      "Pertubing - Run 9\n",
      "Pertubing - Run 10\n",
      "Category 1 of 4. Instance 22 of 250\n",
      "Creating explanations\n"
     ]
    }
   ],
   "source": [
    "if generate_model_shap:\n",
    "    for dataset_name in datasets:\n",
    "\n",
    "        dataset_manager = DatasetManager(dataset_name)\n",
    "\n",
    "        for ii in range(n_iter):\n",
    "            num_buckets = range(len([name for name in os.listdir(os.path.join(PATH,'%s/%s_%s/models'% (dataset_ref, cls_method, method_name)))]))\n",
    "\n",
    "            all_shap_changes = []\n",
    "            all_lens = []\n",
    "            all_probas = []\n",
    "            all_case_ids = []\n",
    "\n",
    "            pos_shap_changes = []\n",
    "            pos_probas = []\n",
    "            pos_nr_events = []\n",
    "            pos_case_ids = []\n",
    "\n",
    "            neg_shap_changes = []\n",
    "            neg_probas = []\n",
    "            neg_nr_events = []\n",
    "            neg_case_ids = []\n",
    "\n",
    "            for bucket in list(num_buckets):\n",
    "                bucketID = bucket+1\n",
    "                print ('Bucket', bucketID)\n",
    "\n",
    "                #import everything needed to sort and predict\n",
    "                feat_comb_path = os.path.join(PATH, \"%s/%s_%s/bucketers_and_encoders/feature_combiner_bucket_%s.joblib\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "                cls_path = os.path.join(PATH, \"%s/%s_%s/models/cls_bucket_%s.joblib\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "                cls = joblib.load(cls_path)\n",
    "                feature_combiner = joblib.load(feat_comb_path)\n",
    "\n",
    "                #import data for bucket\n",
    "                X_test_path = os.path.join(PATH, \"%s/%s_%s/test_data/bucket_%s_prefixes.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "                Y_test_path = os.path.join(PATH, \"%s/%s_%s/test_data/bucket_%s_labels.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "\n",
    "                with open (X_test_path, 'rb') as f:\n",
    "                    dt_test_bucket = pickle.load(f)\n",
    "                with open (Y_test_path, 'rb') as f:\n",
    "                    test_y = pickle.load(f)\n",
    "                #with open (X_test_path, 'rb') as f:\n",
    "                #    dt_test_bucket = pickle.load(f)\n",
    "                #with open (Y_test_path, 'rb') as f:\n",
    "                #    test_y = pickle.load(f)\n",
    "\n",
    "                #import previously identified samples\n",
    "                tn_path = os.path.join(PATH, \"%s/%s_%s/samples/true_neg_bucket_%s_.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "                tp_path = os.path.join(PATH, \"%s/%s_%s/samples/true_pos_bucket_%s_.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "                fn_path = os.path.join(PATH, \"%s/%s_%s/samples/false_neg_bucket_%s_.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "                fp_path = os.path.join(PATH, \"%s/%s_%s/samples/false_pos_bucket_%s_.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "\n",
    "                sample_instances = []\n",
    "\n",
    "                with open (tn_path, 'rb') as f:\n",
    "                    tn_list = pickle.load(f)\n",
    "                with open (tp_path, 'rb') as f:\n",
    "                    tp_list = pickle.load(f)\n",
    "                with open (fn_path, 'rb') as f:\n",
    "                    fn_list = pickle.load(f)\n",
    "                with open (fp_path, 'rb') as f:\n",
    "                    fp_list = pickle.load(f)\n",
    "\n",
    "                #save results to a list\n",
    "                sample_instances.append(tn_list)\n",
    "                sample_instances.append(tp_list)\n",
    "                sample_instances.append(fn_list)\n",
    "                sample_instances.append(fp_list)\n",
    "\n",
    "                tree_explainer = shap.TreeExplainer(cls)\n",
    "                test_x = feature_combiner.fit_transform(dt_test_bucket)\n",
    "                feat_list = feature_combiner.get_feature_names()\n",
    "                type_list = ['True Negatives', 'True Positives', 'False Negatives', 'False Positives']\n",
    "                max_feat = round(len(feat_list)*0.1)\n",
    "                #print(max_feat)\n",
    "                \n",
    "                print(\"Generating distributions for bucket\")\n",
    "                #distribs = generate_distributions(tree_explainer, feat_list, test_x)\n",
    "\n",
    "                for i_type in range(len(sample_instances)):\n",
    "                    changes = []\n",
    "                    probas = []\n",
    "                    nr_events = []\n",
    "                    case_ids = []\n",
    "\n",
    "                    for n in range(len(sample_instances[i_type])):\n",
    "                        print(\"Category %s of %s. Instance %s of %s\" %(i_type+1, len(sample_instances), n+1, len(sample_instances[i_type])))\n",
    "                        instance = sample_instances[i_type][n]\n",
    "\n",
    "                        ind = instance['predicted']\n",
    "                        case_ids.append(instance['caseID'])\n",
    "                        p1 = instance['proba']\n",
    "                        probas.append(p1)\n",
    "                        nr_events.append(instance['nr_events'])\n",
    "                        input_ = instance['input']\n",
    "\n",
    "                        test_x_group = feature_combiner.fit_transform(input_) \n",
    "                        #test_x=np.transpose(test_x_group[0])\n",
    "                        #print(test_x)\n",
    "                        #print(p1)\n",
    "\n",
    "                        print(\"Creating explanations\")\n",
    "                        exp, rel_exp = create_samples(tree_explainer, exp_iter, test_x_group, feat_list, top = max_feat)\n",
    "\n",
    "                        features = []\n",
    "                        shap_vals = []\n",
    "                        \n",
    "                        print(\"Identifying relevant features\")\n",
    "                        for explanation in rel_exp:\n",
    "                            features.extend([feat[0] for feat in explanation])\n",
    "                            shap_vals.extend([feat for feat in explanation])\n",
    "\n",
    "                        counter = Counter(features).most_common(max_feat)\n",
    "\n",
    "                        rel_feats = [feat[0] for feat in counter]\n",
    "\n",
    "                        #rel_feats = []\n",
    "                        #for feat in feats:\n",
    "                        #    vals = [i[1] for i in shap_vals if i[0] == feat]\n",
    "                            #print(feat, vals)\n",
    "                        #    val = np.mean(vals)\n",
    "                        #    rel_feats.append((feat, val))\n",
    "\n",
    "                        intervals = []\n",
    "                        for feat in rel_feats:#for item in rel_feats:\n",
    "                        #    feat = item [0]\n",
    "                        #    val = item[1]\n",
    "\n",
    "                        #    print(\"Creating distribution for feature\", rel_feats.index(item)+1, \"of\", len(rel_feats))\n",
    "\n",
    "                           # n = feat_list.index(feat)\n",
    "                        #    feat_dict = distribs[n]\n",
    "\n",
    "                        #    if feat_dict['Feature Name'] != feat:\n",
    "                        #        for each in distribs:\n",
    "                        #            if feat_dict['Feature Name'] == feat:\n",
    "                        #                feat_dict = each\n",
    "\n",
    "                         #   bins = feat_dict['bins']\n",
    "                         #   mins = feat_dict['mins']\n",
    "                         #   maxes = feat_dict['maxes']\n",
    "                            #print (feat, val, bins, mins, maxes)\n",
    "\n",
    "                          #  i = 0\n",
    "                          #  while val > bins[i] and i < len(bins)-1:\n",
    "                          #      idx = i\n",
    "                          #      i+=1\n",
    "                            #print (i)\n",
    "                         #   if mins[i] != None:\n",
    "                         #       min_val = mins[i]\n",
    "                         #       max_val = maxes[i]\n",
    "                         #   else:\n",
    "                         #       j = i\n",
    "                         #       while mins[j] == None and j > 0:\n",
    "                         #           min_val = mins[j-1]\n",
    "                         #           max_val = maxes[j-1]\n",
    "                         #           j = j-1\n",
    "\n",
    "                         #   interval = max_val - min_val\n",
    "\n",
    "                            index = feat_list.index(feat)\n",
    "                          #  int_min = max_val\n",
    "                          #  int_max = max_val + interval\n",
    "                            intervals.append((feat, index))#, int_min, int_max))\n",
    "\n",
    "\n",
    "                        diffs = []\n",
    "\n",
    "                        for iteration in range(exp_iter):\n",
    "                            print(\"Pertubing - Run\", iteration+1)\n",
    "                            alt_x = np.copy(test_x_group)\n",
    "                            #print(\"original:\", alt_x)\n",
    "                            for each in intervals:\n",
    "                                new_val = np.nan\n",
    "                                alt_x[0][each[1]] = new_val\n",
    "                            p2 = cls.predict_proba(alt_x)[0][0]\n",
    "                            diff = p1-p2\n",
    "                            diffs.append(diff)\n",
    "\n",
    "                        changes.append(np.mean(diffs))\n",
    "\n",
    "                        instance['shap_fid_change'] = diffs\n",
    "                        #print(\"RMSE for instance:\", np.std(diffs))\n",
    "\n",
    "\n",
    "                        if ind == 0:\n",
    "                            pos_shap_changes.append(abs(diff))#np.std(diffs))\n",
    "                            pos_probas.append(p1)\n",
    "                            pos_nr_events.append(instance['nr_events'])\n",
    "                            pos_case_ids.append(instance['caseID'])\n",
    "                        else:\n",
    "                            neg_shap_changes.append(abs(diff))#np.std(diffs))\n",
    "                            neg_probas.append(p1)\n",
    "                            neg_nr_events.append(instance['nr_events'])\n",
    "                            neg_case_ids.append(instance['caseID'])\n",
    "\n",
    "                    fig, ax = plt.subplots()\n",
    "                    ax.plot(probas, changes, 'ro', label = \"SHAP\")\n",
    "                    ax.set_xlabel(\"Prefix Length\")\n",
    "                    ax.set_ylabel(\"Change in prediction probability\")\n",
    "                    #ax.legend(frameon = False, bbox_to_anchor=(1, 1), loc = 'upper left')\n",
    "                    plt.yticks(np.arange(0,1.1, 0.1))\n",
    "                    plt.title(\"Prefix length and change in prediction probability - %s (Bucket %s)\" %(type_list[i_type], bucketID))\n",
    "                    plt.show()\n",
    "\n",
    "                    fig2, ax2 = plt.subplots()\n",
    "                    ax2.plot(nr_events, changes, 'ro', label = \"SHAP\")\n",
    "                    ax2.set_xlabel(\"Prediction Probability\")\n",
    "                    ax2.set_ylabel(\"Change in prediction probability\")\n",
    "                    #ax2.legend(frameon = False, bbox_to_anchor=(1, 1), loc = 'upper left')\n",
    "                    plt.yticks(np.arange(0,1.1, 0.1))\n",
    "                    plt.title(\"Prediction probability and change in prediction probability - %s (Bucket %s)\" %(type_list[i_type], bucketID))\n",
    "                    plt.show()\n",
    "\n",
    "                    all_shap_changes.extend(changes)\n",
    "                    all_lens.extend(nr_events)\n",
    "                    all_probas.extend(probas)\n",
    "                    all_case_ids.extend(case_ids)\n",
    "\n",
    "                #Save dictionaries updated with scores\n",
    "                with open(tn_path, 'wb') as f:\n",
    "                    pickle.dump(sample_instances[0], f)\n",
    "                with open(tp_path, 'wb') as f:\n",
    "                    pickle.dump(sample_instances[1], f)\n",
    "                with open(fn_path, 'wb') as f:\n",
    "                    pickle.dump(sample_instances[2], f)\n",
    "                with open(fp_path, 'wb') as f:\n",
    "                    pickle.dump(sample_instances[3], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(all_lens, all_shap_changes, 'ro', label = \"SHAP\")\n",
    "ax.set_xlabel(\"Prefix Length\")\n",
    "ax.set_ylabel(\"Change in prediction probability\")\n",
    "#ax.legend(frameon = False, bbox_to_anchor=(1, 1), loc = 'upper left')\n",
    "plt.yticks(np.arange(0,1.1, 0.1))\n",
    "plt.title(\"Prefix length and change in prediction probability - All\")\n",
    "plt.show()\n",
    "\n",
    "fig2, ax2 = plt.subplots()\n",
    "ax2.plot(all_probas, all_shap_changes, 'ro', label = \"SHAP\")\n",
    "ax2.set_xlabel(\"Prediction Probability\")\n",
    "ax2.set_ylabel(\"Change in prediction probability\")\n",
    "#ax2.legend(frameon = False, bbox_to_anchor=(1, 1), loc = 'upper left')\n",
    "plt.yticks(np.arange(0,1.1, 0.1))\n",
    "plt.title(\"Prediction probability and change in prediction probability - All\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2, ax2 = plt.subplots()\n",
    "ax2.plot(pos_probas, pos_shap_changes, 'ro', label = \"SHAP\")\n",
    "ax2.set_xlabel(\"Prediction Probability\")\n",
    "ax2.set_ylabel(\"Change in prediction probability\")\n",
    "#ax2.legend(frameon = False, bbox_to_anchor=(1, 1), loc = 'upper left')\n",
    "plt.yticks(np.arange(0,1.1, 0.1))\n",
    "plt.title(\"Prediction probability and change in prediction probability - Positives\")\n",
    "plt.show()\n",
    "\n",
    "fig2, ax2 = plt.subplots()\n",
    "ax2.plot(neg_probas, neg_shap_changes, 'ro', label = \"SHAP\")\n",
    "ax2.set_xlabel(\"Prediction Probability\")\n",
    "ax2.set_ylabel(\"Change in prediction probability\")\n",
    "#ax2.legend(frameon = False, bbox_to_anchor=(1, 1), loc = 'upper left')\n",
    "plt.yticks(np.arange(0,1.1, 0.1))\n",
    "plt.title(\"Prediction probability and change in prediction probability - Negatives\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if generate_lime:\n",
    "    for dataset_name in datasets:\n",
    "\n",
    "        dataset_manager = DatasetManager(dataset_name)\n",
    "\n",
    "        for ii in range(n_iter):\n",
    "            num_buckets = range(len([name for name in os.listdir(os.path.join(PATH,'%s/%s_%s/models'% (dataset_ref, cls_method, method_name)))]))\n",
    "\n",
    "            all_lime_changes = []\n",
    "            all_lens = []\n",
    "            all_probas = []\n",
    "            all_case_ids = []\n",
    "\n",
    "            pos_lime_changes = []\n",
    "            pos_probas = []\n",
    "            pos_nr_events = []\n",
    "            pos_case_ids = []\n",
    "\n",
    "            neg_lime_changes = []\n",
    "            neg_probas = []\n",
    "            neg_nr_events = []\n",
    "            neg_case_ids = []\n",
    "\n",
    "            for bucket in list(num_buckets):\n",
    "                bucketID = bucket+1\n",
    "                print ('Bucket', bucketID)\n",
    "\n",
    "                #import everything needed to sort and predict\n",
    "                feat_comb_path = os.path.join(PATH, \"%s/%s_%s/bucketers_and_encoders/feature_combiner_bucket_%s.joblib\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "                cls_path = os.path.join(PATH, \"%s/%s_%s/models/cls_bucket_%s.joblib\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "                cls = joblib.load(cls_path)\n",
    "                feature_combiner = joblib.load(feat_comb_path)\n",
    "\n",
    "                #import data for bucket\n",
    "                X_train_path = os.path.join(PATH, \"%s/%s_%s/train_data/bucket_%s_prefixes.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "                Y_train_path = os.path.join(PATH, \"%s/%s_%s/train_data/bucket_%s_labels.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "\n",
    "                with open (X_train_path, 'rb') as f:\n",
    "                    dt_train_bucket = pickle.load(f)\n",
    "                with open (Y_train_path, 'rb') as f:\n",
    "                    train_y = pickle.load(f)\n",
    "                #with open (X_test_path, 'rb') as f:\n",
    "                #    dt_test_bucket = pickle.load(f)\n",
    "                #with open (Y_test_path, 'rb') as f:\n",
    "                #    test_y = pickle.load(f)\n",
    "\n",
    "                #import previously identified samples\n",
    "                tn_path = os.path.join(PATH, \"%s/%s_%s/samples/true_neg_bucket_%s_.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "                tp_path = os.path.join(PATH, \"%s/%s_%s/samples/true_pos_bucket_%s_.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "                fn_path = os.path.join(PATH, \"%s/%s_%s/samples/false_neg_bucket_%s_.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "                fp_path = os.path.join(PATH, \"%s/%s_%s/samples/false_pos_bucket_%s_.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "\n",
    "                sample_instances = []\n",
    "\n",
    "                with open (tn_path, 'rb') as f:\n",
    "                    tn_list = pickle.load(f)\n",
    "                with open (tp_path, 'rb') as f:\n",
    "                    tp_list = pickle.load(f)\n",
    "                with open (fn_path, 'rb') as f:\n",
    "                    fn_list = pickle.load(f)\n",
    "                with open (fp_path, 'rb') as f:\n",
    "                    fp_list = pickle.load(f)\n",
    "\n",
    "                #save results to a list\n",
    "                sample_instances.append(tn_list)\n",
    "                sample_instances.append(tp_list)\n",
    "                sample_instances.append(fn_list)\n",
    "                sample_instances.append(fp_list)\n",
    "                \n",
    "                #get the training data as a matrix\n",
    "                trainingdata = feature_combiner.fit_transform(dt_train_bucket);\n",
    "                #importance = generate_global_explanations(trainingdata,train_y, cls, feature_combiner)\n",
    "\n",
    "                feat_list = feature_combiner.get_feature_names()\n",
    "                max_feat = round(len(feat_list)*0.1)\n",
    "                class_names=['regular','deviant']# regular is 0, deviant is 1, 0 is left, 1 is right\n",
    "                lime_explainer = lime.lime_tabular.LimeTabularExplainer(trainingdata, feature_names = feat_list, \n",
    "                                                                        class_names=class_names, discretize_continuous=True)\n",
    "                type_list = ['True Negatives', 'True Positives', 'False Negatives', 'False Positives']\n",
    "\n",
    "                for i in list(range(len(sample_instances))):\n",
    "                    changes = []\n",
    "                    probas = []\n",
    "                    nr_events = []\n",
    "                    case_ids = []\n",
    "\n",
    "                    for j in list(range(len(sample_instances[i]))):\n",
    "                        print(\"Category %s of %s. Instance %s of %s\" %(i+1, len(sample_instances), j+1, len(sample_instances[i])))\n",
    "                        instance = sample_instances[i][j]\n",
    "\n",
    "                        ind = instance['predicted']\n",
    "                        case_ids.append(instance['caseID'])\n",
    "                        p1 = instance['proba']\n",
    "                        probas.append(p1)\n",
    "                        #print(\"proba:\", p1)\n",
    "                        nr_events.append(instance['nr_events'])\n",
    "                        input_ = instance['input']\n",
    "\n",
    "                        test_x_group = feature_combiner.fit_transform(input_) \n",
    "                        test_x=np.transpose(test_x_group[0])\n",
    "                        #print(test_x)\n",
    "                        #print(p1)\n",
    "\n",
    "                        explanations = []\n",
    "                        for iteration in range(exp_iter):\n",
    "                            lime_exp = generate_lime_explanations(lime_explainer, test_x, cls, input_, max_feat = max_feat)\n",
    "                            explanation = lime_exp.as_list()\n",
    "                            explanations.extend(explanation)\n",
    "\n",
    "                        features = []\n",
    "                        for explanation in explanations:\n",
    "                            features.append(explanation[0])\n",
    "\n",
    "                        counter = Counter(features)\n",
    "                        check_dup = []\n",
    "                        for feat in feat_list:\n",
    "                            for feature in counter:\n",
    "                                if feat in feature:\n",
    "                                    check_dup.append(feat)\n",
    "\n",
    "                        dup_counter = Counter(check_dup)\n",
    "                        duplicated = [feat for feat in dup_counter if dup_counter[feat] > 1]\n",
    "\n",
    "                        for each in duplicated:\n",
    "                            dpls = []\n",
    "                            vals = []\n",
    "                            for feat in counter.keys():\n",
    "                                if each in feat:\n",
    "                                    dpls.append(feat)\n",
    "                                    vals.append(counter[feat])\n",
    "                            keepval = vals.index(max(vals))\n",
    "                            for n in range(len(dpls)):\n",
    "                                if n != keepval:\n",
    "                                    del counter[dpls[n]]\n",
    "\n",
    "                        rel_feat = counter.most_common(max_feat)\n",
    "\n",
    "                        intervals = []\n",
    "\n",
    "                        for item in rel_feat:\n",
    "                            print(\"Creating distribution for feature\", rel_feat.index(item))\n",
    "                            feat = item[0]\n",
    "                            #print(item)\n",
    "                            #print(feat)\n",
    "                            for n in range(len(feat_list)):\n",
    "                                if feat_list[n] in feat:\n",
    "                                    #if (\"<\" or \"<=\") in feat and (\">\" or \">=\") in feat:\n",
    "                                    #    two_sided = True\n",
    "                                    #    parts = feat.split(' ')\n",
    "                                    #    l_bound = float(parts[0])\n",
    "                                    #    u_bound = float(parts[-1])\n",
    "                                    #    interval = u_bound - l_bound\n",
    "                                    #    new_min = u_bound\n",
    "                                    #    new_max = u_bound + interval\n",
    "                                    #else:\n",
    "                                    #    two_sided = False\n",
    "                                    #    parts = feat.split(' ')\n",
    "                                    #    if parts[-2] == \"<=\" or parts[-2] == \"<\":\n",
    "                                    #        u_bound = float(parts[-1])\n",
    "                                    #        if u_bound != 0:\n",
    "                                    #            interval = math.ceil(u_bound*1.1)\n",
    "                                    #        else:\n",
    "                                    #            interval = 5\n",
    "                                    #        new_min = u_bound\n",
    "                                    #        new_max = u_bound + interval\n",
    "                                    #    elif parts[-2] == \">=\" or parts[-2] == \">\":\n",
    "                                    #        l_bound = float(parts[-1])\n",
    "                                    #        if l_bound != 0:\n",
    "                                    #            interval = math.ceil(l_bound*1.1)\n",
    "                                    #        else:\n",
    "                                    #            interval = 5\n",
    "                                    #        new_max = l_bound\n",
    "                                    #        new_min = l_bound - interval\n",
    "                                    #    else:\n",
    "                                    #        bound = float(parts[-1])\n",
    "                                    #        interval = math.ceil((bound*1.1)/2)\n",
    "                                    #        new_min = bound\n",
    "                                    #        new_max = bound+interval\n",
    "                                    feature_name = feat_list[n]\n",
    "                                    index = n\n",
    "                                    #int_min = new_min\n",
    "                                    #int_max = new_max\n",
    "                                    intervals.append((feature_name, index))#, int_min, int_max))\n",
    "\n",
    "                        diffs = []\n",
    "                        for iteration in range(exp_iter):\n",
    "                            print(\"Pertubing - Run\", iteration+1)\n",
    "                            alt_x = np.copy(test_x_group)\n",
    "                            #print(\"original:\", alt_x)\n",
    "                            for each in intervals:\n",
    "                                new_val = np.nan\n",
    "                                alt_x[0][each[1]] = new_val\n",
    "                            p2 = cls.predict_proba(alt_x)[0][0]\n",
    "                            diff = p1-p2\n",
    "                            diffs.append(diff)\n",
    "\n",
    "                        changes.append(np.mean(diffs))\n",
    "                        \n",
    "                        instance['lime_fid_change'] = diffs\n",
    "                        #print(\"RMSE for instance:\", np.std(diffs))\n",
    "\n",
    "\n",
    "                        if ind == 0:\n",
    "                            pos_lime_changes.append(abs(diff))#np.std(diffs))\n",
    "                            pos_probas.append(p1)\n",
    "                            pos_nr_events.append(instance['nr_events'])\n",
    "                            pos_case_ids.append(instance['caseID'])\n",
    "                        else:\n",
    "                            neg_lime_changes.append(abs(diff))#np.std(diffs))\n",
    "                            neg_probas.append(p1)\n",
    "                            neg_nr_events.append(instance['nr_events'])\n",
    "                            neg_case_ids.append(instance['caseID'])\n",
    "\n",
    "                    fig, ax = plt.subplots()\n",
    "                    ax.plot(nr_events, changes, 'bo', label = \"LIME\")\n",
    "                    ax.set_xlabel(\"Prefix Length\")\n",
    "                    ax.set_ylabel(\"Change in prediction probability\")\n",
    "                    #ax.legend(frameon = False, bbox_to_anchor=(1, 1), loc = 'upper left')\n",
    "                    #plt.yticks(np.arange(0,1.1, 0.1))\n",
    "                    plt.title(\"Prefix length and change in prediction probability - %s (Bucket %s)\" %(type_list[i], bucketID))\n",
    "                    plt.show()\n",
    "\n",
    "                    fig2, ax2 = plt.subplots()\n",
    "                    ax2.plot(probas, changes, 'bo', label = \"LIME\")\n",
    "                    ax2.set_xlabel(\"Prediction Probability\")\n",
    "                    ax2.set_ylabel(\"Change in prediction probability\")\n",
    "                    #ax2.legend(frameon = False, bbox_to_anchor=(1, 1), loc = 'upper left')\n",
    "                    plt.yticks(np.arange(0,1.1, 0.1))\n",
    "                    plt.title(\"Prediction probability and change in prediction probability - %s (Bucket %s)\" %(type_list[i], bucketID))\n",
    "                    plt.show()\n",
    "\n",
    "                    all_lime_changes.extend(changes)\n",
    "                    all_lens.extend(nr_events)\n",
    "                    all_probas.extend(probas)\n",
    "                    all_case_ids.extend(case_ids)\n",
    "\n",
    "                #Save dictionaries updated with scores\n",
    "                with open(tn_path, 'wb') as f:\n",
    "                    pickle.dump(sample_instances[0], f)\n",
    "                with open(tp_path, 'wb') as f:\n",
    "                    pickle.dump(sample_instances[1], f)\n",
    "                with open(fn_path, 'wb') as f:\n",
    "                    pickle.dump(sample_instances[2], f)\n",
    "                with open(fp_path, 'wb') as f:\n",
    "                    pickle.dump(sample_instances[3], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(all_lens, all_lime_changes, 'bo', label = \"LIME\")\n",
    "ax.set_xlabel(\"Prefix Length\")\n",
    "ax.set_ylabel(\"Change in prediction probability\")\n",
    "#ax.legend(frameon = False, bbox_to_anchor=(1, 1), loc = 'upper left')\n",
    "#plt.yticks(np.arange(0,1.1, 0.1))\n",
    "plt.title(\"Prefix length and change in prediction probability - All\")\n",
    "plt.show()\n",
    "\n",
    "fig2, ax2 = plt.subplots()\n",
    "ax2.plot(all_probas, all_lime_changes, 'bo', label = \"LIME\")\n",
    "ax2.set_xlabel(\"Prediction Probability\")\n",
    "ax2.set_ylabel(\"Change in prediction probability\")\n",
    "#ax2.legend(frameon = False, bbox_to_anchor=(1, 1), loc = 'upper left')\n",
    "#plt.yticks(np.arange(0,1.1, 0.1))\n",
    "plt.title(\"Prediction probability and change in prediction probability - All\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2, ax2 = plt.subplots()\n",
    "ax2.plot(pos_probas, pos_lime_changes, 'bo', label = \"LIME\")\n",
    "ax2.set_xlabel(\"Prediction Probability\")\n",
    "ax2.set_ylabel(\"Change in prediction probability\")\n",
    "#ax2.legend(frameon = False, bbox_to_anchor=(1, 1), loc = 'upper left')\n",
    "plt.yticks(np.arange(0,1.1, 0.1))\n",
    "plt.title(\"Prediction probability and change in prediction probability - Negatives\")\n",
    "plt.show()\n",
    "\n",
    "fig2, ax2 = plt.subplots()\n",
    "ax2.plot(neg_probas, neg_lime_changes, 'bo', label = \"LIME\")\n",
    "ax2.set_xlabel(\"Prediction Probability\")\n",
    "ax2.set_ylabel(\"Change in prediction probability\")\n",
    "#ax2.legend(frameon = False, bbox_to_anchor=(1, 1), loc = 'upper left')\n",
    "plt.yticks(np.arange(0,1.1, 0.1))\n",
    "plt.title(\"Prediction probability and change in prediction probability - Positives\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lime_fid_score = sum(all_lime_changes)/len(all_lime_changes)\n",
    "shap_fid_score = sum(all_shap_changes)/len(all_shap_changes)\n",
    "\n",
    "print(\"LIME Fidelity Score: %s \\nSHAP Fidelity Score: %s\" %(lime_fid_score, shap_fid_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lime_fid_score = sum(pos_lime_changes)/len(pos_lime_changes)\n",
    "shap_fid_score = sum(pos_shap_changes)/len(pos_shap_changes)\n",
    "\n",
    "print(\"Positive predictions: \\nLIME Fidelity Score: %s \\nSHAP Fidelity Score: %s\" %(lime_fid_score, shap_fid_score))\n",
    "\n",
    "lime_fid_score = sum(neg_lime_changes)/len(neg_lime_changes)\n",
    "shap_fid_score = sum(neg_shap_changes)/len(neg_shap_changes)\n",
    "\n",
    "print(\"Negative predictions: \\nLIME Fidelity Score: %s \\nSHAP Fidelity Score: %s\" %(lime_fid_score, shap_fid_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
