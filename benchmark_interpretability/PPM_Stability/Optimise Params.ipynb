{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bpic2012_accepted']\n",
      "splitting data\n",
      "single                                                                                                                 \n",
      "  0%|                                                                            | 0/3 [00:00<?, ?trial/s, best loss=?]"
     ]
    }
   ],
   "source": [
    "import EncoderFactory\n",
    "from DatasetManager import DatasetManager\n",
    "import BucketFactory\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "from sys import argv\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from hyperopt import Trials, STATUS_OK, tpe, fmin, hp\n",
    "import hyperopt\n",
    "from hyperopt.pyll.base import scope\n",
    "from hyperopt.pyll.stochastic import sample\n",
    "\n",
    "\n",
    "def create_and_evaluate_model(args):\n",
    "    global trial_nr\n",
    "    trial_nr += 1\n",
    "    \n",
    "    start = time.time()\n",
    "    score = 0\n",
    "    for cv_iter in range(n_splits):\n",
    "        \n",
    "        dt_test_prefixes = dt_prefixes[cv_iter]\n",
    "        dt_train_prefixes = pd.DataFrame()\n",
    "        for cv_train_iter in range(n_splits): \n",
    "            if cv_train_iter != cv_iter:\n",
    "                dt_train_prefixes = pd.concat([dt_train_prefixes, dt_prefixes[cv_train_iter]], axis=0, sort=False)\n",
    "        \n",
    "        # Bucketing prefixes based on control flow\n",
    "        bucketer_args = {'encoding_method':bucket_encoding, \n",
    "                         'case_id_col':dataset_manager.case_id_col, \n",
    "                         'cat_cols':[dataset_manager.activity_col], \n",
    "                         'num_cols':[], \n",
    "                         'random_state':random_state}\n",
    "        if bucket_method == \"cluster\":\n",
    "            bucketer_args[\"n_clusters\"] = args[\"n_clusters\"]\n",
    "        bucketer = BucketFactory.get_bucketer(bucket_method, **bucketer_args)\n",
    "        bucket_assignments_train = bucketer.fit_predict(dt_train_prefixes)\n",
    "        bucket_assignments_test = bucketer.predict(dt_test_prefixes)\n",
    "        \n",
    "        preds_all = []\n",
    "        test_y_all = []\n",
    "        if \"prefix\" in method_name:\n",
    "            scores = defaultdict(int)\n",
    "        for bucket in set(bucket_assignments_test):\n",
    "            relevant_train_cases_bucket = dataset_manager.get_indexes(dt_train_prefixes)[bucket_assignments_train == bucket]\n",
    "            relevant_test_cases_bucket = dataset_manager.get_indexes(dt_test_prefixes)[bucket_assignments_test == bucket]\n",
    "            dt_test_bucket = dataset_manager.get_relevant_data_by_indexes(dt_test_prefixes, relevant_test_cases_bucket)\n",
    "            test_y = dataset_manager.get_label_numeric(dt_test_bucket)\n",
    "            if len(relevant_train_cases_bucket) == 0:\n",
    "                preds = [class_ratios[cv_iter]] * len(relevant_test_cases_bucket)\n",
    "            else:\n",
    "                dt_train_bucket = dataset_manager.get_relevant_data_by_indexes(dt_train_prefixes, relevant_train_cases_bucket) # one row per event\n",
    "                train_y = dataset_manager.get_label_numeric(dt_train_bucket)\n",
    "                \n",
    "                if len(set(train_y)) < 2:\n",
    "                    preds = [train_y[0]] * len(relevant_test_cases_bucket)\n",
    "                else:\n",
    "                    feature_combiner = FeatureUnion([(method, EncoderFactory.get_encoder(method, **cls_encoder_args)) for method in methods])\n",
    "\n",
    "                    if cls_method == \"rf\":\n",
    "                        cls = RandomForestClassifier(n_estimators=500,\n",
    "                                                     max_features=args['max_features'],\n",
    "                                                     random_state=random_state)\n",
    "\n",
    "                    elif cls_method == \"xgboost\":\n",
    "                        cls = xgb.XGBClassifier(objective='binary:logistic',\n",
    "                                                n_estimators=500,\n",
    "                                                learning_rate= args['learning_rate'],\n",
    "                                                subsample=args['subsample'],\n",
    "                                                max_depth=int(args['max_depth']),\n",
    "                                                colsample_bytree=args['colsample_bytree'],\n",
    "                                                min_child_weight=int(args['min_child_weight']),\n",
    "                                                seed=random_state)\n",
    "\n",
    "                    elif cls_method == \"logit\":\n",
    "                        cls = LogisticRegression(C=2**args['C'],\n",
    "                                                 random_state=random_state)\n",
    "\n",
    "                    elif cls_method == \"svm\":\n",
    "                        cls = SVC(C=2**args['C'],\n",
    "                                  gamma=2**args['gamma'],\n",
    "                                  random_state=random_state)\n",
    "\n",
    "                    if cls_method == \"svm\" or cls_method == \"logit\":\n",
    "                        pipeline = Pipeline([('encoder', feature_combiner), ('scaler', StandardScaler()), ('cls', cls)])\n",
    "                    else:\n",
    "                        pipeline = Pipeline([('encoder', feature_combiner), ('cls', cls)])\n",
    "                    pipeline.fit(dt_train_bucket, train_y)\n",
    "\n",
    "                    if cls_method == \"svm\":\n",
    "                        preds = pipeline.decision_function(dt_test_bucket)\n",
    "                    else:\n",
    "                        preds_pos_label_idx = np.where(cls.classes_ == 1)[0][0]\n",
    "                        preds = pipeline.predict_proba(dt_test_bucket)[:,preds_pos_label_idx]\n",
    "            \n",
    "            if \"prefix\" in method_name:\n",
    "                auc = 0.5\n",
    "                if len(set(test_y)) == 2: \n",
    "                    auc = roc_auc_score(test_y, preds)\n",
    "                scores[bucket] += auc\n",
    "            preds_all.extend(preds)\n",
    "            test_y_all.extend(test_y)\n",
    "\n",
    "        score += roc_auc_score(test_y_all, preds_all)\n",
    "    \n",
    "    if \"prefix\" in method_name:\n",
    "        for k, v in args.items():\n",
    "            for bucket, bucket_score in scores.items():\n",
    "                fout_all.write(\"%s;%s;%s;%s;%s;%s;%s;%s\\n\" % (trial_nr, dataset_name, cls_method, method_name, bucket, k, v, bucket_score / n_splits))   \n",
    "        fout_all.write(\"%s;%s;%s;%s;%s;%s;%s;%s\\n\" % (trial_nr, dataset_name, cls_method, method_name, 0, \"processing_time\", time.time() - start, 0))  \n",
    "    else:\n",
    "        for k, v in args.items():\n",
    "            fout_all.write(\"%s;%s;%s;%s;%s;%s;%s\\n\" % (trial_nr, dataset_name, cls_method, method_name, k, v, score / n_splits))   \n",
    "        fout_all.write(\"%s;%s;%s;%s;%s;%s;%s\\n\" % (trial_nr, dataset_name, cls_method, method_name, \"processing_time\", time.time() - start, 0))   \n",
    "    fout_all.flush()\n",
    "    return {'loss': -score / n_splits, 'status': STATUS_OK, 'model': cls}\n",
    "\n",
    "\n",
    "# dataset_ref = argv[1]\n",
    "# params_dir = argv[2]\n",
    "# n_iter = int(argv[3])\n",
    "# bucket_method = argv[4]\n",
    "# cls_encoding = argv[5]\n",
    "# cls_method = argv[6]\n",
    "\n",
    "dataset_ref = \"bpic2012\"\n",
    "params_dir = \"params\"\n",
    "n_iter = 3\n",
    "bucket_method = \"single\"\n",
    "cls_encoding = \"agg\"\n",
    "cls_method = \"rf\"\n",
    "\n",
    "if bucket_method == \"state\":\n",
    "    bucket_encoding = \"last\"\n",
    "else:\n",
    "    bucket_encoding = \"agg\"\n",
    "\n",
    "method_name = \"%s_%s\"%(bucket_method, cls_encoding)\n",
    "\n",
    "dataset_ref_to_datasets = {\n",
    "    \"bpic2011\": [\"bpic2011_f%s\"%formula for formula in range(1,5)],\n",
    "    \"bpic2015\": [\"bpic2015_%s_f2\"%(municipality) for municipality in range(5,6)],\n",
    "    \"insurance\": [\"insurance_activity\", \"insurance_followup\"],\n",
    "    \"bpic2012\" : [\"bpic2012_accepted\"],\n",
    "    \"sepsis_cases\": [\"sepsis_cases_1\", \"sepsis_cases_2\", \"sepsis_cases_4\"]\n",
    "}\n",
    "\n",
    "encoding_dict = {\n",
    "    \"laststate\": [\"static\", \"last\"],\n",
    "    \"agg\": [\"static\", \"agg\"],\n",
    "    \"index\": [\"static\", \"index\"],\n",
    "    \"combined\": [\"static\", \"last\", \"agg\"]\n",
    "}\n",
    "\n",
    "datasets = [dataset_ref] if dataset_ref not in dataset_ref_to_datasets else dataset_ref_to_datasets[dataset_ref]\n",
    "methods = encoding_dict[cls_encoding]\n",
    "print(datasets)\n",
    "    \n",
    "train_ratio = 0.8\n",
    "n_splits = 3\n",
    "random_state = 22\n",
    "\n",
    "# create results directory\n",
    "if not os.path.exists(os.path.join(params_dir)):\n",
    "    os.makedirs(os.path.join(params_dir))\n",
    "    \n",
    "for dataset_name in datasets:\n",
    "    \n",
    "    # read the data\n",
    "    dataset_manager = DatasetManager(dataset_name)\n",
    "    data = dataset_manager.read_dataset()\n",
    "    cls_encoder_args = {'case_id_col': dataset_manager.case_id_col, \n",
    "                        'static_cat_cols': dataset_manager.static_cat_cols,\n",
    "                        'static_num_cols': dataset_manager.static_num_cols, \n",
    "                        'dynamic_cat_cols': dataset_manager.dynamic_cat_cols,\n",
    "                        'dynamic_num_cols': dataset_manager.dynamic_num_cols, \n",
    "                        'fillna': True}\n",
    "\n",
    "    # determine min and max (truncated) prefix lengths\n",
    "    min_prefix_length = 1\n",
    "    if \"traffic_fines\" in dataset_name:\n",
    "        max_prefix_length = 10\n",
    "    elif \"bpic2017\" in dataset_name:\n",
    "        max_prefix_length = min(20, dataset_manager.get_pos_case_length_quantile(data, 0.90))\n",
    "    else:\n",
    "        max_prefix_length = min(40, dataset_manager.get_pos_case_length_quantile(data, 0.90))\n",
    "\n",
    "    # split into training and test\n",
    "    print(\"splitting data\")\n",
    "    train, _ = dataset_manager.split_data_strict(data, train_ratio, split=\"temporal\")\n",
    "    \n",
    "    # prepare chunks for CV\n",
    "    dt_prefixes = []\n",
    "    class_ratios = []\n",
    "    for train_chunk, test_chunk in dataset_manager.get_stratified_split_generator(train, n_splits=n_splits):\n",
    "        class_ratios.append(dataset_manager.get_class_ratio(train_chunk))\n",
    "        # generate data where each prefix is a separate instance\n",
    "        dt_prefixes.append(dataset_manager.generate_prefix_data(test_chunk, min_prefix_length, max_prefix_length))\n",
    "    del train\n",
    "        \n",
    "    # set up search space\n",
    "    if cls_method == \"rf\":\n",
    "        space = {'max_features': hp.uniform('max_features', 0, 1)}\n",
    "    elif cls_method == \"xgboost\":\n",
    "        space = {'learning_rate': hp.uniform(\"learning_rate\", 0, 1),\n",
    "                 'subsample': hp.uniform(\"subsample\", 0.5, 1),\n",
    "                 'max_depth': scope.int(hp.quniform('max_depth', 4, 30, 1)),\n",
    "                 'colsample_bytree': hp.uniform(\"colsample_bytree\", 0.5, 1),\n",
    "                 'min_child_weight': scope.int(hp.quniform('min_child_weight', 1, 6, 1))}\n",
    "    elif cls_method == \"logit\":\n",
    "        space = {'C': hp.uniform('C', -15, 15)}\n",
    "    elif cls_method == \"svm\":\n",
    "        space = {'C': hp.uniform('C', -15, 15),\n",
    "                 'gamma': hp.uniform('gamma', -15, 15)}\n",
    "    if bucket_method == \"cluster\":\n",
    "        space['n_clusters'] = scope.int(hp.quniform('n_clusters', 2, 50, 1))\n",
    "\n",
    "    # optimize parameters\n",
    "    trial_nr = 1\n",
    "    trials = Trials()\n",
    "    fout_all = open(os.path.join(params_dir, \"param_optim_all_trials_%s_%s_%s.csv\" % (cls_method, dataset_name, method_name)), \"w\")\n",
    "    if \"prefix\" in method_name:\n",
    "        fout_all.write(\"%s;%s;%s;%s;%s;%s;%s;%s\\n\" % (\"iter\", \"dataset\", \"cls\", \"method\", \"nr_events\", \"param\", \"value\", \"score\"))   \n",
    "    else:\n",
    "        fout_all.write(\"%s;%s;%s;%s;%s;%s;%s\\n\" % (\"iter\", \"dataset\", \"cls\", \"method\", \"param\", \"value\", \"score\"))   \n",
    "    best = fmin(create_and_evaluate_model, space, algo=tpe.suggest, max_evals=n_iter, trials=trials, verbose=True)\n",
    "    fout_all.close()\n",
    "\n",
    "    # write the best parameters\n",
    "    best_params = hyperopt.space_eval(space, best)\n",
    "    outfile = os.path.join(params_dir, \"optimal_params_%s_%s_%s.pickle\" % (cls_method, dataset_name, method_name))\n",
    "    # write to file\n",
    "    with open(outfile, \"wb\") as fout:\n",
    "        pickle.dump(best_params, fout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
