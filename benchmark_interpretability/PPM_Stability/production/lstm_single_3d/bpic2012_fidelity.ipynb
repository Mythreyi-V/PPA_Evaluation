{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "import sys\n",
    "#PATH = '/content/drive/My Drive/PPM_Stability/'\n",
    "PATH = \"C:/Users/velmurug/Documents/Stability Experiments/benchmark_interpretability/PPM_Stability/\"\n",
    "#PATH = \"C:/Users/Mythreyi/Documents/GitHub/Stability-Experiments/benchmark_interpretability/PPM_Stability/\"\n",
    "#PATH = \"C:/Users/mythr/Documents/GitHub/Stability-Experiments/benchmark_interpretability/PPM_Stability/\"\n",
    "#PATH = \"C:/Users/n9455647/Documents/GitHub/Stability-Experiments/benchmark_interpretability/PPM_Stability/\"\n",
    "sys.path.append(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost==1.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (1.0.0)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (from xgboost==1.0.0) (1.4.1)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from xgboost==1.0.0) (1.18.5)\n"
     ]
    }
   ],
   "source": [
    "#!pip install lime\n",
    "#!pip install shap\n",
    "#!pip install pandas==0.19.2\n",
    "!pip install xgboost==1.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import EncoderFactory\n",
    "from DatasetManager import DatasetManager\n",
    "import BucketFactory\n",
    "import stability as st #Nogueira, Sechidis, Brown.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import math\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "from sys import argv\n",
    "import pickle\n",
    "from collections import defaultdict, Counter\n",
    "import random\n",
    "import joblib\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import Dense, Embedding, Flatten, Input, LSTM\n",
    "from keras.optimizers import Nadam, RMSprop\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "from tensorflow.keras.backend import print_tensor\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.compat.v1 import disable_v2_behavior#, ConfigProto, Session\n",
    "from tensorflow.compat.v1.keras.backend import get_session\n",
    "disable_v2_behavior()\n",
    "\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "from lime import submodular_pick;\n",
    "\n",
    "import shap\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def imp_df(column_names, importances):\n",
    "        df = pd.DataFrame({'feature': column_names,\n",
    "                       'feature_importance': importances}) \\\n",
    "           .sort_values('feature_importance', ascending = False) \\\n",
    "           .reset_index(drop = True)\n",
    "        return df\n",
    "\n",
    "# plotting a feature importance dataframe (horizontal barchart)\n",
    "def var_imp_plot(imp_df, title, num_feat):\n",
    "        imp_df.columns = ['feature', 'feature_importance']\n",
    "        b= sns.barplot(x = 'feature_importance', y = 'feature', data = imp_df.head(num_feat), orient = 'h', palette=\"Blues_r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_global_explanations(train_X,train_Y, cls, feature_combiner):\n",
    "    \n",
    "    print(\"The number of testing instances is \",len(train_Y))\n",
    "    print(\"The total number of columns is\",train_X.shape[1]);\n",
    "    print(\"The total accuracy is \",cls.score(train_X,train_Y));\n",
    "       \n",
    "    sns.set(rc={'figure.figsize':(10,10), \"font.size\":18,\"axes.titlesize\":18,\"axes.labelsize\":18})\n",
    "    sns.set\n",
    "    feat_names = feature_combiner.get_feature_names()\n",
    "    base_imp = imp_df(feat_names, cls.feature_importances_)\n",
    "    base_imp.head(15)\n",
    "    var_imp_plot(base_imp, 'Feature importance using XGBoost', 15)\n",
    "    return base_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime import submodular_pick\n",
    "def generate_lime_explanations(explainer,test_xi, cls,test_y, submod=False, test_all_data=None, max_feat = 10):\n",
    "    \n",
    "    #print(\"Actual value \", test_y)\n",
    "    exp = explainer.explain_instance(test_xi, \n",
    "                                 cls.predict_proba, num_features=max_feat, labels=[0,1])\n",
    "    \n",
    "    return exp\n",
    "        \n",
    "    if submod==True:\n",
    "        sp_obj=submodular_pick.SubmodularPick(explainer, test_all_data, cls.predict_proba, \n",
    "                                      sample_size=20, num_features=num_features,num_exps_desired=4)\n",
    "        [exp.as_pyplot_figure(label=exp.available_labels()[0]) for exp in sp_obj.sp_explanations];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_samples(shap_explainer, iterations, row, features, top = None):\n",
    "    length = len(features)\n",
    "    \n",
    "    exp = []\n",
    "    rel_exp = []\n",
    "    \n",
    "    for j in range(iterations):\n",
    "        \n",
    "        shap_values = shap_explainer.shap_values(row)\n",
    "        \n",
    "        importances = []\n",
    "        \n",
    "        if type(shap_explainer) == shap.explainers.kernel.KernelExplainer:\n",
    "            for i in range(length):\n",
    "                feat = features[i]\n",
    "                shap_val = shap_values[0][i]\n",
    "                abs_val = abs(shap_values[0][i])\n",
    "                entry = (feat, shap_val, abs_val)\n",
    "                importances.append(entry)\n",
    "                \n",
    "        elif type(shap_explainer) == shap.explainers.tree.TreeExplainer:\n",
    "            for i in range(length):\n",
    "                feat = features[i]\n",
    "                shap_val = shap_values[0][i]\n",
    "                abs_val = abs(shap_values[0][i])\n",
    "                entry = (feat, shap_val, abs_val)\n",
    "                importances.append(entry)\n",
    "        \n",
    "        elif type(shap_explainer) == shap.explainers.deep.DeepExplainer:\n",
    "            for i in range(length):\n",
    "                if len(features.shape) == 2:\n",
    "                    for j in range(len(features[i])):\n",
    "                        feat = features[i][j]\n",
    "                        shap_val = shap_values[0][0][i][j]\n",
    "                        abs_val = abs(shap_values[0][0][i][j])\n",
    "                        entry = (feat, shap_val, abs_val)\n",
    "                        importances.append(entry)\n",
    "                else:\n",
    "                    feat = features[i]\n",
    "                    shap_val = shap_values[0][0][i]\n",
    "                    abs_val = abs(shap_values[0][0][i])\n",
    "                    entry = (feat, shap_val, abs_val)\n",
    "                    importances.append(entry)\n",
    "    \n",
    "        importances.sort(key=lambda tup: tup[2], reverse = True)\n",
    "        \n",
    "        exp.append(importances)\n",
    "\n",
    "        rel_feat = []\n",
    "\n",
    "        if top != None:\n",
    "            for i in range(top):\n",
    "                feat = importances[i]\n",
    "                if feat[2] > 0:\n",
    "                    rel_feat.append(feat)\n",
    "\n",
    "            rel_exp.append(rel_feat)\n",
    "        else:\n",
    "            rel_exp = exp\n",
    "        \n",
    "    return exp, rel_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_distributions(explainer, features, test_x, bin_min = -1, bin_max = 1, bin_width = 0.05):\n",
    "    \n",
    "    #if type(explainer) == shap.explainers.tree.TreeExplainer:\n",
    "        #generate shap values for entire test set\n",
    "        shap_values = explainer.shap_values(test_x, check_additivity = False)\n",
    "    #    print(shap_values)\n",
    "        if type(explainer) == shap.explainers.tree.TreeExplainer:\n",
    "            shap_val_feat = np.transpose(shap_values)\n",
    "            feats = np.transpose(test_x)\n",
    "        elif type(explainer) == shap.explainers.deep.DeepExplainer:\n",
    "            copy_val = []\n",
    "            for each in shap_values[0]:\n",
    "                copy_val.append(list(each.flatten()))\n",
    "            copy_val = np.array(copy_val)\n",
    "            shap_val_feat = np.transpose(copy_val)\n",
    "            \n",
    "            copy_feat = []\n",
    "            for each in test_x:\n",
    "                copy_feat.append(list(each.flatten()))\n",
    "            copy_feat = np.array(copy_feat)\n",
    "            feats = np.transpose(copy_feat)\n",
    "            \n",
    "        \n",
    "        features = features.flatten()\n",
    "\n",
    "        shap_distribs = []\n",
    "\n",
    "        #For each feature\n",
    "        for i in range(len(features)):\n",
    "            print (i+1, \"of\", len(features), \"features\")\n",
    "            shap_vals = shap_val_feat[i]\n",
    "    #        print(shap_vals)\n",
    "\n",
    "            #create bins based on shap value ranges\n",
    "            bins = np.arange(bin_min, bin_max, bin_width)\n",
    "\n",
    "            feat_vals = []\n",
    "            for sbin in range(len(bins)):\n",
    "                nl = []\n",
    "                feat_vals.append(nl)\n",
    "\n",
    "            #place relevant feature values into each bin\n",
    "            for j in range(len(shap_vals)):\n",
    "                val = shap_vals[j]\n",
    "                b = 0\n",
    "                cur_bin = bins[b]\n",
    "                idx = b\n",
    "\n",
    "                while val > cur_bin and b < len(bins)-1:\n",
    "                    #print(cur_bin)\n",
    "                    idx = b\n",
    "                    b+=1\n",
    "                    #print(b)\n",
    "                    cur_bin = bins[b]\n",
    "\n",
    "                #print(val, idx)\n",
    "                #print(val, idx, i, j)\n",
    "                feat_vals[idx].append(feats[i][j])\n",
    "\n",
    "            #Remove feature values that are outliers\n",
    "            #for each in feat_vals:\n",
    "            #    zscore = stats.zscore(each)\n",
    "                #print(each)\n",
    "            #    for n in range(len(zscore)):\n",
    "            #        if zscore[n] > 3 or zscore[n] < -3:\n",
    "            #            np.delete(zscore, n)\n",
    "            #            del each[n]\n",
    "                #print(each)\n",
    "\n",
    "            #Find min and max values for each shap value bin\n",
    "            mins = []\n",
    "            maxes = []\n",
    "            #width = []\n",
    "            #print(feat_vals)\n",
    "            #n = 0\n",
    "            for each in feat_vals:\n",
    "                if each != []:\n",
    "                    mins.append(min(each))\n",
    "                    maxes.append(max(each))\n",
    "             #       width.append(\"Bin \"+str(n))\n",
    "             #       n+=1\n",
    "            #plt.bar(width, maxes, bottom = mins)\n",
    "            #plt.show()\n",
    "\n",
    "            #Create dictionary with list of bins and max and min feature values for each bin\n",
    "            feat_name = features[i]\n",
    "\n",
    "            feat_dict = {'Feature Name': feat_name}\n",
    "            for each in feat_vals:\n",
    "                if each != []:\n",
    "                    mins.append(min(each))\n",
    "                    maxes.append(max(each))\n",
    "                else:\n",
    "                    mins.append(None)\n",
    "                    maxes.append(None)\n",
    "\n",
    "            feat_dict['bins'] = bins\n",
    "            feat_dict['mins'] = mins\n",
    "            feat_dict['maxes'] = maxes\n",
    "\n",
    "            shap_distribs.append(feat_dict)\n",
    "        \n",
    "#     elif type(explainer) == shap.explainers.deep.DeepExplainer:\n",
    "#         #generate shap values for entire test set\n",
    "#         shap_values = explainer.shap_values(test_x, check_additivity = False)\n",
    "        \n",
    "#         for each in shap_values[0]:\n",
    "#             print(each.shape)\n",
    "#         print(test_x.shape)\n",
    "#         print(features.shape)\n",
    "        \n",
    "#         for l in range(len(shap_values[0])):\n",
    "#             print (l+1, \"of\", (len(shap_values[0])))\n",
    "#     #    print(shap_values)\n",
    "#             shap_val_feat = np.transpose(shap_values[l][0])\n",
    "#             #print(shap_val_feat)\n",
    "#             feats = np.transpose(test_x[l])\n",
    "\n",
    "#             shap_distribs = []\n",
    "\n",
    "#             #For each feature\n",
    "#             for i in range(len(features[l])):\n",
    "#                 print (i+1, \"of\", len(features[l]), \"features\")\n",
    "#                 shap_vals = shap_val_feat[i]\n",
    "#         #        print(shap_vals)\n",
    "\n",
    "#                 #create bins based on shap value ranges\n",
    "#                 bins = np.arange(bin_min, bin_max, bin_width)\n",
    "\n",
    "#                 feat_vals = []\n",
    "#                 for sbin in range(len(bins)):\n",
    "#                     nl = []\n",
    "#                     feat_vals.append(nl)\n",
    "\n",
    "#                 #place relevant feature values into each bin\n",
    "#                 for j in range(len(shap_vals)):\n",
    "#                     val = shap_vals[j]\n",
    "#                     b = 0\n",
    "#                     cur_bin = bins[b]\n",
    "#                     idx = b\n",
    "                    \n",
    "#                     #print(val)\n",
    "#                     #print(cur_bin)\n",
    "                    \n",
    "#                     while val > cur_bin and b < len(bins)-1:\n",
    "#                         #print(cur_bin)\n",
    "#                         idx = b\n",
    "#                         b+=1\n",
    "#                         #print(b)\n",
    "#                         cur_bin = bins[b]\n",
    "\n",
    "#                     #print(val, idx)\n",
    "#                     feat_vals[idx].append(feats[i][j])\n",
    "\n",
    "#                 #Remove feature values that are outliers\n",
    "#                 #for each in feat_vals:\n",
    "#                 #    zscore = stats.zscore(each)\n",
    "#                     #print(each)\n",
    "#                 #    for n in range(len(zscore)):\n",
    "#                 #        if zscore[n] > 3 or zscore[n] < -3:\n",
    "#                 #            np.delete(zscore, n)\n",
    "#                 #            del each[n]\n",
    "#                     #print(each)\n",
    "\n",
    "#                 #Find min and max values for each shap value bin\n",
    "#                 mins = []\n",
    "#                 maxes = []\n",
    "#                 #width = []\n",
    "#                 #print(feat_vals)\n",
    "#                 #n = 0\n",
    "#                 for each in feat_vals:\n",
    "#                     if each != []:\n",
    "#                         mins.append(min(each))\n",
    "#                         maxes.append(max(each))\n",
    "#                  #       width.append(\"Bin \"+str(n))\n",
    "#                  #       n+=1\n",
    "#                 #plt.bar(width, maxes, bottom = mins)\n",
    "#                 #plt.show()\n",
    "\n",
    "#                 #Create dictionary with list of bins and max and min feature values for each bin\n",
    "#                 feat_name = features[l][i]\n",
    "\n",
    "#                 feat_dict = {'Feature Name': feat_name}\n",
    "#                 for each in feat_vals:\n",
    "#                     if each != []:\n",
    "#                         mins.append(min(each))\n",
    "#                         maxes.append(max(each))\n",
    "#                     else:\n",
    "#                         mins.append(None)\n",
    "#                         maxes.append(None)\n",
    "\n",
    "#                 feat_dict['bins'] = bins\n",
    "#                 feat_dict['mins'] = mins\n",
    "#                 feat_dict['maxes'] = maxes\n",
    "\n",
    "#                 shap_distribs.append(feat_dict)\n",
    "        \n",
    "        return shap_distribs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "['production']"
      ],
      "text/plain": [
       "['production']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_ref = \"production\"\n",
    "params_dir = PATH + \"params\"\n",
    "results_dir = \"results\"\n",
    "bucket_method = \"single\"\n",
    "cls_encoding = \"3d\"\n",
    "cls_method = \"lstm\"\n",
    "\n",
    "gap = 1\n",
    "n_iter = 1\n",
    "\n",
    "method_name = \"%s_%s\"%(bucket_method, cls_encoding)\n",
    "\n",
    "generate_samples = False\n",
    "generate_lime = True\n",
    "generate_kernel_shap = False\n",
    "generate_model_shap = True\n",
    "\n",
    "sample_size = 2\n",
    "exp_iter = 10\n",
    "#max_feat = 10\n",
    "\n",
    "dataset_ref_to_datasets = {\n",
    "    #\"bpic2011\": [\"bpic2011_f%s\"%formula for formula in range(1,5)],\n",
    "    \"bpic2015\": [\"bpic2015_%s_f2\"%(municipality) for municipality in range(5,6)],\n",
    "    \"bpic2017\" : [\"bpic2017_accepted\"],\n",
    "    \"bpic2012\" : [\"bpic2012_accepted\"],\n",
    "    #\"insurance\": [\"insurance_activity\", \"insurance_followup\"],\n",
    "    \"sepsis_cases\": [\"sepsis_cases_1\"],# \"sepsis_cases_2\", \"sepsis_cases_4\"]\n",
    "    \"production\" : [\"production\"]\n",
    "}\n",
    "\n",
    "datasets = [dataset_ref] if dataset_ref not in dataset_ref_to_datasets else dataset_ref_to_datasets[dataset_ref]\n",
    "\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------SHAP----------------------------------------------\n",
      "Bucket all\n",
      "get everything to create model\n",
      "Parameters loaded\n",
      "defining input layer\n",
      "adding lstm layers\n",
      "adding dense layers\n",
      "putting together layers\n",
      "choosing optimiser\n",
      "adding weights to model\n",
      "compiling model\n",
      "Generating distributions for bucket\n",
      "Time taken to generate distribution: 233.3896062374115\n",
      "Category 1 of 4. Instance 1 of 106\n",
      "(23, 114)\n",
      "(1, 23, 114)\n",
      "Creating explanations\n",
      "Identifying relevant features\n",
      "Creating distribution for feature 1 of 262\n",
      "Creating distribution for feature 2 of 262\n",
      "Creating distribution for feature 3 of 262\n",
      "Creating distribution for feature 4 of 262\n",
      "Creating distribution for feature 5 of 262\n",
      "Creating distribution for feature 6 of 262\n",
      "Creating distribution for feature 7 of 262\n",
      "Creating distribution for feature 8 of 262\n",
      "Creating distribution for feature 9 of 262\n",
      "Creating distribution for feature 10 of 262\n",
      "Creating distribution for feature 11 of 262\n",
      "Creating distribution for feature 12 of 262\n",
      "Creating distribution for feature 13 of 262\n",
      "Creating distribution for feature 14 of 262\n",
      "Creating distribution for feature 15 of 262\n",
      "Creating distribution for feature 16 of 262\n",
      "Creating distribution for feature 17 of 262\n",
      "Creating distribution for feature 18 of 262\n",
      "Creating distribution for feature 19 of 262\n",
      "Creating distribution for feature 20 of 262\n",
      "Creating distribution for feature 21 of 262\n",
      "Creating distribution for feature 22 of 262\n",
      "Creating distribution for feature 23 of 262\n",
      "Creating distribution for feature 24 of 262\n",
      "Creating distribution for feature 25 of 262\n",
      "Creating distribution for feature 26 of 262\n",
      "Creating distribution for feature 27 of 262\n",
      "Creating distribution for feature 28 of 262\n",
      "Creating distribution for feature 29 of 262\n",
      "Creating distribution for feature 30 of 262\n",
      "Creating distribution for feature 31 of 262\n",
      "Creating distribution for feature 32 of 262\n",
      "Creating distribution for feature 33 of 262\n",
      "Creating distribution for feature 34 of 262\n",
      "Creating distribution for feature 35 of 262\n",
      "Creating distribution for feature 36 of 262\n",
      "Creating distribution for feature 37 of 262\n",
      "Creating distribution for feature 38 of 262\n",
      "Creating distribution for feature 39 of 262\n",
      "Creating distribution for feature 40 of 262\n",
      "Creating distribution for feature 41 of 262\n",
      "Creating distribution for feature 42 of 262\n",
      "Creating distribution for feature 43 of 262\n",
      "Creating distribution for feature 44 of 262\n",
      "Creating distribution for feature 45 of 262\n",
      "Creating distribution for feature 46 of 262\n",
      "Creating distribution for feature 47 of 262\n",
      "Creating distribution for feature 48 of 262\n",
      "Creating distribution for feature 49 of 262\n",
      "Creating distribution for feature 50 of 262\n",
      "Creating distribution for feature 51 of 262\n",
      "Creating distribution for feature 52 of 262\n",
      "Creating distribution for feature 53 of 262\n",
      "Creating distribution for feature 54 of 262\n",
      "Creating distribution for feature 55 of 262\n",
      "Creating distribution for feature 56 of 262\n",
      "Creating distribution for feature 57 of 262\n",
      "Creating distribution for feature 58 of 262\n",
      "Creating distribution for feature 59 of 262\n",
      "Creating distribution for feature 60 of 262\n",
      "Creating distribution for feature 61 of 262\n",
      "Creating distribution for feature 62 of 262\n",
      "Creating distribution for feature 63 of 262\n",
      "Creating distribution for feature 64 of 262\n",
      "Creating distribution for feature 65 of 262\n",
      "Creating distribution for feature 66 of 262\n",
      "Creating distribution for feature 67 of 262\n",
      "Creating distribution for feature 68 of 262\n",
      "Creating distribution for feature 69 of 262\n",
      "Creating distribution for feature 70 of 262\n",
      "Creating distribution for feature 71 of 262\n",
      "Creating distribution for feature 72 of 262\n",
      "Creating distribution for feature 73 of 262\n",
      "Creating distribution for feature 74 of 262\n",
      "Creating distribution for feature 75 of 262\n",
      "Creating distribution for feature 76 of 262\n",
      "Creating distribution for feature 77 of 262\n",
      "Creating distribution for feature 78 of 262\n",
      "Creating distribution for feature 79 of 262\n",
      "Creating distribution for feature 80 of 262\n",
      "Creating distribution for feature 81 of 262\n",
      "Creating distribution for feature 82 of 262\n",
      "Creating distribution for feature 83 of 262\n",
      "Creating distribution for feature 84 of 262\n",
      "Creating distribution for feature 85 of 262\n",
      "Creating distribution for feature 86 of 262\n",
      "Creating distribution for feature 87 of 262\n",
      "Creating distribution for feature 88 of 262\n",
      "Creating distribution for feature 89 of 262\n",
      "Creating distribution for feature 90 of 262\n",
      "Creating distribution for feature 91 of 262\n",
      "Creating distribution for feature 92 of 262\n",
      "Creating distribution for feature 93 of 262\n",
      "Creating distribution for feature 94 of 262\n",
      "Creating distribution for feature 95 of 262\n",
      "Creating distribution for feature 96 of 262\n",
      "Creating distribution for feature 97 of 262\n",
      "Creating distribution for feature 98 of 262\n",
      "Creating distribution for feature 99 of 262\n",
      "Creating distribution for feature 100 of 262\n",
      "Creating distribution for feature 101 of 262\n",
      "Creating distribution for feature 102 of 262\n",
      "Creating distribution for feature 103 of 262\n",
      "Creating distribution for feature 104 of 262\n",
      "Creating distribution for feature 105 of 262\n",
      "Creating distribution for feature 106 of 262\n",
      "Creating distribution for feature 107 of 262\n",
      "Creating distribution for feature 108 of 262\n",
      "Creating distribution for feature 109 of 262\n",
      "Creating distribution for feature 110 of 262\n",
      "Creating distribution for feature 111 of 262\n",
      "Creating distribution for feature 112 of 262\n",
      "Creating distribution for feature 113 of 262\n",
      "Creating distribution for feature 114 of 262\n",
      "Creating distribution for feature 115 of 262\n",
      "Creating distribution for feature 116 of 262\n",
      "Creating distribution for feature 117 of 262\n",
      "Creating distribution for feature 118 of 262\n",
      "Creating distribution for feature 119 of 262\n",
      "Creating distribution for feature 120 of 262\n",
      "Creating distribution for feature 121 of 262\n",
      "Creating distribution for feature 122 of 262\n",
      "Creating distribution for feature 123 of 262\n",
      "Creating distribution for feature 124 of 262\n",
      "Creating distribution for feature 125 of 262\n",
      "Creating distribution for feature 126 of 262\n",
      "Creating distribution for feature 127 of 262\n",
      "Creating distribution for feature 128 of 262\n",
      "Creating distribution for feature 129 of 262\n",
      "Creating distribution for feature 130 of 262\n",
      "Creating distribution for feature 131 of 262\n",
      "Creating distribution for feature 132 of 262\n",
      "Creating distribution for feature 133 of 262\n",
      "Creating distribution for feature 134 of 262\n",
      "Creating distribution for feature 135 of 262\n",
      "Creating distribution for feature 136 of 262\n",
      "Creating distribution for feature 137 of 262\n",
      "Creating distribution for feature 138 of 262\n",
      "Creating distribution for feature 139 of 262\n",
      "Creating distribution for feature 140 of 262\n",
      "Creating distribution for feature 141 of 262\n",
      "Creating distribution for feature 142 of 262\n",
      "Creating distribution for feature 143 of 262\n",
      "Creating distribution for feature 144 of 262\n",
      "Creating distribution for feature 145 of 262\n",
      "Creating distribution for feature 146 of 262\n",
      "Creating distribution for feature 147 of 262\n",
      "Creating distribution for feature 148 of 262\n",
      "Creating distribution for feature 149 of 262\n",
      "Creating distribution for feature 150 of 262\n",
      "Creating distribution for feature 151 of 262\n",
      "Creating distribution for feature 152 of 262\n",
      "Creating distribution for feature 153 of 262\n",
      "Creating distribution for feature 154 of 262\n",
      "Creating distribution for feature 155 of 262\n",
      "Creating distribution for feature 156 of 262\n",
      "Creating distribution for feature 157 of 262\n",
      "Creating distribution for feature 158 of 262\n",
      "Creating distribution for feature 159 of 262\n",
      "Creating distribution for feature 160 of 262\n",
      "Creating distribution for feature 161 of 262\n",
      "Creating distribution for feature 162 of 262\n",
      "Creating distribution for feature 163 of 262\n",
      "Creating distribution for feature 164 of 262\n",
      "Creating distribution for feature 165 of 262\n",
      "Creating distribution for feature 166 of 262\n",
      "Creating distribution for feature 167 of 262\n",
      "Creating distribution for feature 168 of 262\n",
      "Creating distribution for feature 169 of 262\n",
      "Creating distribution for feature 170 of 262\n",
      "Creating distribution for feature 171 of 262\n",
      "Creating distribution for feature 172 of 262\n",
      "Creating distribution for feature 173 of 262\n",
      "Creating distribution for feature 174 of 262\n",
      "Creating distribution for feature 175 of 262\n",
      "Creating distribution for feature 176 of 262\n",
      "Creating distribution for feature 177 of 262\n",
      "Creating distribution for feature 178 of 262\n",
      "Creating distribution for feature 179 of 262\n",
      "Creating distribution for feature 180 of 262\n",
      "Creating distribution for feature 181 of 262\n",
      "Creating distribution for feature 182 of 262\n",
      "Creating distribution for feature 183 of 262\n",
      "Creating distribution for feature 184 of 262\n",
      "Creating distribution for feature 185 of 262\n",
      "Creating distribution for feature 186 of 262\n",
      "Creating distribution for feature 187 of 262\n",
      "Creating distribution for feature 188 of 262\n",
      "Creating distribution for feature 189 of 262\n",
      "Creating distribution for feature 190 of 262\n",
      "Creating distribution for feature 191 of 262\n",
      "Creating distribution for feature 192 of 262\n",
      "Creating distribution for feature 193 of 262\n",
      "Creating distribution for feature 194 of 262\n",
      "Creating distribution for feature 195 of 262\n",
      "Creating distribution for feature 196 of 262\n",
      "Creating distribution for feature 197 of 262\n",
      "Creating distribution for feature 198 of 262\n",
      "Creating distribution for feature 199 of 262\n",
      "Creating distribution for feature 200 of 262\n",
      "Creating distribution for feature 201 of 262\n",
      "Creating distribution for feature 202 of 262\n",
      "Creating distribution for feature 203 of 262\n",
      "Creating distribution for feature 204 of 262\n",
      "Creating distribution for feature 205 of 262\n",
      "Creating distribution for feature 206 of 262\n",
      "Creating distribution for feature 207 of 262\n",
      "Creating distribution for feature 208 of 262\n",
      "Creating distribution for feature 209 of 262\n",
      "Creating distribution for feature 210 of 262\n",
      "Creating distribution for feature 211 of 262\n",
      "Creating distribution for feature 212 of 262\n",
      "Creating distribution for feature 213 of 262\n",
      "Creating distribution for feature 214 of 262\n",
      "Creating distribution for feature 215 of 262\n",
      "Creating distribution for feature 216 of 262\n",
      "Creating distribution for feature 217 of 262\n",
      "Creating distribution for feature 218 of 262\n",
      "Creating distribution for feature 219 of 262\n",
      "Creating distribution for feature 220 of 262\n",
      "Creating distribution for feature 221 of 262\n",
      "Creating distribution for feature 222 of 262\n",
      "Creating distribution for feature 223 of 262\n",
      "Creating distribution for feature 224 of 262\n",
      "Creating distribution for feature 225 of 262\n",
      "Creating distribution for feature 226 of 262\n",
      "Creating distribution for feature 227 of 262\n",
      "Creating distribution for feature 228 of 262\n",
      "Creating distribution for feature 229 of 262\n",
      "Creating distribution for feature 230 of 262\n",
      "Creating distribution for feature 231 of 262\n",
      "Creating distribution for feature 232 of 262\n",
      "Creating distribution for feature 233 of 262\n",
      "Creating distribution for feature 234 of 262\n",
      "Creating distribution for feature 235 of 262\n",
      "Creating distribution for feature 236 of 262\n",
      "Creating distribution for feature 237 of 262\n",
      "Creating distribution for feature 238 of 262\n",
      "Creating distribution for feature 239 of 262\n",
      "Creating distribution for feature 240 of 262\n",
      "Creating distribution for feature 241 of 262\n",
      "Creating distribution for feature 242 of 262\n",
      "Creating distribution for feature 243 of 262\n",
      "Creating distribution for feature 244 of 262\n",
      "Creating distribution for feature 245 of 262\n",
      "Creating distribution for feature 246 of 262\n",
      "Creating distribution for feature 247 of 262\n",
      "Creating distribution for feature 248 of 262\n",
      "Creating distribution for feature 249 of 262\n",
      "Creating distribution for feature 250 of 262\n",
      "Creating distribution for feature 251 of 262\n",
      "Creating distribution for feature 252 of 262\n",
      "Creating distribution for feature 253 of 262\n",
      "Creating distribution for feature 254 of 262\n",
      "Creating distribution for feature 255 of 262\n",
      "Creating distribution for feature 256 of 262\n",
      "Creating distribution for feature 257 of 262\n",
      "Creating distribution for feature 258 of 262\n",
      "Creating distribution for feature 259 of 262\n",
      "Creating distribution for feature 260 of 262\n",
      "Creating distribution for feature 261 of 262\n",
      "Creating distribution for feature 262 of 262\n",
      "Pertubing - Run 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pertubing - Run 2\n",
      "Pertubing - Run 3\n",
      "Pertubing - Run 4\n",
      "Pertubing - Run 5\n",
      "Pertubing - Run 6\n",
      "Pertubing - Run 7\n",
      "Pertubing - Run 8\n",
      "Pertubing - Run 9\n",
      "Pertubing - Run 10\n",
      "[0.11507781]\n"
     ]
    }
   ],
   "source": [
    "#Try SHAP\n",
    "print(\"----------------------------------------------SHAP----------------------------------------------\")\n",
    "\n",
    "if generate_model_shap:\n",
    "    for dataset_name in datasets:\n",
    "\n",
    "        dataset_manager = DatasetManager(dataset_name)\n",
    "\n",
    "        for ii in range(n_iter):\n",
    "            if cls_method == \"lstm\":\n",
    "                num_buckets = range(1)\n",
    "            else:\n",
    "                num_buckets = range(len([name for name in os.listdir(os.path.join(PATH,'%s/%s_%s/models'% (dataset_ref, cls_method, method_name)))]))\n",
    "\n",
    "            all_shap_changes = []\n",
    "            all_lens = []\n",
    "            all_probas = []\n",
    "            all_case_ids = []\n",
    "\n",
    "            pos_shap_changes = []\n",
    "            pos_probas = []\n",
    "            pos_nr_events = []\n",
    "            pos_case_ids = []\n",
    "\n",
    "            neg_shap_changes = []\n",
    "            neg_probas = []\n",
    "            neg_nr_events = []\n",
    "            neg_case_ids = []\n",
    "\n",
    "            for bucket in list(num_buckets):\n",
    "                bucketID = \"all\"\n",
    "                print ('Bucket', bucketID)\n",
    "\n",
    "                #import everything needed to sort and predict\n",
    "                if cls_method == \"lstm\":\n",
    "                    print(\"get everything to create model\")\n",
    "                    params_path = os.path.join(PATH, \"%s/%s_%s/cls/params.pickle\" % (dataset_ref, cls_method, method_name))\n",
    "                    with open(params_path, 'rb') as f:\n",
    "                        args = pickle.load(f)\n",
    "\n",
    "                    max_len = args['max_len']\n",
    "                    data_dim = args['data_dim']\n",
    "                    print(\"Parameters loaded\")\n",
    "\n",
    "                    #create model\n",
    "                    print(\"defining input layer\")\n",
    "                    main_input = Input(shape=(max_len, data_dim), name='main_input')\n",
    "                    \n",
    "                    print(\"adding lstm layers\")\n",
    "                    if args[\"lstm_layers\"][\"layers\"] == \"one\":\n",
    "                        l2_3 = LSTM(args['lstm1_nodes'], input_shape=(max_len, data_dim), implementation=2, \n",
    "                                    kernel_initializer='glorot_uniform', return_sequences=False, \n",
    "                                    recurrent_dropout=args['lstm1_dropouts'], stateful = False)(main_input)\n",
    "                        b2_3 = BatchNormalization()(l2_3)\n",
    "\n",
    "                    if args[\"lstm_layers\"][\"layers\"] == \"two\":\n",
    "                        l1 = LSTM(args['lstm1_nodes'], input_shape=(max_len, data_dim), implementation=2, \n",
    "                                kernel_initializer='glorot_uniform', return_sequences=True, \n",
    "                                recurrent_dropout=args['lstm1_dropouts'], stateful = False)(main_input)\n",
    "                        b1 = BatchNormalization()(l1)\n",
    "                        l2_3 = LSTM(args[\"lstm_layers\"][\"lstm2_nodes\"], activation=\"sigmoid\", \n",
    "                                    implementation=2, kernel_initializer='glorot_uniform', return_sequences=False, \n",
    "                                    recurrent_dropout=args[\"lstm_layers\"][\"lstm2_dropouts\"], stateful = False)(b1)\n",
    "                        b2_3 = BatchNormalization()(l2_3)\n",
    "                        \n",
    "                    if args[\"lstm_layers\"][\"layers\"] == \"three\":\n",
    "                        l1 = LSTM(args['lstm1_nodes'], input_shape=(max_len, data_dim),implementation=2, \n",
    "                                kernel_initializer='glorot_uniform', return_sequences=True, \n",
    "                                recurrent_dropout=args['lstm1_dropouts'], stateful = False)(main_input)\n",
    "                        b1 = BatchNormalization()(l1)\n",
    "                        l2 = LSTM(args[\"lstm_layers\"][\"lstm2_nodes\"], activation=\"sigmoid\", \n",
    "                                    implementation=2, kernel_initializer='glorot_uniform', return_sequences=True, \n",
    "                                    recurrent_dropout=args[\"lstm_layers\"][\"lstm2_dropouts\"], stateful = False)(b1)\n",
    "                        b2 = BatchNormalization()(l2)\n",
    "                        l2_3 = LSTM(args[\"lstm_layers\"][\"lstm3_nodes\"], activation=\"sigmoid\", \n",
    "                                    implementation=2, kernel_initializer='glorot_uniform', return_sequences=False, \n",
    "                                    recurrent_dropout=args[\"lstm_layers\"][\"lstm3_dropouts\"], stateful = False)(b2)\n",
    "                        b2_3 = BatchNormalization()(l2_3)\n",
    "                    \n",
    "                    print(\"adding dense layers\")\n",
    "                    if args['dense_layers']['layers'] == \"two\":\n",
    "                        d1 = Dense(args['dense_layers']['dense2_nodes'], activation = \"relu\")(b2_3)\n",
    "                        outcome_output = Dense(2, activation='sigmoid', kernel_initializer='glorot_uniform', name='outcome_output')(d1)\n",
    "\n",
    "                    else:\n",
    "                        outcome_output = Dense(2, activation='sigmoid', kernel_initializer='glorot_uniform', name='outcome_output')(b2_3)\n",
    "                    \n",
    "                    print(\"putting together layers\")\n",
    "                    cls = Model(inputs=[main_input], outputs=[outcome_output])\n",
    "                    \n",
    "                    print(\"choosing optimiser\")\n",
    "                    if args['optimizer'] == \"adam\":\n",
    "                        opt = Nadam(lr=args['learning_rate'], beta_1=0.9, beta_2=0.999, epsilon=1e-08, schedule_decay=0.004, clipvalue=3)\n",
    "                    elif args['optimizer'] == \"rmsprop\":\n",
    "                        opt = RMSprop(lr=args['learning_rate'], rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "                        \n",
    "                    print(\"adding weights to model\")\n",
    "                    checkpoint_path = os.path.join(PATH, \"%s/%s_%s/cls/checkpoint.cpt\" % (dataset_ref, cls_method, method_name))\n",
    "                    weights = cls.load_weights(checkpoint_path)\n",
    "                    #print(weights.assert_consumed())\n",
    "                     \n",
    "                    print(\"compiling model\")\n",
    "                    cls.compile(loss='binary_crossentropy', optimizer=opt)\n",
    "                else:\n",
    "                    pipeline_path = os.path.join(PATH, \"%s/%s_%s/pipelines/pipeline_bucket_%s.joblib\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "                    feat_comb_path = os.path.join(PATH, \"%s/%s_%s/bucketers_and_encoders/feature_combiner_bucket_%s.joblib\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "                    bucketer_path = os.path.join(PATH, \"%s/%s_%s/bucketers_and_encoders/bucketer_bucket_%s.joblib\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "                    cls_path = os.path.join(PATH, \"%s/%s_%s/models/cls_bucket_%s.joblib\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "\n",
    "                    predictor = joblib.load(pipeline_path)\n",
    "                    cls = joblib.load(cls_path)\n",
    "                    feature_combiner = joblib.load(feat_comb_path)\n",
    "                    bucketer = joblib.load(bucketer_path)\n",
    "\n",
    "                #import data for bucket\n",
    "                X_train_path = os.path.join(PATH, \"%s/%s_%s/train_data/bucket_%s_prefixes.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "                Y_train_path = os.path.join(PATH, \"%s/%s_%s/train_data/bucket_%s_labels.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "                X_test_path = os.path.join(PATH, \"%s/%s_%s/test_data/bucket_%s_prefixes.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "                Y_test_path = os.path.join(PATH, \"%s/%s_%s/test_data/bucket_%s_labels.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "\n",
    "                with open (X_train_path, 'rb') as f:\n",
    "                    dt_train_bucket = pickle.load(f)\n",
    "                with open (Y_train_path, 'rb') as f:\n",
    "                    train_y = pickle.load(f)\n",
    "                with open (X_test_path, 'rb') as f:\n",
    "                    dt_test_bucket = pickle.load(f)\n",
    "                with open (Y_test_path, 'rb') as f:\n",
    "                    test_y = pickle.load(f)\n",
    "\n",
    "                #import previously identified samples\n",
    "                tn_path = os.path.join(PATH, \"%s/%s_%s/samples/true_neg_bucket_%s_.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "                tp_path = os.path.join(PATH, \"%s/%s_%s/samples/true_pos_bucket_%s_.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "                fn_path = os.path.join(PATH, \"%s/%s_%s/samples/false_neg_bucket_%s_.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "                fp_path = os.path.join(PATH, \"%s/%s_%s/samples/false_pos_bucket_%s_.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "\n",
    "                sample_instances = []\n",
    "\n",
    "                with open (tn_path, 'rb') as f:\n",
    "                    tn_list = pickle.load(f)\n",
    "                with open (tp_path, 'rb') as f:\n",
    "                    tp_list = pickle.load(f)\n",
    "                with open (fn_path, 'rb') as f:\n",
    "                    fn_list = pickle.load(f)\n",
    "                with open (fp_path, 'rb') as f:\n",
    "                    fp_list = pickle.load(f)\n",
    "\n",
    "                #save results to a list\n",
    "                sample_instances.append(tn_list)\n",
    "                sample_instances.append(tp_list)\n",
    "                sample_instances.append(fn_list)\n",
    "                sample_instances.append(fp_list)\n",
    "                \n",
    "                #break\n",
    "\n",
    "                if cls_method == \"xgboost\":\n",
    "                    tree_explainer = shap.TreeExplainer(cls)\n",
    "                    test_x = feature_combiner.fit_transform(dt_test_bucket)\n",
    "                    feat_list = feature_combiner.get_feature_names()\n",
    "                elif cls_method == \"lstm\":\n",
    "                    if len(dt_train_bucket) >10000:\n",
    "                        training_sample = shap.sample(dt_train_bucket, 10000)\n",
    "                    else:\n",
    "                        training_sample = dt_train_bucket\n",
    "                    deep_explainer = shap.DeepExplainer(cls, training_sample)\n",
    "                    test_x = dt_test_bucket\n",
    "                    feat_list_path = os.path.join(PATH, \"%s/%s_%s/cls/feature_names.pickle\" % (dataset_ref, cls_method, method_name))\n",
    "                    with open(feat_list_path, 'rb') as f:\n",
    "                        file = f.read()\n",
    "                        feat_list = np.array(pickle.loads(file))\n",
    "                #break\n",
    "                type_list = ['True Negatives', 'True Positives', 'False Negatives', 'False Positives']\n",
    "                max_feat = round(len(feat_list.flatten())*0.1)\n",
    "                #print(max_feat)\n",
    "                \n",
    "                print(\"Generating distributions for bucket\")\n",
    "                start_time = time.time()\n",
    "                if cls_method == \"lstm\":\n",
    "                    distribs = generate_distributions(deep_explainer, feat_list, test_x)\n",
    "                if cls_method == \"xgboost\":\n",
    "                    distribs = generate_distributions(tree_explainer, feat_list, test_x)\n",
    "                dist_elapsed = time.time()-start_time\n",
    "                print(\"Time taken to generate distribution:\", dist_elapsed)\n",
    "                \n",
    "                start_time = time.time()\n",
    "                for i_type in range(len(sample_instances[:1])):\n",
    "                    changes = []\n",
    "                    probas = []\n",
    "                    nr_events = []\n",
    "                    case_ids = []\n",
    "\n",
    "                    for n in range(len(sample_instances[i_type][:1])):\n",
    "                        print(\"Category %s of %s. Instance %s of %s\" %(i_type+1, len(sample_instances), n+1, len(sample_instances[i_type])))\n",
    "                        instance = sample_instances[i_type][n]\n",
    "\n",
    "                        ind = instance['predicted']\n",
    "                        case_ids.append(instance['caseID'])\n",
    "                        p1 = instance['proba']\n",
    "                        probas.append(p1)\n",
    "                        nr_events.append(instance['nr_events'])\n",
    "                        input_ = instance['input']\n",
    "\n",
    "                        if cls_method != \"lstm\":\n",
    "                            test_x_group = feature_combiner.fit_transform(input_) \n",
    "                        else:\n",
    "                            print(input_.shape)\n",
    "                            test_x_group = np.array([input_])\n",
    "                            print(test_x_group.shape)\n",
    "                        #test_x=np.transpose(test_x_group[0])\n",
    "                        #print(test_x)\n",
    "                        #print(p1)\n",
    "\n",
    "                        print(\"Creating explanations\")\n",
    "                        if cls_method == \"xgboost\":\n",
    "                            exp, rel_exp = create_samples(tree_explainer, exp_iter, test_x_group, feat_list, top = max_feat)\n",
    "                            features = []\n",
    "                            shap_vals = []\n",
    "\n",
    "                            print(\"Identifying relevant features\")\n",
    "                            for ts in rel_exp:\n",
    "                                for explanation in ts:\n",
    "                                    features.extend([feat[0] for feat in explanation])\n",
    "                                    shap_vals.extend([feat for feat in explanation])\n",
    "\n",
    "                        elif cls_method == \"lstm\":\n",
    "                            exp, rel_exp = create_samples(deep_explainer, exp_iter, test_x_group, feat_list, top = max_feat)\n",
    "                            features = []\n",
    "                            shap_vals = []\n",
    "                        \n",
    "                            print(\"Identifying relevant features\")\n",
    "                            for explanation in rel_exp:\n",
    "                                features.extend([feat[0] for feat in explanation])\n",
    "                                shap_vals.extend([feat for feat in explanation])\n",
    "\n",
    "                        counter = Counter(features).most_common(max_feat)\n",
    "\n",
    "                        feats = [feat[0] for feat in counter]\n",
    "\n",
    "                        rel_feats = []\n",
    "                        for feat in feats:\n",
    "                            vals = [i[1] for i in shap_vals if i[0] == feat]\n",
    "                            #print(feat, vals)\n",
    "                            val = np.mean(vals)\n",
    "                            rel_feats.append((feat, val))\n",
    "\n",
    "                        intervals = []\n",
    "                        for item in rel_feats:\n",
    "                            feat = item [0]\n",
    "                            val = item[1]\n",
    "\n",
    "                            print(\"Creating distribution for feature\", rel_feats.index(item)+1, \"of\", len(rel_feats))\n",
    "\n",
    "                            if type(feat_list) == list:\n",
    "                                n = feat_list.index(feat)\n",
    "                                feat_dict = distribs[n]\n",
    "                            elif type(feat_list) == np.ndarray:\n",
    "                                f_ind = int(np.where(feat_list == feat)[0])\n",
    "                                length = len(feat_list[f_ind])\n",
    "                                s_ind = int(np.where(feat_list == feat)[1])\n",
    "                                feat_dict = distribs[f_ind*length+s_ind]\n",
    "    \n",
    "                            if feat_dict['Feature Name'] != feat:\n",
    "                                for each in distribs:\n",
    "                                    if feat_dict['Feature Name'] == feat:\n",
    "                                        feat_dict = each\n",
    "\n",
    "                            bins = feat_dict['bins']\n",
    "                            mins = feat_dict['mins']\n",
    "                            maxes = feat_dict['maxes']\n",
    "                            #print (feat, val, bins, mins, maxes)\n",
    "\n",
    "                            i = 0\n",
    "                            while val > bins[i] and i < len(bins)-1:\n",
    "                                idx = i\n",
    "                                i+=1\n",
    "                            #print (i)\n",
    "                            if mins[i] != None:\n",
    "                                min_val = mins[i]\n",
    "                                max_val = maxes[i]\n",
    "                            else:\n",
    "                                j = i\n",
    "                                while mins[j] == None and j > 0:\n",
    "                                    min_val = mins[j-1]\n",
    "                                    max_val = maxes[j-1]\n",
    "                                    j = j-1\n",
    "\n",
    "                            interval = max_val - min_val\n",
    "                            \n",
    "                            if type(feat_list) == list:\n",
    "                                index = feat_list.index(feat)\n",
    "                            elif type(feat_list) == np.ndarray:\n",
    "                                index = [int(np.where(feat_list==feat)[0]),int(np.where(feat_list==feat)[1])]\n",
    "                            int_min = max_val\n",
    "                            int_max = max_val + interval\n",
    "                            intervals.append((feat, index, int_min, int_max))\n",
    "\n",
    "\n",
    "                        diffs = []\n",
    "\n",
    "                        for iteration in range(exp_iter):\n",
    "                            print(\"Pertubing - Run\", iteration+1)\n",
    "                            alt_x = np.copy(test_x_group)\n",
    "                            #print(\"original:\", alt_x)\n",
    "                            for each in intervals:\n",
    "                                new_val = random.uniform(each[2], each[3])\n",
    "                                if cls_encoding == \"3d\":\n",
    "                                    alt_x[0][each[1][0]][each[1][1]] = new_val\n",
    "                                else:\n",
    "                                    alt_x[0][each[1]] = new_val\n",
    "                            if cls_method != \"lstm\":\n",
    "                                p2 = cls.predict_proba(alt_x)[0][ind]\n",
    "                            else:\n",
    "                                p2 = cls.predict(alt_x)[0][ind]\n",
    "                            diff = p1-p2\n",
    "                            diffs.append(diff)\n",
    "\n",
    "                        changes.append(np.mean(diffs))\n",
    "                        shap_elapsed = time.time()-start_time\n",
    "                        instance['shap_fid_change'] = diffs\n",
    "                        #print(\"RMSE for instance:\", np.std(diffs))\n",
    "                        \n",
    "                        if ind == 0:\n",
    "                            pos_shap_changes.append(abs(diff))#np.std(diffs))\n",
    "                            pos_probas.append(p1)\n",
    "                            pos_nr_events.append(instance['nr_events'])\n",
    "                            pos_case_ids.append(instance['caseID'])\n",
    "                        else:\n",
    "                            neg_shap_changes.append(abs(diff))#np.std(diffs))\n",
    "                            neg_probas.append(p1)\n",
    "                            neg_nr_events.append(instance['nr_events'])\n",
    "                            neg_case_ids.append(instance['caseID'])\n",
    "\n",
    "#                     fig, ax = plt.subplots()\n",
    "#                     ax.plot(probas, changes, 'ro', label = \"SHAP\")\n",
    "#                     ax.set_xlabel(\"Prefix Length\")\n",
    "#                     ax.set_ylabel(\"Change in prediction probability\")\n",
    "#                     #ax.legend(frameon = False, bbox_to_anchor=(1, 1), loc = 'upper left')\n",
    "#                     plt.yticks(np.arange(0,1.1, 0.1))\n",
    "#                     plt.title(\"Prefix length and change in prediction probability - %s (Bucket %s)\" %(type_list[i_type], bucketID))\n",
    "#                     plt.show()\n",
    "\n",
    "#                     fig2, ax2 = plt.subplots()\n",
    "#                     ax2.plot(nr_events, changes, 'ro', label = \"SHAP\")\n",
    "#                     ax2.set_xlabel(\"Prediction Probability\")\n",
    "#                     ax2.set_ylabel(\"Change in prediction probability\")\n",
    "#                     #ax2.legend(frameon = False, bbox_to_anchor=(1, 1), loc = 'upper left')\n",
    "#                     plt.yticks(np.arange(0,1.1, 0.1))\n",
    "#                     plt.title(\"Prediction probability and change in prediction probability - %s (Bucket %s)\" %(type_list[i_type], bucketID))\n",
    "#                     plt.show()\n",
    "\n",
    "#                     all_shap_changes.extend(changes)\n",
    "#                     all_lens.extend(nr_events)\n",
    "#                     all_probas.extend(probas)\n",
    "#                     all_case_ids.extend(case_ids)\n",
    "\n",
    "                #Save dictionaries updated with scores\n",
    "                with open(tn_path, 'wb') as f:\n",
    "                    pickle.dump(sample_instances[0], f)\n",
    "                with open(tp_path, 'wb') as f:\n",
    "                    pickle.dump(sample_instances[1], f)\n",
    "                with open(fn_path, 'wb') as f:\n",
    "                    pickle.dump(sample_instances[2], f)\n",
    "                with open(fp_path, 'wb') as f:\n",
    "                    pickle.dump(sample_instances[3], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots()\n",
    "# ax.plot(all_lens, all_shap_changes, 'ro', label = \"SHAP\")\n",
    "# ax.set_xlabel(\"Prefix Length\")\n",
    "# ax.set_ylabel(\"Change in prediction probability\")\n",
    "# #ax.legend(frameon = False, bbox_to_anchor=(1, 1), loc = 'upper left')\n",
    "# plt.yticks(np.arange(0,1.1, 0.1))\n",
    "# plt.title(\"Prefix length and change in prediction probability - All\")\n",
    "# plt.show()\n",
    "\n",
    "# fig2, ax2 = plt.subplots()\n",
    "# ax2.plot(all_probas, all_shap_changes, 'ro', label = \"SHAP\")\n",
    "# ax2.set_xlabel(\"Prediction Probability\")\n",
    "# ax2.set_ylabel(\"Change in prediction probability\")\n",
    "# #ax2.legend(frameon = False, bbox_to_anchor=(1, 1), loc = 'upper left')\n",
    "# plt.yticks(np.arange(0,1.1, 0.1))\n",
    "# plt.title(\"Prediction probability and change in prediction probability - All\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig2, ax2 = plt.subplots()\n",
    "# ax2.plot(pos_probas, pos_shap_changes, 'ro', label = \"SHAP\")\n",
    "# ax2.set_xlabel(\"Prediction Probability\")\n",
    "# ax2.set_ylabel(\"Change in prediction probability\")\n",
    "# #ax2.legend(frameon = False, bbox_to_anchor=(1, 1), loc = 'upper left')\n",
    "# plt.yticks(np.arange(0,1.1, 0.1))\n",
    "# plt.title(\"Prediction probability and change in prediction probability - Positives\")\n",
    "# plt.show()\n",
    "\n",
    "# fig2, ax2 = plt.subplots()\n",
    "# ax2.plot(neg_probas, neg_shap_changes, 'ro', label = \"SHAP\")\n",
    "# ax2.set_xlabel(\"Prediction Probability\")\n",
    "# ax2.set_ylabel(\"Change in prediction probability\")\n",
    "# #ax2.legend(frameon = False, bbox_to_anchor=(1, 1), loc = 'upper left')\n",
    "# plt.yticks(np.arange(0,1.1, 0.1))\n",
    "# plt.title(\"Prediction probability and change in prediction probability - Negatives\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket all\n",
      "get everything to create model\n",
      "Parameters loaded\n",
      "defining input layer\n",
      "adding lstm layers\n",
      "adding dense layers\n",
      "putting together layers\n",
      "choosing optimiser\n",
      "adding weights to model\n",
      "compiling model\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'feature_combiner' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-0a96c56b4db2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m                 \u001b[1;31m#get the training data as a matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 149\u001b[1;33m                 \u001b[0mtrainingdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeature_combiner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdt_train_bucket\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    150\u001b[0m                 \u001b[1;31m#importance = generate_global_explanations(trainingdata,train_y, cls, feature_combiner)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'feature_combiner' is not defined"
     ]
    }
   ],
   "source": [
    "if generate_lime:\n",
    "    for dataset_name in datasets:\n",
    "\n",
    "        dataset_manager = DatasetManager(dataset_name)\n",
    "\n",
    "        for ii in range(n_iter):\n",
    "            if cls_method == \"lstm\":\n",
    "                num_buckets = range(0,1)\n",
    "            else:\n",
    "                num_buckets = range(len([name for name in os.listdir(os.path.join(PATH,'%s/%s_%s/models'% (dataset_ref, cls_method, method_name)))]))\n",
    "\n",
    "            all_lime_changes = []\n",
    "            all_lens = []\n",
    "            all_probas = []\n",
    "            all_case_ids = []\n",
    "\n",
    "            pos_lime_changes = []\n",
    "            pos_probas = []\n",
    "            pos_nr_events = []\n",
    "            pos_case_ids = []\n",
    "\n",
    "            neg_lime_changes = []\n",
    "            neg_probas = []\n",
    "            neg_nr_events = []\n",
    "            neg_case_ids = []\n",
    "\n",
    "            for bucket in list(num_buckets):\n",
    "                bucketID = \"all\"\n",
    "                print ('Bucket', bucketID)\n",
    "\n",
    "                #import everything needed to sort and predict\n",
    "                if cls_method == \"lstm\":\n",
    "                    print(\"get everything to create model\")\n",
    "                    params_path = os.path.join(PATH, \"%s/%s_%s/cls/params.pickle\" % (dataset_ref, cls_method, method_name))\n",
    "                    with open(params_path, 'rb') as f:\n",
    "                        args = pickle.load(f)\n",
    "\n",
    "                    max_len = args['max_len']\n",
    "                    data_dim = args['data_dim']\n",
    "                    print(\"Parameters loaded\")\n",
    "\n",
    "                    #create model\n",
    "                    print(\"defining input layer\")\n",
    "                    main_input = Input(shape=(max_len, data_dim), name='main_input')\n",
    "                    \n",
    "                    print(\"adding lstm layers\")\n",
    "                    if args[\"lstm_layers\"][\"layers\"] == \"one\":\n",
    "                        l2_3 = LSTM(args['lstm1_nodes'], input_shape=(max_len, data_dim), implementation=2, \n",
    "                                    kernel_initializer='glorot_uniform', return_sequences=False, \n",
    "                                    recurrent_dropout=args['lstm1_dropouts'], stateful = False)(main_input)\n",
    "                        b2_3 = BatchNormalization()(l2_3)\n",
    "\n",
    "                    if args[\"lstm_layers\"][\"layers\"] == \"two\":\n",
    "                        l1 = LSTM(args['lstm1_nodes'], input_shape=(max_len, data_dim), implementation=2, \n",
    "                                kernel_initializer='glorot_uniform', return_sequences=True, \n",
    "                                recurrent_dropout=args['lstm1_dropouts'], stateful = False)(main_input)\n",
    "                        b1 = BatchNormalization()(l1)\n",
    "                        l2_3 = LSTM(args[\"lstm_layers\"][\"lstm2_nodes\"], activation=\"sigmoid\", \n",
    "                                    implementation=2, kernel_initializer='glorot_uniform', return_sequences=False, \n",
    "                                    recurrent_dropout=args[\"lstm_layers\"][\"lstm2_dropouts\"], stateful = False)(b1)\n",
    "                        b2_3 = BatchNormalization()(l2_3)\n",
    "                        \n",
    "                    if args[\"lstm_layers\"][\"layers\"] == \"three\":\n",
    "                        l1 = LSTM(args['lstm1_nodes'], input_shape=(max_len, data_dim),implementation=2, \n",
    "                                kernel_initializer='glorot_uniform', return_sequences=True, \n",
    "                                recurrent_dropout=args['lstm1_dropouts'], stateful = False)(main_input)\n",
    "                        b1 = BatchNormalization()(l1)\n",
    "                        l2 = LSTM(args[\"lstm_layers\"][\"lstm2_nodes\"], activation=\"sigmoid\", \n",
    "                                    implementation=2, kernel_initializer='glorot_uniform', return_sequences=True, \n",
    "                                    recurrent_dropout=args[\"lstm_layers\"][\"lstm2_dropouts\"], stateful = False)(b1)\n",
    "                        b2 = BatchNormalization()(l2)\n",
    "                        l2_3 = LSTM(args[\"lstm_layers\"][\"lstm3_nodes\"], activation=\"sigmoid\", \n",
    "                                    implementation=2, kernel_initializer='glorot_uniform', return_sequences=False, \n",
    "                                    recurrent_dropout=args[\"lstm_layers\"][\"lstm3_dropouts\"], stateful = False)(b2)\n",
    "                        b2_3 = BatchNormalization()(l2_3)\n",
    "                    \n",
    "                    print(\"adding dense layers\")\n",
    "                    if args['dense_layers']['layers'] == \"two\":\n",
    "                        d1 = Dense(args['dense_layers']['dense2_nodes'], activation = \"relu\")(b2_3)\n",
    "                        outcome_output = Dense(2, activation='sigmoid', kernel_initializer='glorot_uniform', name='outcome_output')(d1)\n",
    "\n",
    "                    else:\n",
    "                        outcome_output = Dense(2, activation='sigmoid', kernel_initializer='glorot_uniform', name='outcome_output')(b2_3)\n",
    "                    \n",
    "                    print(\"putting together layers\")\n",
    "                    cls = Model(inputs=[main_input], outputs=[outcome_output])\n",
    "                    \n",
    "                    print(\"choosing optimiser\")\n",
    "                    if args['optimizer'] == \"adam\":\n",
    "                        opt = Nadam(lr=args['learning_rate'], beta_1=0.9, beta_2=0.999, epsilon=1e-08, schedule_decay=0.004, clipvalue=3)\n",
    "                    elif args['optimizer'] == \"rmsprop\":\n",
    "                        opt = RMSprop(lr=args['learning_rate'], rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "                        \n",
    "                    print(\"adding weights to model\")\n",
    "                    checkpoint_path = os.path.join(PATH, \"%s/%s_%s/cls/checkpoint.cpt\" % (dataset_ref, cls_method, method_name))\n",
    "                    weights = cls.load_weights(checkpoint_path)\n",
    "                    #print(weights.assert_consumed())\n",
    "                     \n",
    "                    print(\"compiling model\")\n",
    "                    cls.compile(loss='binary_crossentropy', optimizer=opt)\n",
    "                else:\n",
    "                    pipeline_path = os.path.join(PATH, \"%s/%s_%s/pipelines/pipeline_bucket_%s.joblib\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "                    feat_comb_path = os.path.join(PATH, \"%s/%s_%s/bucketers_and_encoders/feature_combiner_bucket_%s.joblib\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "                    bucketer_path = os.path.join(PATH, \"%s/%s_%s/bucketers_and_encoders/bucketer_bucket_%s.joblib\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "                    cls_path = os.path.join(PATH, \"%s/%s_%s/models/cls_bucket_%s.joblib\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "\n",
    "                    predictor = joblib.load(pipeline_path)\n",
    "                    cls = joblib.load(cls_path)\n",
    "                    feature_combiner = joblib.load(feat_comb_path)\n",
    "                    bucketer = joblib.load(bucketer_path)\n",
    "\n",
    "                #import data for bucket\n",
    "                X_train_path = os.path.join(PATH, \"%s/%s_%s/train_data/bucket_%s_prefixes.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "                Y_train_path = os.path.join(PATH, \"%s/%s_%s/train_data/bucket_%s_labels.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "\n",
    "                with open (X_train_path, 'rb') as f:\n",
    "                    dt_train_bucket = pickle.load(f)\n",
    "                with open (Y_train_path, 'rb') as f:\n",
    "                    train_y = pickle.load(f)\n",
    "                #with open (X_test_path, 'rb') as f:\n",
    "                #    dt_test_bucket = pickle.load(f)\n",
    "                #with open (Y_test_path, 'rb') as f:\n",
    "                #    test_y = pickle.load(f)\n",
    "\n",
    "                #import previously identified samples\n",
    "                tn_path = os.path.join(PATH, \"%s/%s_%s/samples/true_neg_bucket_%s_.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "                tp_path = os.path.join(PATH, \"%s/%s_%s/samples/true_pos_bucket_%s_.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "                fn_path = os.path.join(PATH, \"%s/%s_%s/samples/false_neg_bucket_%s_.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "                fp_path = os.path.join(PATH, \"%s/%s_%s/samples/false_pos_bucket_%s_.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "\n",
    "                sample_instances = []\n",
    "\n",
    "                with open (tn_path, 'rb') as f:\n",
    "                    tn_list = pickle.load(f)\n",
    "                with open (tp_path, 'rb') as f:\n",
    "                    tp_list = pickle.load(f)\n",
    "                with open (fn_path, 'rb') as f:\n",
    "                    fn_list = pickle.load(f)\n",
    "                with open (fp_path, 'rb') as f:\n",
    "                    fp_list = pickle.load(f)\n",
    "\n",
    "                #save results to a list\n",
    "                sample_instances.append(tn_list)\n",
    "                sample_instances.append(tp_list)\n",
    "                sample_instances.append(fn_list)\n",
    "                sample_instances.append(fp_list)\n",
    "                \n",
    "                #get the training data as a matrix\n",
    "                trainingdata = feature_combiner.fit_transform(dt_train_bucket);\n",
    "                #importance = generate_global_explanations(trainingdata,train_y, cls, feature_combiner)\n",
    "\n",
    "                feat_list = feature_combiner.get_feature_names()\n",
    "                max_feat = round(len(feat_list)*0.1)\n",
    "                class_names=['regular','deviant']# regular is 0, deviant is 1, 0 is left, 1 is right\n",
    "                lime_explainer = lime.lime_tabular.LimeTabularExplainer(trainingdata, feature_names = feat_list, \n",
    "                                                                        class_names=class_names, discretize_continuous=True)\n",
    "                type_list = ['True Negatives', 'True Positives', 'False Negatives', 'False Positives']\n",
    "\n",
    "                for i in list(range(len(sample_instances[:1]))):\n",
    "                    changes = []\n",
    "                    probas = []\n",
    "                    nr_events = []\n",
    "                    case_ids = []\n",
    "\n",
    "                    for j in list(range(len(sample_instances[i][:1]))):\n",
    "                        print(\"Category %s of %s. Instance %s of %s\" %(i+1, len(sample_instances), j+1, len(sample_instances[i])))\n",
    "                        instance = sample_instances[i][j]\n",
    "                        \n",
    "                        start_time = time.time()\n",
    "                        \n",
    "                        ind = instance['predicted']\n",
    "                        case_ids.append(instance['caseID'])\n",
    "                        p1 = instance['proba']\n",
    "                        probas.append(p1)\n",
    "                        #print(\"proba:\", p1)\n",
    "                        nr_events.append(instance['nr_events'])\n",
    "                        input_ = instance['input']\n",
    "\n",
    "                        test_x_group = feature_combiner.fit_transform(input_) \n",
    "                        test_x=np.transpose(test_x_group[0])\n",
    "                        #print(test_x)\n",
    "                        #print(p1)\n",
    "\n",
    "                        explanations = []\n",
    "                        for iteration in range(exp_iter):\n",
    "                            lime_exp = generate_lime_explanations(lime_explainer, test_x, cls, input_, max_feat = max_feat)\n",
    "                            explanation = lime_exp.as_list()\n",
    "                            explanations.extend(explanation)\n",
    "\n",
    "                        features = []\n",
    "                        for explanation in explanations:\n",
    "                            features.append(explanation[0])\n",
    "\n",
    "                        counter = Counter(features)\n",
    "                        check_dup = []\n",
    "                        for feat in feat_list:\n",
    "                            for feature in counter:\n",
    "                                if feat in feature:\n",
    "                                    check_dup.append(feat)\n",
    "\n",
    "                        dup_counter = Counter(check_dup)\n",
    "                        duplicated = [feat for feat in dup_counter if dup_counter[feat] > 1]\n",
    "\n",
    "                        for each in duplicated:\n",
    "                            dpls = []\n",
    "                            vals = []\n",
    "                            for feat in counter.keys():\n",
    "                                if each in feat:\n",
    "                                    dpls.append(feat)\n",
    "                                    vals.append(counter[feat])\n",
    "                            keepval = vals.index(max(vals))\n",
    "                            for n in range(len(dpls)):\n",
    "                                if n != keepval:\n",
    "                                    del counter[dpls[n]]\n",
    "\n",
    "                        rel_feat = counter.most_common(max_feat)\n",
    "\n",
    "                        intervals = []\n",
    "\n",
    "                        for item in rel_feat:\n",
    "                            print(\"Creating distribution for feature\", rel_feat.index(item))\n",
    "                            feat = item[0]\n",
    "                            #print(item)\n",
    "                            #print(feat)\n",
    "                            for n in range(len(feat_list)):\n",
    "                                if feat_list[n] in feat:\n",
    "                                    if (\"<\" or \"<=\") in feat and (\">\" or \">=\") in feat:\n",
    "                                        two_sided = True\n",
    "                                        parts = feat.split(' ')\n",
    "                                        l_bound = float(parts[0])\n",
    "                                        u_bound = float(parts[-1])\n",
    "                                        interval = u_bound - l_bound\n",
    "                                        new_min = u_bound\n",
    "                                        new_max = u_bound + interval\n",
    "                                    else:\n",
    "                                        two_sided = False\n",
    "                                        parts = feat.split(' ')\n",
    "                                        if parts[-2] == \"<=\" or parts[-2] == \"<\":\n",
    "                                            u_bound = float(parts[-1])\n",
    "                                            if u_bound != 0:\n",
    "                                                interval = math.ceil(u_bound*1.1)\n",
    "                                            else:\n",
    "                                                interval = 5\n",
    "                                            new_min = u_bound\n",
    "                                            new_max = u_bound + interval\n",
    "                                        elif parts[-2] == \">=\" or parts[-2] == \">\":\n",
    "                                            l_bound = float(parts[-1])\n",
    "                                            if l_bound != 0:\n",
    "                                                interval = math.ceil(l_bound*1.1)\n",
    "                                            else:\n",
    "                                                interval = 5\n",
    "                                            new_max = l_bound\n",
    "                                            new_min = l_bound - interval\n",
    "                                        else:\n",
    "                                            bound = float(parts[-1])\n",
    "                                            interval = math.ceil((bound*1.1)/2)\n",
    "                                            new_min = bound\n",
    "                                            new_max = bound+interval\n",
    "                                    feature_name = feat_list[n]\n",
    "                                    index = n\n",
    "                                    int_min = new_min\n",
    "                                    int_max = new_max\n",
    "                                    intervals.append((feature_name, index, int_min, int_max))\n",
    "\n",
    "                        diffs = []\n",
    "                        for iteration in range(exp_iter):\n",
    "                            print(\"Pertubing - Run\", iteration+1)\n",
    "                            alt_x = np.copy(test_x_group)\n",
    "                            #print(\"original:\", alt_x)\n",
    "                            for each in intervals:\n",
    "                                new_val = random.uniform(each[2], each[3])\n",
    "                                alt_x[0][each[1]] = new_val\n",
    "                            p2 = cls.predict_proba(alt_x)[0][ind]\n",
    "                            diff = p1-p2\n",
    "                            diffs.append(diff)\n",
    "\n",
    "                        changes.append(np.mean(diffs))\n",
    "                        lime_elapsed = time.time()-start_time\n",
    "                        instance['lime_fid_change'] = diffs\n",
    "                        #print(\"RMSE for instance:\", np.std(diffs))\n",
    "\n",
    "\n",
    "                        if ind == 0:\n",
    "                            pos_lime_changes.append(abs(diff))#np.std(diffs))\n",
    "                            pos_probas.append(p1)\n",
    "                            pos_nr_events.append(instance['nr_events'])\n",
    "                            pos_case_ids.append(instance['caseID'])\n",
    "                        else:\n",
    "                            neg_lime_changes.append(abs(diff))#np.std(diffs))\n",
    "                            neg_probas.append(p1)\n",
    "                            neg_nr_events.append(instance['nr_events'])\n",
    "                            neg_case_ids.append(instance['caseID'])\n",
    "\n",
    "#                     fig, ax = plt.subplots()\n",
    "#                     ax.plot(nr_events, changes, 'bo', label = \"LIME\")\n",
    "#                     ax.set_xlabel(\"Prefix Length\")\n",
    "#                     ax.set_ylabel(\"Change in prediction probability\")\n",
    "#                     #ax.legend(frameon = False, bbox_to_anchor=(1, 1), loc = 'upper left')\n",
    "#                     #plt.yticks(np.arange(0,1.1, 0.1))\n",
    "#                     plt.title(\"Prefix length and change in prediction probability - %s (Bucket %s)\" %(type_list[i], bucketID))\n",
    "#                     plt.show()\n",
    "\n",
    "#                     fig2, ax2 = plt.subplots()\n",
    "#                     ax2.plot(probas, changes, 'bo', label = \"LIME\")\n",
    "#                     ax2.set_xlabel(\"Prediction Probability\")\n",
    "#                     ax2.set_ylabel(\"Change in prediction probability\")\n",
    "#                     #ax2.legend(frameon = False, bbox_to_anchor=(1, 1), loc = 'upper left')\n",
    "#                     plt.yticks(np.arange(0,1.1, 0.1))\n",
    "#                     plt.title(\"Prediction probability and change in prediction probability - %s (Bucket %s)\" %(type_list[i], bucketID))\n",
    "#                     plt.show()\n",
    "\n",
    "                    all_lime_changes.extend(changes)\n",
    "                    all_lens.extend(nr_events)\n",
    "                    all_probas.extend(probas)\n",
    "                    all_case_ids.extend(case_ids)\n",
    "\n",
    "                #Save dictionaries updated with scores\n",
    "                with open(tn_path, 'wb') as f:\n",
    "                    pickle.dump(sample_instances[0], f)\n",
    "                with open(tp_path, 'wb') as f:\n",
    "                    pickle.dump(sample_instances[1], f)\n",
    "                with open(fn_path, 'wb') as f:\n",
    "                    pickle.dump(sample_instances[2], f)\n",
    "                with open(fp_path, 'wb') as f:\n",
    "                    pickle.dump(sample_instances[3], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHAP distribution creation: 233.3896062374115 seconds\n",
      "SHAP perturbation: 5.695953369140625 seconds\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'lime_elapsed' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-73e10bb04fac>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"SHAP distribution creation: %s seconds\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdist_elapsed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"SHAP perturbation: %s seconds\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mshap_elapsed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"LIME perturbation: %s seconds\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlime_elapsed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'lime_elapsed' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"SHAP distribution creation: %s seconds\" % (dist_elapsed))\n",
    "print(\"SHAP perturbation: %s seconds\" % (shap_elapsed))\n",
    "print(\"LIME perturbation: %s seconds\" % (lime_elapsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots()\n",
    "# ax.plot(all_lens, all_lime_changes, 'bo', label = \"LIME\")\n",
    "# ax.set_xlabel(\"Prefix Length\")\n",
    "# ax.set_ylabel(\"Change in prediction probability\")\n",
    "# #ax.legend(frameon = False, bbox_to_anchor=(1, 1), loc = 'upper left')\n",
    "# #plt.yticks(np.arange(0,1.1, 0.1))\n",
    "# plt.title(\"Prefix length and change in prediction probability - All\")\n",
    "# plt.show()\n",
    "\n",
    "# fig2, ax2 = plt.subplots()\n",
    "# ax2.plot(all_probas, all_lime_changes, 'bo', label = \"LIME\")\n",
    "# ax2.set_xlabel(\"Prediction Probability\")\n",
    "# ax2.set_ylabel(\"Change in prediction probability\")\n",
    "# #ax2.legend(frameon = False, bbox_to_anchor=(1, 1), loc = 'upper left')\n",
    "# #plt.yticks(np.arange(0,1.1, 0.1))\n",
    "# plt.title(\"Prediction probability and change in prediction probability - All\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig2, ax2 = plt.subplots()\n",
    "# ax2.plot(pos_probas, pos_lime_changes, 'bo', label = \"LIME\")\n",
    "# ax2.set_xlabel(\"Prediction Probability\")\n",
    "# ax2.set_ylabel(\"Change in prediction probability\")\n",
    "# #ax2.legend(frameon = False, bbox_to_anchor=(1, 1), loc = 'upper left')\n",
    "# plt.yticks(np.arange(0,1.1, 0.1))\n",
    "# plt.title(\"Prediction probability and change in prediction probability - Negatives\")\n",
    "# plt.show()\n",
    "\n",
    "# fig2, ax2 = plt.subplots()\n",
    "# ax2.plot(neg_probas, neg_lime_changes, 'bo', label = \"LIME\")\n",
    "# ax2.set_xlabel(\"Prediction Probability\")\n",
    "# ax2.set_ylabel(\"Change in prediction probability\")\n",
    "# #ax2.legend(frameon = False, bbox_to_anchor=(1, 1), loc = 'upper left')\n",
    "# plt.yticks(np.arange(0,1.1, 0.1))\n",
    "# plt.title(\"Prediction probability and change in prediction probability - Positives\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lime_fid_score = sum(all_lime_changes)/len(all_lime_changes)\n",
    "# shap_fid_score = sum(all_shap_changes)/len(all_shap_changes)\n",
    "\n",
    "# print(\"LIME Fidelity Score: %s \\nSHAP Fidelity Score: %s\" %(lime_fid_score, shap_fid_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lime_fid_score = sum(pos_lime_changes)/len(pos_lime_changes)\n",
    "# shap_fid_score = sum(pos_shap_changes)/len(pos_shap_changes)\n",
    "\n",
    "# print(\"Positive predictions: \\nLIME Fidelity Score: %s \\nSHAP Fidelity Score: %s\" %(lime_fid_score, shap_fid_score))\n",
    "\n",
    "# lime_fid_score = sum(neg_lime_changes)/len(neg_lime_changes)\n",
    "# shap_fid_score = sum(neg_shap_changes)/len(neg_shap_changes)\n",
    "\n",
    "# print(\"Negative predictions: \\nLIME Fidelity Score: %s \\nSHAP Fidelity Score: %s\" %(lime_fid_score, shap_fid_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = []\n",
    "arr = np.array([[1,3],[4,6]])\n",
    "lst.append(list(arr.flatten()))\n",
    "lst"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
