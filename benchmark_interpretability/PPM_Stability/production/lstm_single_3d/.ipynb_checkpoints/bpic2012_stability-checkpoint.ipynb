{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 683,
     "status": "ok",
     "timestamp": 1597835061991,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh6DP4Hn-qpp593Bc6lrayXXcdQv42KAS3zJ-Ay=s64",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "vTbNLZGY9Zjr",
    "outputId": "8f89e413-7d2c-49e0-8040-3f8cd3b3048d"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "import sys\n",
    "#PATH = '/content/drive/My Drive/PPM_Stability/'\n",
    "PATH = \"C:/Users/velmurug/Documents/Stability Experiments/benchmark_interpretability/PPM_Stability/\"\n",
    "#PATH = \"C:/Users/Mythreyi/Documents/GitHub/Stability-Experiments/benchmark_interpretability/PPM_Stability/\"\n",
    "sys.path.append(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 454
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 10776,
     "status": "ok",
     "timestamp": 1597835076495,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh6DP4Hn-qpp593Bc6lrayXXcdQv42KAS3zJ-Ay=s64",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "NOTE47wJ9v8E",
    "outputId": "616caafa-afb8-48e2-bcbb-794684297715"
   },
   "outputs": [],
   "source": [
    "#!pip install lime\n",
    "#!pip install shap\n",
    "#!pip install pandas==0.19.2\n",
    "#!pip install xgboost==1.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 14587,
     "status": "ok",
     "timestamp": 1597835083465,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh6DP4Hn-qpp593Bc6lrayXXcdQv42KAS3zJ-Ay=s64",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "-wVsWqvt9zzb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import EncoderFactory\n",
    "from DatasetManager import DatasetManager\n",
    "import BucketFactory\n",
    "import stability as st #Nogueira, Sechidis, Brown.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "from sys import argv\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import joblib\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import Dense, Embedding, Flatten, Input, LSTM\n",
    "from keras.optimizers import Nadam, RMSprop\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "from tensorflow.keras.backend import print_tensor\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.compat.v1 import disable_v2_behavior#, ConfigProto, Session\n",
    "from tensorflow.compat.v1.keras.backend import get_session\n",
    "disable_v2_behavior()\n",
    "\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "from lime import submodular_pick\n",
    "\n",
    "import shap\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11333,
     "status": "ok",
     "timestamp": 1597835083468,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh6DP4Hn-qpp593Bc6lrayXXcdQv42KAS3zJ-Ay=s64",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "NgQh__fq9_xK"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def imp_df(column_names, importances):\n",
    "        df = pd.DataFrame({'feature': column_names,\n",
    "                       'feature_importance': importances}) \\\n",
    "           .sort_values('feature_importance', ascending = False) \\\n",
    "           .reset_index(drop = True)\n",
    "        return df\n",
    "\n",
    "# plotting a feature importance dataframe (horizontal barchart)\n",
    "def var_imp_plot(imp_df, title, num_feat):\n",
    "        imp_df.columns = ['feature', 'feature_importance']\n",
    "        b= sns.barplot(x = 'feature_importance', y = 'feature', data = imp_df.head(num_feat), orient = 'h', palette=\"Blues_r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9499,
     "status": "ok",
     "timestamp": 1597835083469,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh6DP4Hn-qpp593Bc6lrayXXcdQv42KAS3zJ-Ay=s64",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "1X1WzmXc-Hoa"
   },
   "outputs": [],
   "source": [
    "def generate_global_explanations(train_X,train_Y, cls, feature_combiner):\n",
    "    \n",
    "    print(\"The number of testing instances is \",len(train_Y))\n",
    "    print(\"The total number of columns is\",train_X.shape[1]);\n",
    "    print(\"The total accuracy is \",cls.score(train_X,train_Y));\n",
    "       \n",
    "    sns.set(rc={'figure.figsize':(10,10), \"font.size\":18,\"axes.titlesize\":18,\"axes.labelsize\":18})\n",
    "    sns.set\n",
    "    feat_names = feature_combiner.get_feature_names()\n",
    "    base_imp = imp_df(feat_names, cls.feature_importances_)\n",
    "    base_imp.head(15)\n",
    "    var_imp_plot(base_imp, 'Feature importance using XGBoost', 15)\n",
    "    return base_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 827,
     "status": "ok",
     "timestamp": 1597835084306,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh6DP4Hn-qpp593Bc6lrayXXcdQv42KAS3zJ-Ay=s64",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "ss0KwacD-MaC"
   },
   "outputs": [],
   "source": [
    "from lime import submodular_pick\n",
    "def generate_lime_explanations(explainer,test_xi, cls,test_y, submod=False, test_all_data=None, max_feat = 10, lstm = False):\n",
    "    \n",
    "    #print(\"Actual value \", test_y)\n",
    "    if lstm:\n",
    "        exp = explainer.explain_instance(test_xi, cls.predict, num_features=max_feat, labels=[0,1])\n",
    "    else:\n",
    "        exp = explainer.explain_instance(test_xi, \n",
    "                                 cls.predict_proba, num_features=max_feat, labels=[0,1])\n",
    "    \n",
    "    return exp\n",
    "        \n",
    "    if submod==True:\n",
    "        sp_obj=submodular_pick.SubmodularPick(explainer, test_all_data, cls.predict_proba, \n",
    "                                      sample_size=20, num_features=num_features,num_exps_desired=4)\n",
    "        [exp.as_pyplot_figure(label=exp.available_labels()[0]) for exp in sp_obj.sp_explanations];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_samples(shap_explainer, iterations, row, features, top = None):\n",
    "    length = len(features)\n",
    "    \n",
    "    exp = []\n",
    "    rel_exp = []\n",
    "    \n",
    "    for j in range(iterations):\n",
    "        \n",
    "        shap_values = shap_explainer.shap_values(row)\n",
    "        \n",
    "        importances = []\n",
    "        \n",
    "        if type(shap_explainer) == shap.explainers.kernel.KernelExplainer:\n",
    "            for i in range(length):\n",
    "                feat = features[i]\n",
    "                shap_val = shap_values[0][i]\n",
    "                abs_val = abs(shap_values[0][i])\n",
    "                entry = (feat, shap_val, abs_val)\n",
    "                importances.append(entry)\n",
    "                \n",
    "        elif type(shap_explainer) == shap.explainers.tree.TreeExplainer:\n",
    "            for i in range(length):\n",
    "                feat = features[i]\n",
    "                shap_val = shap_values[0][i]\n",
    "                abs_val = abs(shap_values[0][i])\n",
    "                entry = (feat, shap_val, abs_val)\n",
    "                importances.append(entry)\n",
    "        \n",
    "        elif type(shap_explainer) == shap.explainers.deep.DeepExplainer:\n",
    "            for i in range(length):\n",
    "                if len(features.shape) == 2:\n",
    "                    for j in range(len(features[i])):\n",
    "                        feat = features[i][j]\n",
    "                        shap_val = shap_values[0][0][i][j]\n",
    "                        abs_val = abs(shap_values[0][0][i][j])\n",
    "                        entry = (feat, shap_val, abs_val)\n",
    "                        importances.append(entry)\n",
    "                else:\n",
    "                    feat = features[i]\n",
    "                    shap_val = shap_values[0][0][i]\n",
    "                    abs_val = abs(shap_values[0][0][i])\n",
    "                    entry = (feat, shap_val, abs_val)\n",
    "                    importances.append(entry)\n",
    "    \n",
    "        importances.sort(key=lambda tup: tup[2], reverse = True)\n",
    "        \n",
    "        exp.append(importances)\n",
    "\n",
    "        rel_feat = []\n",
    "\n",
    "        if top != None:\n",
    "            for i in range(top):\n",
    "                feat = importances[i]\n",
    "                if feat[2] > 0:\n",
    "                    rel_feat.append(feat)\n",
    "\n",
    "            rel_exp.append(rel_feat)\n",
    "        else:\n",
    "            rel_exp = exp\n",
    "        \n",
    "    return exp, rel_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dispersal(weights, features):\n",
    "    feat_len = len(features)\n",
    "    #print(feat_len)\n",
    "    weights_by_feat = []\n",
    "       \n",
    "    for i in list(range(feat_len)):\n",
    "        feat_weight = []\n",
    "        for iteration in weights:\n",
    "            feat_weight.append(iteration[i])\n",
    "        weights_by_feat.append(feat_weight)\n",
    "        \n",
    "    \n",
    "    dispersal = []\n",
    "    dispersal_no_outlier = []\n",
    "    \n",
    "    for each in weights_by_feat:\n",
    "        #print(\"Feature\", weights_by_feat.index(each)+1)\n",
    "        mean = np.mean(each)\n",
    "        std_dev = np.std(each)\n",
    "        var = std_dev**2\n",
    "        \n",
    "        if mean == 0:\n",
    "            dispersal.append(0)\n",
    "            dispersal_no_outlier.append(0)\n",
    "        #print(each)\n",
    "        else:\n",
    "            #dispersal with outliers\n",
    "            rel_var = var/abs(mean)\n",
    "            dispersal.append(rel_var)\n",
    "            \n",
    "            #dispersal without outliers - remove anything with a z-score higher\n",
    "            #than 3 (more than 3 standard deviations away from the mean)\n",
    "            rem_outlier = []\n",
    "            z_scores = stats.zscore(each)\n",
    "            #print(z_scores)\n",
    "            #print(\"New list:\")\n",
    "            for i in range(len(z_scores)):\n",
    "                #print(each[i],\":\",z_scores[i])\n",
    "                if -3 < z_scores[i] < 3:\n",
    "                    rem_outlier.append(each[i])\n",
    "                #print(rem_outlier)\n",
    "            if rem_outlier != []:\n",
    "                new_mean = np.mean(rem_outlier)\n",
    "                if new_mean == 0:\n",
    "                    dispersal_no_outlier.append(0)\n",
    "                else:\n",
    "                    new_std = np.std(rem_outlier)\n",
    "                    new_var = new_std**2\n",
    "                    new_rel_var = new_var/abs(new_mean)\n",
    "                    dispersal_no_outlier.append(new_rel_var)\n",
    "            else:\n",
    "                dispersal_no_outlier.append(rel_var)\n",
    "    #print(dispersal_no_outlier)\n",
    "    return dispersal, dispersal_no_outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 828,
     "status": "ok",
     "timestamp": 1597835084311,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh6DP4Hn-qpp593Bc6lrayXXcdQv42KAS3zJ-Ay=s64",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "SynFz2rV-arK"
   },
   "outputs": [],
   "source": [
    "#Set up dataset\n",
    "dataset_ref = \"production\"\n",
    "params_dir = PATH + \"params\"\n",
    "results_dir = \"results\"\n",
    "bucket_method = \"single\"\n",
    "cls_encoding = \"3d\"\n",
    "cls_method = \"lstm\"\n",
    "\n",
    "gap = 1\n",
    "n_iter = 1\n",
    "\n",
    "method_name = \"%s_%s\"%(bucket_method, cls_encoding)\n",
    "\n",
    "generate_samples = False\n",
    "generate_lime = True\n",
    "generate_kernel_shap = False\n",
    "generate_model_shap = True\n",
    "\n",
    "sample_size = 2\n",
    "exp_iter = 10\n",
    "max_feat = 10\n",
    "max_prefix = 20\n",
    "\n",
    "dataset_ref_to_datasets = {\n",
    "    #\"bpic2011\": [\"bpic2011_f%s\"%formula for formula in range(1,5)],\n",
    "    \"bpic2015\": [\"bpic2015_%s_f2\"%(municipality) for municipality in range(5,6)],\n",
    "    \"bpic2017\" : [\"bpic2017_accepted\"],\n",
    "    \"bpic2012\" : [\"bpic2012_accepted\"],\n",
    "    #\"insurance\": [\"insurance_activity\", \"insurance_followup\"],\n",
    "    \"sepsis_cases\": [\"sepsis_cases_1\"],# \"sepsis_cases_2\", \"sepsis_cases_4\"]\n",
    "    \"production\" : [\"production\"]\n",
    "}\n",
    "\n",
    "datasets = [dataset_ref] if dataset_ref not in dataset_ref_to_datasets else dataset_ref_to_datasets[dataset_ref]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "hYO7FF3t-wUI",
    "outputId": "bbe62c34-0eaf-440f-83c4-42f496943ed4"
   },
   "outputs": [],
   "source": [
    "if generate_samples:\n",
    "\n",
    "    for dataset_name in datasets:\n",
    "        \n",
    "        dataset_manager = DatasetManager(dataset_name)\n",
    "        \n",
    "        for ii in range(n_iter):\n",
    "            if cls_method == \"lstm\":\n",
    "                num_buckets = 1\n",
    "            else:\n",
    "                num_buckets = len([name for name in os.listdir(os.path.join(PATH,'%s/%s_%s/models'% (dataset_ref, cls_method, method_name)))])\n",
    "            \n",
    "            for bucket in range(num_buckets):\n",
    "                bucketID = \"all\"\n",
    "                print ('Bucket', bucketID)\n",
    "\n",
    "                #import everything needed to sort and predict\n",
    "                if cls_method == \"lstm\":\n",
    "                    cls_path = os.path.join(PATH, \"%s/%s_%s/cls/pred_cls.h5\" % (dataset_ref, cls_method, method_name))\n",
    "                    pred_cls = load_model(cls_path)\n",
    "                else:\n",
    "                    pipeline_path = os.path.join(PATH, \"%s/%s_%s/pipelines/pipeline_bucket_%s.joblib\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "                    feat_comb_path = os.path.join(PATH, \"%s/%s_%s/bucketers_and_encoders/feature_combiner_bucket_%s.joblib\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "                    bucketer_path = os.path.join(PATH, \"%s/%s_%s/bucketers_and_encoders/bucketer_bucket_%s.joblib\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "                    cls_path = os.path.join(PATH, \"%s/%s_%s/models/cls_bucket_%s.joblib\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "\n",
    "                    predictor = joblib.load(pipeline_path)\n",
    "                    cls = joblib.load(cls_path)\n",
    "                    feature_combiner = joblib.load(feat_comb_path)\n",
    "                    bucketer = joblib.load(bucketer_path)\n",
    "\n",
    "                #import data for bucket\n",
    "                X_train_path = os.path.join(PATH, \"%s/%s_%s/train_data/bucket_%s_prefixes.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "                Y_train_path = os.path.join(PATH, \"%s/%s_%s/train_data/bucket_%s_labels.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "                X_test_path = os.path.join(PATH, \"%s/%s_%s/test_data/bucket_%s_prefixes.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "                Y_test_path = os.path.join(PATH, \"%s/%s_%s/test_data/bucket_%s_labels.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "\n",
    "                with open (X_train_path, 'rb') as f:\n",
    "                    dt_train_bucket = pickle.load(f)\n",
    "                with open (Y_train_path, 'rb') as f:\n",
    "                    train_y = pickle.load(f)\n",
    "                with open (X_test_path, 'rb') as f:\n",
    "                    dt_test_bucket = pickle.load(f)\n",
    "                with open (Y_test_path, 'rb') as f:\n",
    "                    test_y = pickle.load(f)\n",
    "\n",
    "                #import previous results from predictions\n",
    "                tn_path = os.path.join(PATH, \"%s/%s_%s/instances/true_neg_bucket_%s_.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "                tp_path = os.path.join(PATH, \"%s/%s_%s/instances/true_pos_bucket_%s_.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "                fn_path = os.path.join(PATH, \"%s/%s_%s/instances/false_neg_bucket_%s_.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "                fp_path = os.path.join(PATH, \"%s/%s_%s/instances/false_pos_bucket_%s_.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "\n",
    "                instances = []\n",
    "\n",
    "                with open (tn_path, 'rb') as f:\n",
    "                    tn_list = pickle.load(f)\n",
    "                with open (tp_path, 'rb') as f:\n",
    "                    tp_list = pickle.load(f)\n",
    "                with open (fn_path, 'rb') as f:\n",
    "                    fn_list = pickle.load(f)\n",
    "                with open (fp_path, 'rb') as f:\n",
    "                    fp_list = pickle.load(f)\n",
    "\n",
    "                #save results to a list\n",
    "                instances.append(tn_list)\n",
    "                instances.append(tp_list)\n",
    "                instances.append(fn_list)\n",
    "                instances.append(fp_list)\n",
    "\n",
    "                #choose instances from the event log to explain, based on different prefix lengths\n",
    "                sample_instances = []\n",
    "                \n",
    "                for each in instances:\n",
    "                    prefixes = range(0, max_prefix+1, gap)\n",
    "                    \n",
    "                    sample = []\n",
    "                    for length in prefixes:\n",
    "                        #print(length)\n",
    "                        #Find instances of relevant length\n",
    "                        relevant = [d for d in each if (d['nr_events'] == length)]\n",
    "                        #Find instances of different prediction probabilities\n",
    "                        prs = [0.5, 0.6, 0.7, 0.8, 0.9, 1.1]\n",
    "                        for i in list(range(len(prs)-1)):\n",
    "                            low = prs[i]\n",
    "                            high = prs[i+1]\n",
    "                            ins = [d for d in relevant if (d['proba'] >= low) & (d['proba'] < high)]\n",
    "                            if len(ins) >= sample_size:\n",
    "                                sample.extend(random.sample(ins, k=sample_size))\n",
    "                            else:\n",
    "                                sample.extend(ins)\n",
    "                    sample = sorted(sample, key = lambda i: (i['proba'], i['nr_events']))\n",
    "                    sample_instances.append(sample)\n",
    "\n",
    "                tn_path = os.path.join(PATH, \"%s/%s_%s/samples/true_neg_bucket_%s_.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "                tp_path = os.path.join(PATH, \"%s/%s_%s/samples/true_pos_bucket_%s_.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "                fn_path = os.path.join(PATH, \"%s/%s_%s/samples/false_neg_bucket_%s_.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "                fp_path = os.path.join(PATH, \"%s/%s_%s/samples/false_pos_bucket_%s_.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "\n",
    "                with open(tn_path, 'wb') as f:\n",
    "                    pickle.dump(sample_instances[0], f)\n",
    "                with open(tp_path, 'wb') as f:\n",
    "                    pickle.dump(sample_instances[1], f)\n",
    "                with open(fn_path, 'wb') as f:\n",
    "                    pickle.dump(sample_instances[2], f)\n",
    "                with open(fp_path, 'wb') as f:\n",
    "                    pickle.dump(sample_instances[3], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------SHAP----------------------------------------------\n",
      "Bucket all\n",
      "get everything to create model\n",
      "Parameters loaded\n",
      "defining input layer\n",
      "adding lstm layers\n",
      "adding dense layers\n",
      "putting together layers\n",
      "choosing optimiser\n",
      "adding weights to model\n",
      "compiling model\n",
      "creating explainer\n",
      "Category 1 of 4 . Testing 1 of 106 .\n",
      "Computing feature presence in each iteration\n",
      "Computing feature weights in each iteration\n",
      "Stability: 1.0\n",
      "Dispersal of feature importance: 0.0\n",
      "Dispersal with no outliers: 0.0\n",
      "Time taken by SHAP: 16.72560477256775\n"
     ]
    }
   ],
   "source": [
    "#Try SHAP\n",
    "print(\"----------------------------------------------SHAP----------------------------------------------\")\n",
    "start_time = time.time()\n",
    "\n",
    "if generate_model_shap:\n",
    "    \n",
    "    for dataset_name in datasets:\n",
    "        \n",
    "        dataset_manager = DatasetManager(dataset_name)\n",
    "        #data = dataset_manager.read_dataset()\n",
    "        \n",
    "        for ii in range(n_iter):\n",
    "            if cls_method == \"lstm\":\n",
    "                num_buckets = 1\n",
    "            else:\n",
    "                num_buckets = len([name for name in os.listdir(os.path.join(PATH,'%s/%s_%s/models'% (dataset_ref, cls_method, method_name)))])\n",
    "            \n",
    "            for bucket in range(num_buckets):\n",
    "                bucketID = \"all\"\n",
    "                print ('Bucket', bucketID)\n",
    "\n",
    "                #import everything needed to sort and predict\n",
    "                if cls_method == \"lstm\":\n",
    "                    print(\"get everything to create model\")\n",
    "                    params_path = os.path.join(PATH, \"%s/%s_%s/cls/params.pickle\" % (dataset_ref, cls_method, method_name))\n",
    "                    with open(params_path, 'rb') as f:\n",
    "                        args = pickle.load(f)\n",
    "\n",
    "                    max_len = args['max_len']\n",
    "                    data_dim = args['data_dim']\n",
    "                    print(\"Parameters loaded\")\n",
    "\n",
    "                    #create model\n",
    "                    print(\"defining input layer\")\n",
    "                    main_input = Input(shape=(max_len, data_dim), name='main_input')\n",
    "                    \n",
    "                    print(\"adding lstm layers\")\n",
    "                    if args[\"lstm_layers\"][\"layers\"] == \"one\":\n",
    "                        l2_3 = LSTM(args['lstm1_nodes'], input_shape=(max_len, data_dim), implementation=2, \n",
    "                                    kernel_initializer='glorot_uniform', return_sequences=False, \n",
    "                                    recurrent_dropout=args['lstm1_dropouts'], stateful = False)(main_input)\n",
    "                        b2_3 = BatchNormalization()(l2_3)\n",
    "\n",
    "                    if args[\"lstm_layers\"][\"layers\"] == \"two\":\n",
    "                        l1 = LSTM(args['lstm1_nodes'], input_shape=(max_len, data_dim), implementation=2, \n",
    "                                kernel_initializer='glorot_uniform', return_sequences=True, \n",
    "                                recurrent_dropout=args['lstm1_dropouts'], stateful = False)(main_input)\n",
    "                        b1 = BatchNormalization()(l1)\n",
    "                        l2_3 = LSTM(args[\"lstm_layers\"][\"lstm2_nodes\"], activation=\"sigmoid\", \n",
    "                                    implementation=2, kernel_initializer='glorot_uniform', return_sequences=False, \n",
    "                                    recurrent_dropout=args[\"lstm_layers\"][\"lstm2_dropouts\"], stateful = False)(b1)\n",
    "                        b2_3 = BatchNormalization()(l2_3)\n",
    "                        \n",
    "                    if args[\"lstm_layers\"][\"layers\"] == \"three\":\n",
    "                        l1 = LSTM(args['lstm1_nodes'], input_shape=(max_len, data_dim),implementation=2, \n",
    "                                kernel_initializer='glorot_uniform', return_sequences=True, \n",
    "                                recurrent_dropout=args['lstm1_dropouts'], stateful = False)(main_input)\n",
    "                        b1 = BatchNormalization()(l1)\n",
    "                        l2 = LSTM(args[\"lstm_layers\"][\"lstm2_nodes\"], activation=\"sigmoid\", \n",
    "                                    implementation=2, kernel_initializer='glorot_uniform', return_sequences=True, \n",
    "                                    recurrent_dropout=args[\"lstm_layers\"][\"lstm2_dropouts\"], stateful = False)(b1)\n",
    "                        b2 = BatchNormalization()(l2)\n",
    "                        l2_3 = LSTM(args[\"lstm_layers\"][\"lstm3_nodes\"], activation=\"sigmoid\", \n",
    "                                    implementation=2, kernel_initializer='glorot_uniform', return_sequences=False, \n",
    "                                    recurrent_dropout=args[\"lstm_layers\"][\"lstm3_dropouts\"], stateful = False)(b2)\n",
    "                        b2_3 = BatchNormalization()(l2_3)\n",
    "                    \n",
    "                    print(\"adding dense layers\")\n",
    "                    if args['dense_layers']['layers'] == \"two\":\n",
    "                        d1 = Dense(args['dense_layers']['dense2_nodes'], activation = \"relu\")(b2_3)\n",
    "                        outcome_output = Dense(2, activation='sigmoid', kernel_initializer='glorot_uniform', name='outcome_output')(d1)\n",
    "\n",
    "                    else:\n",
    "                        outcome_output = Dense(2, activation='sigmoid', kernel_initializer='glorot_uniform', name='outcome_output')(b2_3)\n",
    "                    \n",
    "                    print(\"putting together layers\")\n",
    "                    cls = Model(inputs=[main_input], outputs=[outcome_output])\n",
    "                    \n",
    "                    print(\"choosing optimiser\")\n",
    "                    if args['optimizer'] == \"adam\":\n",
    "                        opt = Nadam(lr=args['learning_rate'], beta_1=0.9, beta_2=0.999, epsilon=1e-08, schedule_decay=0.004, clipvalue=3)\n",
    "                    elif args['optimizer'] == \"rmsprop\":\n",
    "                        opt = RMSprop(lr=args['learning_rate'], rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "                        \n",
    "                    print(\"adding weights to model\")\n",
    "                    checkpoint_path = os.path.join(PATH, \"%s/%s_%s/cls/checkpoint.cpt\" % (dataset_ref, cls_method, method_name))\n",
    "                    weights = cls.load_weights(checkpoint_path)\n",
    "                    #print(weights.assert_consumed())\n",
    "                     \n",
    "                    print(\"compiling model\")\n",
    "                    cls.compile(loss='binary_crossentropy', optimizer=opt)\n",
    "\n",
    "            #        cls_path = os.path.join(PATH, \"%s/%s_%s/cls/pred_cls.h5\" % (dataset_ref, cls_method, method_name))\n",
    "             #       pred_cls = load_model(cls_path)\n",
    "                else:\n",
    "                    pipeline_path = os.path.join(PATH, \"%s/%s_%s/pipelines/pipeline_bucket_%s.joblib\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "                    feat_comb_path = os.path.join(PATH, \"%s/%s_%s/bucketers_and_encoders/feature_combiner_bucket_%s.joblib\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "                    bucketer_path = os.path.join(PATH, \"%s/%s_%s/bucketers_and_encoders/bucketer_bucket_%s.joblib\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "                    cls_path = os.path.join(PATH, \"%s/%s_%s/models/cls_bucket_%s.joblib\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "\n",
    "                    predictor = joblib.load(pipeline_path)\n",
    "                    cls = joblib.load(cls_path)\n",
    "                    feature_combiner = joblib.load(feat_comb_path)\n",
    "                    bucketer = joblib.load(bucketer_path)\n",
    "                    \n",
    "                #import data for bucket\n",
    "                X_train_path = os.path.join(PATH, \"%s/%s_%s/train_data/bucket_%s_prefixes.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "                Y_train_path = os.path.join(PATH, \"%s/%s_%s/train_data/bucket_%s_labels.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "                \n",
    "                with open (X_train_path, 'rb') as f:\n",
    "                    dt_train_bucket = pickle.load(f)\n",
    "                with open (Y_train_path, 'rb') as f:\n",
    "                    train_y = pickle.load(f)\n",
    "                    \n",
    "                #import test set\n",
    "                X_test_path = os.path.join(PATH, \"%s/%s_%s/test_data/bucket_%s_prefixes.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "                Y_test_path = os.path.join(PATH, \"%s/%s_%s/test_data/bucket_%s_labels.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "                with open(X_test_path, 'rb') as f:\n",
    "                    dt_test_bucket = pickle.load(f)\n",
    "                with open(Y_test_path, 'rb') as f:\n",
    "                    test_y = pickle.load(f)\n",
    "\n",
    "                #import previously identified samples\n",
    "                tn_path = os.path.join(PATH, \"%s/%s_%s/samples/true_neg_bucket_%s_.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "                tp_path = os.path.join(PATH, \"%s/%s_%s/samples/true_pos_bucket_%s_.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "                fn_path = os.path.join(PATH, \"%s/%s_%s/samples/false_neg_bucket_%s_.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "                fp_path = os.path.join(PATH, \"%s/%s_%s/samples/false_pos_bucket_%s_.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "\n",
    "                sample_instances = []\n",
    "\n",
    "                with open (tn_path, 'rb') as f:\n",
    "                    tn_list = pickle.load(f)\n",
    "                with open (tp_path, 'rb') as f:\n",
    "                    tp_list = pickle.load(f)\n",
    "                with open (fn_path, 'rb') as f:\n",
    "                    fn_list = pickle.load(f)\n",
    "                with open (fp_path, 'rb') as f:\n",
    "                    fp_list = pickle.load(f)\n",
    "\n",
    "                #save results to a list\n",
    "                sample_instances.append(tn_list)\n",
    "                sample_instances.append(tp_list)\n",
    "                sample_instances.append(fn_list)\n",
    "                sample_instances.append(fp_list)\n",
    "\n",
    "                #create explainers now that can be passed later\n",
    "                if cls_method == \"xgboost\":\n",
    "                    tree_explainer = shap.TreeExplainer(cls)\n",
    "                elif cls_method == \"lstm\":\n",
    "                    print(\"creating explainer\")\n",
    "                    if len(dt_train_bucket) > 10000:\n",
    "                        training_sample = shap.sample(dt_train_bucket, 10000)\n",
    "                    else:\n",
    "                        training_sample = dt_train_bucket\n",
    "                    deep_explainer = shap.DeepExplainer(cls, training_sample)\n",
    "\n",
    "                #explain the chosen instances and find the stability score\n",
    "                cat_no = 0\n",
    "                for category in sample_instances[:1]:\n",
    "                    cat_no += 1\n",
    "                    instance_no = 0\n",
    "                    \n",
    "                    for instance in category[:1]:\n",
    "                        instance_no += 1    \n",
    "                        print(\"Category\", cat_no, \"of\", len(sample_instances), \". Testing\", instance_no, \"of\", len(category), \".\")\n",
    "\n",
    "                        group = instance['input']\n",
    "\n",
    "                        #print(group.shape,instance['actual'], instance['predicted'])\n",
    "                        if cls_method != \"lstm\":\n",
    "                            test_x_group = feature_combiner.fit_transform(group)\n",
    "                        else:\n",
    "                            test_x_group = np.array([group])\n",
    "                            \n",
    "                        if cls_method == \"lstm\":\n",
    "                            feat_list_path = os.path.join(PATH, \"%s/%s_%s/cls/feature_names.pickle\" % (dataset_ref, cls_method, method_name))\n",
    "                            with open(feat_list_path, 'rb') as f:\n",
    "                                file = f.read()\n",
    "                                feat_list = np.array(pickle.loads(file))\n",
    "                        else:\n",
    "                            feat_list = feature_combiner.get_feature_names()\n",
    "\n",
    "                        #Get Tree SHAP explanations for instance\n",
    "                        exp, rel_exp = create_samples(deep_explainer, exp_iter, test_x_group, feat_list, top = max_feat)\n",
    "\n",
    "                        feat_pres = []\n",
    "                        feat_weights = []\n",
    "\n",
    "                        print(\"Computing feature presence in each iteration\")\n",
    "                        for iteration in rel_exp:\n",
    "                            #print(\"Computing feature presence for iteration\", rel_exp.index(iteration))\n",
    "                            \n",
    "                            if cls_encoding == \"3d\":\n",
    "                                #The stability measure functions can only handle two dimensional arrays and lists\n",
    "                                presence_list = [0]*(feat_list.shape[0]*feat_list.shape[1])\n",
    "                                length = feat_list.shape[1]\n",
    "                                for i in range(len(feat_list)):\n",
    "                                    for j in range(len(feat_list[i])):\n",
    "                                        each = feat_list[i][j]\n",
    "                                        for explanation in iteration:\n",
    "                                            if each == explanation[0]:\n",
    "                                                list_idx = i*length+j\n",
    "                                                presence_list[list_idx] = 1\n",
    "                            else:\n",
    "                                presence_list = [0]*len(feat_list)\n",
    "                                list_idx = feat_list.index(each)\n",
    "                                for explanation in iteration:\n",
    "                                    if each in explanation[0]:\n",
    "                                        presence_list[list_idx] = 1\n",
    "\n",
    "                            feat_pres.append(presence_list)\n",
    "\n",
    "                        print(\"Computing feature weights in each iteration\")                            \n",
    "                        for iteration in exp:\n",
    "                            #print(\"Compiling feature weights for iteration\", exp.index(iteration))\n",
    "                            \n",
    "                            if cls_encoding == \"3d\":\n",
    "                                #The stability measure functions can only handle two dimensional arrays and lists\n",
    "                                weights = [0]*(feat_list.shape[0]*feat_list.shape[1])\n",
    "                                length = feat_list.shape[1]\n",
    "                                for i in range(len(feat_list)):\n",
    "                                    for j in range(len(feat_list[i])):\n",
    "                                        each = feat_list[i][j]\n",
    "                                        for explanation in iteration:\n",
    "                                            if each == explanation[0]:\n",
    "                                                list_idx = i*length+j\n",
    "                                                weights[list_idx] = explanation[1]\n",
    "                            else:\n",
    "                                presence_list = [0]*len(feat_list)\n",
    "                                list_idx = feat_list.index(each)\n",
    "                                for explanation in iteration:\n",
    "                                    if each in explanation[0]:\n",
    "                                        weights[list_idx] = explanation[1]\n",
    "\n",
    "                            feat_weights.append(weights)\n",
    "\n",
    "                        stability = st.getStability(feat_pres)\n",
    "                        print (\"Stability:\", round(stability,2))\n",
    "                        instance['tree_shap_stability'] = stability\n",
    "                        \n",
    "                        rel_var, second_var = dispersal(feat_weights, feat_list)\n",
    "                        avg_dispersal = np.mean(rel_var)\n",
    "                        print (\"Dispersal of feature importance:\", round(avg_dispersal, 2))\n",
    "                        instance['shap_weights_dispersal'] = rel_var\n",
    "                        adj_dispersal = np.mean(second_var)\n",
    "                        print (\"Dispersal with no outliers:\", round(adj_dispersal, 2))\n",
    "                        instance['adjusted_shap_weights_dispersal'] = second_var\n",
    "                        \n",
    "                with open(tn_path, 'wb') as f:\n",
    "                    pickle.dump(sample_instances[0], f)\n",
    "                with open(tp_path, 'wb') as f:\n",
    "                    pickle.dump(sample_instances[1], f)\n",
    "                with open(fn_path, 'wb') as f:\n",
    "                    pickle.dump(sample_instances[2], f)\n",
    "                with open(fp_path, 'wb') as f:\n",
    "                    pickle.dump(sample_instances[3], f)\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "print(\"Time taken by SHAP:\", elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------LIME----------------------------------------------\n",
      "Bucket all\n",
      "get everything to create model\n",
      "Parameters loaded\n",
      "defining input layer\n",
      "adding lstm layers\n",
      "adding dense layers\n",
      "putting together layers\n",
      "choosing optimiser\n",
      "adding weights to model\n",
      "compiling model\n",
      "Category 1 of 4 . Testing 1 of 106 .\n",
      "Run 0\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_v1.py:2070: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "Run 1\n",
      "Run 2\n",
      "Run 3\n",
      "Run 4\n",
      "Run 5\n",
      "Run 6\n",
      "Run 7\n",
      "Run 8\n",
      "Run 9\n",
      "Stability: 0.35\n",
      "Dispersal of feature importance: 0.0\n",
      "Dispersal with no outliers: 0.0\n",
      "Time taken by LIME: 93.84836554527283\n"
     ]
    }
   ],
   "source": [
    "#Try LIME\n",
    "print(\"----------------------------------------------LIME----------------------------------------------\")\n",
    "start_time = time.time()\n",
    "if generate_lime:\n",
    "    \n",
    "    for dataset_name in datasets:\n",
    "        \n",
    "        dataset_manager = DatasetManager(dataset_name)\n",
    "        \n",
    "        for ii in range(n_iter):\n",
    "            if cls_method == \"lstm\":\n",
    "                num_buckets = 1\n",
    "            else:\n",
    "                num_buckets = len([name for name in os.listdir(os.path.join(PATH,'%s/%s_%s/models'% (dataset_ref, cls_method, method_name)))])\n",
    "            \n",
    "            for bucket in range(num_buckets):\n",
    "                bucketID = \"all\"\n",
    "                print ('Bucket', bucketID)\n",
    "\n",
    "                #import everything needed to sort and predict\n",
    "                if cls_method == \"lstm\":\n",
    "                    print (\"get everything to create model\")\n",
    "                    params_path = os.path.join(PATH, \"%s/%s_%s/cls/params.pickle\" % (dataset_ref, cls_method, method_name))\n",
    "                    with open(params_path, 'rb') as f:\n",
    "                        args = pickle.load(f)\n",
    "\n",
    "                    max_len = args['max_len']\n",
    "                    data_dim = args['data_dim']\n",
    "                    print(\"Parameters loaded\")\n",
    "\n",
    "                    #create model\n",
    "                    print(\"defining input layer\")\n",
    "                    main_input = Input(shape=(max_len, data_dim), name='main_input')\n",
    "                    \n",
    "                    print(\"adding lstm layers\")\n",
    "                    if args[\"lstm_layers\"][\"layers\"] == \"one\":\n",
    "                        l2_3 = LSTM(args['lstm1_nodes'], input_shape=(max_len, data_dim), implementation=2, \n",
    "                                    kernel_initializer='glorot_uniform', return_sequences=False, \n",
    "                                    recurrent_dropout=args['lstm1_dropouts'], stateful = False)(main_input)\n",
    "                        b2_3 = BatchNormalization()(l2_3)\n",
    "\n",
    "                    if args[\"lstm_layers\"][\"layers\"] == \"two\":\n",
    "                        l1 = LSTM(args['lstm1_nodes'], input_shape=(max_len, data_dim), implementation=2, \n",
    "                                kernel_initializer='glorot_uniform', return_sequences=True, \n",
    "                                recurrent_dropout=args['lstm1_dropouts'], stateful = False)(main_input)\n",
    "                        b1 = BatchNormalization()(l1)\n",
    "                        l2_3 = LSTM(args[\"lstm_layers\"][\"lstm2_nodes\"], activation=\"sigmoid\", \n",
    "                                    implementation=2, kernel_initializer='glorot_uniform', return_sequences=False, \n",
    "                                    recurrent_dropout=args[\"lstm_layers\"][\"lstm2_dropouts\"], stateful = False)(b1)\n",
    "                        b2_3 = BatchNormalization()(l2_3)\n",
    "                        \n",
    "                    if args[\"lstm_layers\"][\"layers\"] == \"three\":\n",
    "                        l1 = LSTM(args['lstm1_nodes'], input_shape=(max_len, data_dim),implementation=2, \n",
    "                                kernel_initializer='glorot_uniform', return_sequences=True, \n",
    "                                recurrent_dropout=args['lstm1_dropouts'], stateful = False)(main_input)\n",
    "                        b1 = BatchNormalization()(l1)\n",
    "                        l2 = LSTM(args[\"lstm_layers\"][\"lstm2_nodes\"], activation=\"sigmoid\", \n",
    "                                    implementation=2, kernel_initializer='glorot_uniform', return_sequences=True, \n",
    "                                    recurrent_dropout=args[\"lstm_layers\"][\"lstm2_dropouts\"], stateful = False)(b1)\n",
    "                        b2 = BatchNormalization()(l2)\n",
    "                        l2_3 = LSTM(args[\"lstm_layers\"][\"lstm3_nodes\"], activation=\"sigmoid\", \n",
    "                                    implementation=2, kernel_initializer='glorot_uniform', return_sequences=False, \n",
    "                                    recurrent_dropout=args[\"lstm_layers\"][\"lstm3_dropouts\"], stateful = False)(b2)\n",
    "                        b2_3 = BatchNormalization()(l2_3)\n",
    "                        \n",
    "                    print(\"adding dense layers\")\n",
    "                    if args['dense_layers']['layers'] == \"two\":\n",
    "                        d1 = Dense(args['dense_layers']['dense2_nodes'], activation = \"relu\")(b2_3)\n",
    "                        outcome_output = Dense(2, activation='sigmoid', kernel_initializer='glorot_uniform', name='outcome_output')(d1)\n",
    "\n",
    "                    else:\n",
    "                        outcome_output = Dense(2, activation='sigmoid', kernel_initializer='glorot_uniform', name='outcome_output')(b2_3)\n",
    "                    \n",
    "                    print(\"putting together layers\")\n",
    "                    cls = Model(inputs=[main_input], outputs=[outcome_output])\n",
    "                    \n",
    "                    print(\"choosing optimiser\")\n",
    "                    if args['optimizer'] == \"adam\":\n",
    "                        opt = Nadam(lr=args['learning_rate'], beta_1=0.9, beta_2=0.999, epsilon=1e-08, schedule_decay=0.004, clipvalue=3)\n",
    "                    elif args['optimizer'] == \"rmsprop\":\n",
    "                        opt = RMSprop(lr=args['learning_rate'], rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "                        \n",
    "                    print(\"adding weights to model\")\n",
    "                    checkpoint_path = os.path.join(PATH, \"%s/%s_%s/cls/checkpoint.cpt\" % (dataset_ref, cls_method, method_name))\n",
    "                    weights = cls.load_weights(checkpoint_path)\n",
    "                    #print(weights.assert_consumed())\n",
    "                     \n",
    "                    print(\"compiling model\")\n",
    "                    cls.compile(loss='binary_crossentropy', optimizer=opt)\n",
    "\n",
    "                    #cls_path = os.path.join(PATH, \"%s/%s_%s/cls/pred_cls.h5\" % (dataset_ref, cls_method, method_name))\n",
    "                    #cls = load_model(cls_path)\n",
    "                else:\n",
    "                    pipeline_path = os.path.join(PATH, \"%s/%s_%s/pipelines/pipeline_bucket_%s.joblib\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "                    feat_comb_path = os.path.join(PATH, \"%s/%s_%s/bucketers_and_encoders/feature_combiner_bucket_%s.joblib\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "                    bucketer_path = os.path.join(PATH, \"%s/%s_%s/bucketers_and_encoders/bucketer_bucket_%s.joblib\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "                    cls_path = os.path.join(PATH, \"%s/%s_%s/models/cls_bucket_%s.joblib\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "\n",
    "                    predictor = joblib.load(pipeline_path)\n",
    "                    cls = joblib.load(cls_path)\n",
    "                    feature_combiner = joblib.load(feat_comb_path)\n",
    "                    bucketer = joblib.load(bucketer_path)\n",
    "                \n",
    "                #import data for bucket\n",
    "                X_train_path = os.path.join(PATH, \"%s/%s_%s/train_data/bucket_%s_prefixes.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "                Y_train_path = os.path.join(PATH, \"%s/%s_%s/train_data/bucket_%s_labels.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "                \n",
    "                with open (X_train_path, 'rb') as f:\n",
    "                    dt_train_bucket = pickle.load(f)\n",
    "                with open (Y_train_path, 'rb') as f:\n",
    "                    train_y = pickle.load(f)\n",
    "                with open (X_test_path, 'rb') as f:\n",
    "                    dt_test_bucket = pickle.load(f)\n",
    "                with open (Y_test_path, 'rb') as f:\n",
    "                    test_y = pickle.load(f)\n",
    "\n",
    "                #import previously identified samples\n",
    "                tn_path = os.path.join(PATH, \"%s/%s_%s/samples/true_neg_bucket_%s_.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "                tp_path = os.path.join(PATH, \"%s/%s_%s/samples/true_pos_bucket_%s_.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "                fn_path = os.path.join(PATH, \"%s/%s_%s/samples/false_neg_bucket_%s_.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "                fp_path = os.path.join(PATH, \"%s/%s_%s/samples/false_pos_bucket_%s_.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
    "\n",
    "                sample_instances = []\n",
    "\n",
    "                with open (tn_path, 'rb') as f:\n",
    "                    tn_list = pickle.load(f)\n",
    "                with open (tp_path, 'rb') as f:\n",
    "                    tp_list = pickle.load(f)\n",
    "                with open (fn_path, 'rb') as f:\n",
    "                    fn_list = pickle.load(f)\n",
    "                with open (fp_path, 'rb') as f:\n",
    "                    fp_list = pickle.load(f)\n",
    "\n",
    "                #save results to a list\n",
    "                sample_instances.append(tn_list)\n",
    "                sample_instances.append(tp_list)\n",
    "                sample_instances.append(fn_list)\n",
    "                sample_instances.append(fp_list)\n",
    "\n",
    "                #get the training data as a matrix\n",
    "                if cls_method == \"lstm\":\n",
    "                    trainingdata = dt_train_bucket\n",
    "                else:\n",
    "                    trainingdata = feature_combiner.fit_transform(dt_train_bucket)\n",
    "                \n",
    "                #print('Generating local Explanations for', instance['caseID'])\n",
    "                if cls_method == \"lstm\":\n",
    "                    feat_list_path = os.path.join(PATH, \"%s/%s_%s/cls/feature_names.pickle\" % (dataset_ref, cls_method, method_name))\n",
    "                    with open (feat_list_path, 'rb') as f:\n",
    "                        file = f.read()\n",
    "                        orig_list = np.array(pickle.loads(file))\n",
    "                        feat_list = orig_list[0]\n",
    "                        comparison_list = []\n",
    "                        for i in range(max_len):\n",
    "                            nl = [name+\"_t-\"+str(i) for name in feat_list]\n",
    "                            comparison_list.append(nl)\n",
    "                else:\n",
    "                    feat_list = feature_combiner.get_feature_names()\n",
    "                \n",
    "                #explain the chosen instances and find the stability score\n",
    "                cat_no = 0\n",
    "                for category in sample_instances[:1]:\n",
    "                    cat_no += 1\n",
    "                    instance_no = 0\n",
    "                    for instance in category[:1]:\n",
    "                        instance_no += 1\n",
    "                        \n",
    "                        print(\"Category\", cat_no, \"of\", len(sample_instances), \". Testing\", instance_no, \"of\", len(category), \".\")\n",
    "                        \n",
    "                        group = instance['input']\n",
    "                        \n",
    "                        #create explainer now that can be passed later\n",
    "                        class_names=['regular','deviant']# regular is 0, deviant is 1, 0 is left, 1 is right\n",
    "                        if cls_method == \"lstm\":\n",
    "                            lime_explainer = lime.lime_tabular.RecurrentTabularExplainer(trainingdata,\n",
    "                                          feature_names =feat_list,\n",
    "                                          class_names=class_names, discretize_continuous=True)\n",
    "                        else:\n",
    "                            lime_explainer = lime.lime_tabular.LimeTabularExplainer(trainingdata,\n",
    "                                          feature_names =feat_list,\n",
    "                                          class_names=class_names, discretize_continuous=True)\n",
    "\n",
    "                        #print(group.shape,instance['actual'], instance['predicted'])\n",
    "                        if cls_method != \"lstm\":\n",
    "                            test_x_group= feature_combiner.fit_transform(group) \n",
    "                            test_x=np.transpose(test_x_group[0])\n",
    "                        else:\n",
    "                            test_x = group\n",
    "    \n",
    "                        #Get lime explanations for instance\n",
    "                        feat_pres = []\n",
    "                        feat_weights = []\n",
    "\n",
    "                        for iteration in list(range(exp_iter)):\n",
    "                            print(\"Run\", iteration)\n",
    "                            \n",
    "                            lime_exp = generate_lime_explanations(lime_explainer, test_x, cls, instance['actual'], max_feat = len(feat_list), lstm = True)\n",
    "                            #print(lime_exp.as_list())\n",
    "\n",
    "                            if cls_encoding == \"3d\":\n",
    "                                #The stability measure functions can only handle two dimensional arrays and lists\n",
    "                                presence_list = [0]*(orig_list.shape[0]*orig_list.shape[1])\n",
    "                                weights = [0]*(orig_list.shape[0]*orig_list.shape[1])\n",
    "                                length = orig_list.shape[1]\n",
    "                                for i in range(len(comparison_list)):\n",
    "                                    for j in range(len(comparison_list[i])):\n",
    "                                        each = comparison_list[i][j]\n",
    "                                        for explanation in lime_exp.as_list():\n",
    "                                            if each in explanation[0]:\n",
    "                                                parts = explanation[0].split(' ')\n",
    "                                                feat_name = parts[-3].split('-')\n",
    "                                                ts = int(feat_name[-1])\n",
    "                                                list_idx = ts*length+i\n",
    "                                                weights[list_idx] = explanation[1]\n",
    "                                                if lime_exp.as_list().index(explanation) < max_feat:\n",
    "                                                    presence_list[list_idx] = 1\n",
    "\n",
    "                            else:\n",
    "                                presence_list = [0]*len(feat_list)\n",
    "                                weights = [0]*len(feat_list)\n",
    "\n",
    "                                for each in feat_list:\n",
    "                                    list_idx = feat_list.index(each)\n",
    "                                    #print (\"Feature\", list_idx)\n",
    "                                    for explanation in lime_exp.as_list():\n",
    "                                        if each in explanation[0]:\n",
    "                                            if lime_exp.as_list().index(explanation) < max_feat:\n",
    "                                                presence_list[list_idx] = 1\n",
    "                                            weights[list_idx] = explanation[1]\n",
    "\n",
    "                            feat_pres.append(presence_list)\n",
    "                            feat_weights.append(weights)\n",
    "\n",
    "                        stability = st.getStability(feat_pres)\n",
    "                        print (\"Stability:\", round(stability,2))\n",
    "                        instance['lime_stability'] = stability\n",
    "                        \n",
    "                        rel_var, second_var = dispersal(feat_weights, feat_list)\n",
    "                        avg_dispersal = np.mean(rel_var)\n",
    "                        print (\"Dispersal of feature importance:\", round(avg_dispersal, 2))\n",
    "                        instance['lime_weights_dispersal'] = rel_var\n",
    "                        adj_dispersal = np.mean(second_var)\n",
    "                        print (\"Dispersal with no outliers:\", round(adj_dispersal, 2))\n",
    "                        instance['adjusted_lime_weights_dispersal'] = second_var\n",
    "                                        \n",
    "                #Save dictionaries updated with stability scores\n",
    "                with open(tn_path, 'wb') as f:\n",
    "                    pickle.dump(sample_instances[0], f)\n",
    "                with open(tp_path, 'wb') as f:\n",
    "                    pickle.dump(sample_instances[1], f)\n",
    "                with open(fn_path, 'wb') as f:\n",
    "                    pickle.dump(sample_instances[2], f)\n",
    "                with open(fp_path, 'wb') as f:\n",
    "                    pickle.dump(sample_instances[3], f)\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "print(\"Time taken by LIME:\", elapsed)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPLd3CKoi62ghquFtMVq2SC",
   "collapsed_sections": [],
   "name": "bpic2012_lime_stability.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
