{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "timing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vTbNLZGY9Zjr",
        "outputId": "ad98f449-8962-40fd-bea2-8e585067a421"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import sys\n",
        "PATH = '/content/drive/My Drive/PPM_Stability/'\n",
        "#PATH = \"C:/Users/velmurug/Documents/Stability Experiments/benchmark_interpretability/PPM_Stability/\"\n",
        "#PATH = \"C:/Users/Mythreyi/Documents/GitHub/Stability-Experiments/benchmark_interpretability/PPM_Stability/\"\n",
        "sys.path.append(PATH)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOTE47wJ9v8E",
        "outputId": "0e69dc67-1419-4609-e176-e3e8fd1dc9c6"
      },
      "source": [
        "!pip install lime==0.2.0.1\n",
        "!pip install shap==0.35.0\n",
        "#!pip install pandas==0.19.2\n",
        "!pip install xgboost==1.0.0"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting lime==0.2.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/86/91a13127d83d793ecb50eb75e716f76e6eda809b6803c5a4ff462339789e/lime-0.2.0.1.tar.gz (275kB)\n",
            "\r\u001b[K     |█▏                              | 10kB 13.2MB/s eta 0:00:01\r\u001b[K     |██▍                             | 20kB 11.1MB/s eta 0:00:01\r\u001b[K     |███▋                            | 30kB 7.8MB/s eta 0:00:01\r\u001b[K     |████▊                           | 40kB 7.3MB/s eta 0:00:01\r\u001b[K     |██████                          | 51kB 4.3MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 61kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 71kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 81kB 5.1MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 92kB 5.6MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 102kB 5.6MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 112kB 5.6MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 122kB 5.6MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 133kB 5.6MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 143kB 5.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 153kB 5.6MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 163kB 5.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 174kB 5.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 184kB 5.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 194kB 5.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 204kB 5.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 215kB 5.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 225kB 5.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 235kB 5.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 245kB 5.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 256kB 5.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 266kB 5.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 276kB 5.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from lime==0.2.0.1) (3.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from lime==0.2.0.1) (1.18.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from lime==0.2.0.1) (1.4.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from lime==0.2.0.1) (4.41.1)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.6/dist-packages (from lime==0.2.0.1) (0.22.2.post1)\n",
            "Requirement already satisfied: scikit-image>=0.12 in /usr/local/lib/python3.6/dist-packages (from lime==0.2.0.1) (0.16.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->lime==0.2.0.1) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->lime==0.2.0.1) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->lime==0.2.0.1) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->lime==0.2.0.1) (1.3.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.18->lime==0.2.0.1) (0.17.0)\n",
            "Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.12->lime==0.2.0.1) (7.0.0)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.12->lime==0.2.0.1) (1.1.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.12->lime==0.2.0.1) (2.5)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.12->lime==0.2.0.1) (2.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.1->matplotlib->lime==0.2.0.1) (1.15.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->scikit-image>=0.12->lime==0.2.0.1) (4.4.2)\n",
            "Building wheels for collected packages: lime\n",
            "  Building wheel for lime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lime: filename=lime-0.2.0.1-cp36-none-any.whl size=283845 sha256=4ed95d988d8b53a6aeba3e695bc45009223c64ebacbb15fd0ab7c48d6d194574\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/4f/a5/0bc765457bd41378bf3ce8d17d7495369d6e7ca3b712c60c89\n",
            "Successfully built lime\n",
            "Installing collected packages: lime\n",
            "Successfully installed lime-0.2.0.1\n",
            "Collecting shap==0.35.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/77/b504e43e21a2ba543a1ac4696718beb500cfa708af2fb57cb54ce299045c/shap-0.35.0.tar.gz (273kB)\n",
            "\u001b[K     |████████████████████████████████| 276kB 6.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from shap==0.35.0) (1.18.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from shap==0.35.0) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from shap==0.35.0) (0.22.2.post1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from shap==0.35.0) (1.1.4)\n",
            "Requirement already satisfied: tqdm>4.25.0 in /usr/local/lib/python3.6/dist-packages (from shap==0.35.0) (4.41.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->shap==0.35.0) (0.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->shap==0.35.0) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->shap==0.35.0) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas->shap==0.35.0) (1.15.0)\n",
            "Building wheels for collected packages: shap\n",
            "  Building wheel for shap (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for shap: filename=shap-0.35.0-cp36-cp36m-linux_x86_64.whl size=394113 sha256=a0ac24a15e12dc179fdcaabcc5f24cec7888a507294cdc630dc2c617009cf654\n",
            "  Stored in directory: /root/.cache/pip/wheels/e7/f7/0f/b57055080cf8894906b3bd3616d2fc2bfd0b12d5161bcb24ac\n",
            "Successfully built shap\n",
            "Installing collected packages: shap\n",
            "Successfully installed shap-0.35.0\n",
            "Collecting xgboost==1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c1/e8/88d7afd859a52092e86e0f1d9fa5ed7718c73ee5ef68a71d0123019a197e/xgboost-1.0.0-py3-none-manylinux1_x86_64.whl (109.7MB)\n",
            "\u001b[K     |████████████████████████████████| 109.8MB 85kB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from xgboost==1.0.0) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from xgboost==1.0.0) (1.18.5)\n",
            "Installing collected packages: xgboost\n",
            "  Found existing installation: xgboost 0.90\n",
            "    Uninstalling xgboost-0.90:\n",
            "      Successfully uninstalled xgboost-0.90\n",
            "Successfully installed xgboost-1.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wVsWqvt9zzb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5dc50ef-5eba-4b9d-ede2-c4c23600c064"
      },
      "source": [
        "import EncoderFactory\n",
        "from DatasetManager_for_colab import DatasetManager\n",
        "import BucketFactory\n",
        "import stability as st #Nogueira, Sechidis, Brown.\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.pipeline import FeatureUnion, Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import time\n",
        "import os\n",
        "import sys\n",
        "from sys import argv\n",
        "import pickle\n",
        "from collections import defaultdict\n",
        "import random\n",
        "import joblib\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import xgboost as xgb\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "from keras.models import Sequential, Model, load_model\n",
        "from keras.layers.core import Dense, Activation, Dropout\n",
        "from keras.preprocessing import sequence\n",
        "from keras.layers import Dense, Embedding, Flatten, Input, LSTM\n",
        "from keras.optimizers import Nadam, RMSprop\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "\n",
        "from tensorflow.keras.backend import print_tensor\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.compat.v1 import disable_v2_behavior#, ConfigProto, Session\n",
        "from tensorflow.compat.v1.keras.backend import get_session\n",
        "disable_v2_behavior()\n",
        "\n",
        "import lime\n",
        "import lime.lime_tabular\n",
        "from lime import submodular_pick;\n",
        "\n",
        "import shap\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NgQh__fq9_xK"
      },
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "lime_pal = sns.diverging_palette(100, 200, s=150, as_cmap=True)\n",
        "shap_pal = sns.diverging_palette(0, 240, s=150, as_cmap=True)\n",
        "\n",
        "sns.set_style('whitegrid')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "SynFz2rV-arK",
        "outputId": "26de37b4-9690-48e8-90a1-bc6889559dc4"
      },
      "source": [
        "dataset_ref = \"bpic2012\"\n",
        "params_dir = PATH + \"params\"\n",
        "results_dir = \"results\"\n",
        "bucket_method = \"single\"\n",
        "cls_encoding = \"agg\"\n",
        "cls_method = \"xgboost\"\n",
        "\n",
        "gap = 1\n",
        "n_iter = 1\n",
        "\n",
        "method_name = \"%s_%s\"%(bucket_method, cls_encoding)\n",
        "\n",
        "generate_samples = True\n",
        "generate_lime = True\n",
        "generate_kernel_shap = False\n",
        "generate_model_shap = True\n",
        "\n",
        "sample_size = 0.25\n",
        "exp_iter = 1\n",
        "max_feat = 10\n",
        "\n",
        "dataset_ref_to_datasets = {\n",
        "    #\"bpic2011\": [\"bpic2011_f%s\"%formula for formula in range(1,5)],\n",
        "    \"bpic2015\": [\"bpic2015_%s_f2\"%(municipality) for municipality in range(5,6)],\n",
        "    \"bpic2017\" : [\"bpic2017_accepted\"],\n",
        "    \"bpic2012\" : [\"bpic2012_accepted\"],\n",
        "    #\"insurance\": [\"insurance_activity\", \"insurance_followup\"],\n",
        "    \"sepsis_cases\": [\"sepsis_cases_1\"],# \"sepsis_cases_2\", \"sepsis_cases_4\"]\n",
        "    \"production\" : [\"production\"]\n",
        "}\n",
        "\n",
        "datasets = [dataset_ref] if dataset_ref not in dataset_ref_to_datasets else dataset_ref_to_datasets[dataset_ref]\n",
        "\n",
        "datasets"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "['bpic2012_accepted']"
            ],
            "text/plain": [
              "['bpic2012_accepted']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9BPaNPbQo39O",
        "outputId": "c49d5458-664e-4912-f5bb-cc1a065d403a"
      },
      "source": [
        "  for dataset_name in datasets:\n",
        "      \n",
        "      dataset_manager = DatasetManager(dataset_name)\n",
        "      \n",
        "      for ii in range(n_iter):\n",
        "          if cls_method == \"lstm\":\n",
        "              num_buckets = 1\n",
        "          else:\n",
        "              num_buckets = len([name for name in os.listdir(os.path.join(PATH,'%s/%s_%s/models'% (dataset_ref, cls_method, method_name)))])\n",
        "\n",
        "          all_pref_len = []\n",
        "          all_feat_len = []\n",
        "          all_shap_times = []\n",
        "          all_lime_times = []\n",
        "          sep_pref_len = []\n",
        "          sep_feat_len = []\n",
        "          all_times = []\n",
        "          all_types = []\n",
        "\n",
        "          sep_data_dict = {'Prefix Length': sep_pref_len, 'Feature Vector Length': sep_feat_len, 'SHAP Running Time': all_shap_times, 'LIME Running Time': all_lime_times}\n",
        "          all_data_dict = {'Prefix Length': all_pref_len, 'Feature Vector Length': all_feat_len, 'Explainer': all_types, 'Running Time': all_times}\n",
        "          sep_timing_path = os.path.join(PATH, \"%s/%s_%s/sep_timing.csv\" % (dataset_ref, cls_method, method_name))\n",
        "          all_timing_path = os.path.join(PATH, \"%s/%s_%s/all_timing.csv\" % (dataset_ref, cls_method, method_name))\n",
        "\n",
        "\n",
        "          for bucket in range(num_buckets):\n",
        "              if cls_method == \"lstm\":\n",
        "                bucketID = \"all\"\n",
        "              else:\n",
        "                bucketID = bucket+1\n",
        "              print ('Bucket', bucketID)\n",
        "\n",
        "              if cls_method == \"lstm\":\n",
        "                  print(\"get everything to create model\")\n",
        "                  params_path = os.path.join(PATH, \"%s/%s_%s/cls/params.pickle\" % (dataset_ref, cls_method, method_name))\n",
        "                  with open(params_path, 'rb') as f:\n",
        "                      args = pickle.load(f)\n",
        "\n",
        "                  max_len = args['max_len']\n",
        "                  data_dim = args['data_dim']\n",
        "                  print(\"Parameters loaded\")\n",
        "\n",
        "                  #create model\n",
        "                  print(\"defining input layer\")\n",
        "                  main_input = Input(shape=(max_len, data_dim), name='main_input')\n",
        "                  \n",
        "                  print(\"adding lstm layers\")\n",
        "                  if args[\"lstm_layers\"][\"layers\"] == \"one\":\n",
        "                      l2_3 = LSTM(args['lstm1_nodes'], input_shape=(max_len, data_dim), implementation=2, \n",
        "                                  kernel_initializer='glorot_uniform', return_sequences=False, \n",
        "                                  recurrent_dropout=args['lstm1_dropouts'], stateful = False)(main_input)\n",
        "                      b2_3 = BatchNormalization()(l2_3)\n",
        "\n",
        "                  if args[\"lstm_layers\"][\"layers\"] == \"two\":\n",
        "                      l1 = LSTM(args['lstm1_nodes'], input_shape=(max_len, data_dim), implementation=2, \n",
        "                              kernel_initializer='glorot_uniform', return_sequences=True, \n",
        "                              recurrent_dropout=args['lstm1_dropouts'], stateful = False)(main_input)\n",
        "                      b1 = BatchNormalization()(l1)\n",
        "                      l2_3 = LSTM(args[\"lstm_layers\"][\"lstm2_nodes\"], activation=\"sigmoid\", \n",
        "                                  implementation=2, kernel_initializer='glorot_uniform', return_sequences=False, \n",
        "                                  recurrent_dropout=args[\"lstm_layers\"][\"lstm2_dropouts\"], stateful = False)(b1)\n",
        "                      b2_3 = BatchNormalization()(l2_3)\n",
        "                      \n",
        "                  if args[\"lstm_layers\"][\"layers\"] == \"three\":\n",
        "                      l1 = LSTM(args['lstm1_nodes'], input_shape=(max_len, data_dim),implementation=2, \n",
        "                              kernel_initializer='glorot_uniform', return_sequences=True, \n",
        "                              recurrent_dropout=args['lstm1_dropouts'], stateful = False)(main_input)\n",
        "                      b1 = BatchNormalization()(l1)\n",
        "                      l2 = LSTM(args[\"lstm_layers\"][\"lstm2_nodes\"], activation=\"sigmoid\", \n",
        "                                  implementation=2, kernel_initializer='glorot_uniform', return_sequences=True, \n",
        "                                  recurrent_dropout=args[\"lstm_layers\"][\"lstm2_dropouts\"], stateful = False)(b1)\n",
        "                      b2 = BatchNormalization()(l2)\n",
        "                      l2_3 = LSTM(args[\"lstm_layers\"][\"lstm3_nodes\"], activation=\"sigmoid\", \n",
        "                                  implementation=2, kernel_initializer='glorot_uniform', return_sequences=False, \n",
        "                                  recurrent_dropout=args[\"lstm_layers\"][\"lstm3_dropouts\"], stateful = False)(b2)\n",
        "                      b2_3 = BatchNormalization()(l2_3)\n",
        "                  \n",
        "                  print(\"adding dense layers\")\n",
        "                  if args['dense_layers']['layers'] == \"two\":\n",
        "                      d1 = Dense(args['dense_layers']['dense2_nodes'], activation = \"relu\")(b2_3)\n",
        "                      outcome_output = Dense(2, activation='sigmoid', kernel_initializer='glorot_uniform', name='outcome_output')(d1)\n",
        "\n",
        "                  else:\n",
        "                      outcome_output = Dense(2, activation='sigmoid', kernel_initializer='glorot_uniform', name='outcome_output')(b2_3)\n",
        "                  \n",
        "                  print(\"putting together layers\")\n",
        "                  cls = Model(inputs=[main_input], outputs=[outcome_output])\n",
        "                  \n",
        "                  print(\"choosing optimiser\")\n",
        "                  if args['optimizer'] == \"adam\":\n",
        "                      opt = Nadam(lr=args['learning_rate'], beta_1=0.9, beta_2=0.999, epsilon=1e-08, schedule_decay=0.004, clipvalue=3)\n",
        "                  elif args['optimizer'] == \"rmsprop\":\n",
        "                      opt = RMSprop(lr=args['learning_rate'], rho=0.9, epsilon=1e-08, decay=0.0)\n",
        "                      \n",
        "                  print(\"adding weights to model\")\n",
        "                  checkpoint_path = os.path.join(PATH, \"%s/%s_%s/cls/checkpoint.cpt\" % (dataset_ref, cls_method, method_name))\n",
        "                  weights = cls.load_weights(checkpoint_path)\n",
        "\n",
        "                  print(\"compiling model\")\n",
        "                  cls.compile(loss='binary_crossentropy', optimizer=opt)\n",
        "\n",
        "                  X_train_path = os.path.join(PATH, \"%s/%s_%s/train_data/bucket_%s_prefixes.pickle\" % (dataset_ref, cls_method, method_name, bucketID))                  \n",
        "                  with open (X_train_path, 'rb') as f:\n",
        "                      dt_train_bucket = pickle.load(f)\n",
        "\n",
        "                  X_test_path = os.path.join(PATH, \"%s/%s_%s/test_data/bucket_%s_prefixes.pickle\" % (dataset_ref, cls_method, method_name, bucketID))                  \n",
        "                  with open (X_test_path, 'rb') as f:\n",
        "                      dt_test_bucket = pickle.load(f)\n",
        "\n",
        "                  sizes = []\n",
        "\n",
        "                  indices = list(range(len(dt_test_bucket)))\n",
        "                  sample_indices = random.sample(indices, len(dt_test_bucket)*sample_size)\n",
        "                  dt_testing_sample = [dt_test_bucket[i] for i in indices]\n",
        "\n",
        "                  for instance in dt_testing_sample:\n",
        "                    filled = [np.any(ts != 0) for ts in instance]\n",
        "                    prefs = filled.count(True)\n",
        "                    sizes.append(prefs)\n",
        "                  \n",
        "                  feat_len = [vec.shape for vec in dt_testing_sample]\n",
        "\n",
        "                  # samples = []\n",
        "                  # tn_path = os.path.join(PATH, \"%s/%s_%s/instances/true_neg_bucket_%s_.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
        "                  # tp_path = os.path.join(PATH, \"%s/%s_%s/instances/true_pos_bucket_%s_.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
        "                  # fn_path = os.path.join(PATH, \"%s/%s_%s/instances/false_neg_bucket_%s_.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
        "                  # fp_path = os.path.join(PATH, \"%s/%s_%s/instances/false_pos_bucket_%s_.pickle\" % (dataset_ref, cls_method, method_name, bucketID))\n",
        "\n",
        "                  # with open (tn_path, 'rb') as f:\n",
        "                  #     tn_list = pickle.load(f)\n",
        "                  # with open (tp_path, 'rb') as f:\n",
        "                  #     tp_list = pickle.load(f)\n",
        "                  # with open (fn_path, 'rb') as f:\n",
        "                  #     fn_list = pickle.load(f)\n",
        "                  # with open (fp_path, 'rb') as f:\n",
        "                  #     fp_list = pickle.load(f)\n",
        "                  \n",
        "                  # sa,\n",
        "\n",
        "              else:\n",
        "                  pipeline_path = os.path.join(PATH, \"%s/%s_%s/pipelines/pipeline_bucket_%s.joblib\" % (dataset_ref, cls_method, method_name, bucketID))\n",
        "                  feat_comb_path = os.path.join(PATH, \"%s/%s_%s/bucketers_and_encoders/feature_combiner_bucket_%s.joblib\" % (dataset_ref, cls_method, method_name, bucketID))\n",
        "                  bucketer_path = os.path.join(PATH, \"%s/%s_%s/bucketers_and_encoders/bucketer_bucket_%s.joblib\" % (dataset_ref, cls_method, method_name, bucketID))\n",
        "                  cls_path = os.path.join(PATH, \"%s/%s_%s/models/cls_bucket_%s.joblib\" % (dataset_ref, cls_method, method_name, bucketID))\n",
        "\n",
        "                  predictor = joblib.load(pipeline_path)\n",
        "                  cls = joblib.load(cls_path)\n",
        "                  feature_combiner = joblib.load(feat_comb_path)\n",
        "                  bucketer = joblib.load(bucketer_path)\n",
        "\n",
        "                  #import data for bucket\n",
        "                  X_train_path = os.path.join(PATH, \"%s/%s_%s/train_data/bucket_%s_prefixes.pickle\" % (dataset_ref, cls_method, method_name, bucketID))                  \n",
        "                  with open (X_train_path, 'rb') as f:\n",
        "                      dt_train_bucket = pickle.load(f)\n",
        "\n",
        "                  X_test_path = os.path.join(PATH, \"%s/%s_%s/test_data/bucket_%s_prefixes.pickle\" % (dataset_ref, cls_method, method_name, bucketID))                  \n",
        "                  with open (X_test_path, 'rb') as f:\n",
        "                      dt_test_bucket = pickle.load(f)\n",
        "\n",
        "                  dt_test_bucket = dt_test_bucket.dropna()\n",
        "                  dt_test_sample = dt_test_bucket.sample(frac = sample_size)\n",
        "                  \n",
        "                  dt_testing_sample = feature_combiner.fit_transform(dt_test_sample)\n",
        "                  lens = dataset_manager.get_prefix_lengths(dt_test_sample)\n",
        "\n",
        "                  feat_len = [len(vec) for vec in dt_testing_sample]\n",
        "\n",
        "              #Get a list of feature names\n",
        "              if cls_method == \"lstm\":\n",
        "                feat_list_path = os.path.join(PATH, \"%s/%s_%s/cls/feature_names.pickle\" % (dataset_ref, cls_method, method_name))\n",
        "                with open (feat_list_path, 'rb') as f:\n",
        "                    file = f.read()\n",
        "                    orig_list = np.array(pickle.loads(file))\n",
        "                    feat_list = orig_list[0]\n",
        "              else:\n",
        "                feat_list = feature_combiner.get_feature_names()\n",
        "\n",
        "              #create explainers now that can be passed later\n",
        "              start_time = time.time()\n",
        "              if cls_method == \"lstm\":\n",
        "                  if len(dt_train_bucket) > 10000:\n",
        "                      training_sample = shap.sample(dt_train_bucket, 10000)\n",
        "                  else:\n",
        "                      training_sample = dt_train_bucket\n",
        "                  shap_explainer = shap.DeepExplainer(cls, training_sample)\n",
        "              else:\n",
        "                  shap_explainer = shap.TreeExplainer(cls)\n",
        "              duration = time.time() - start_time\n",
        "              \n",
        "              print(\"Time taken to create SHAP explainer:\", duration, \"seconds.\")\n",
        "\n",
        "              start_time = time.time()\n",
        "              class_names=['regular','deviant']# regular is 0, deviant is 1, 0 is left, 1 is right\n",
        "              if cls_method == \"lstm\":\n",
        "                  trainingdata = dt_train_bucket\n",
        "                  lime_explainer = lime.lime_tabular.RecurrentTabularExplainer(trainingdata,\n",
        "                                feature_names = feat_list,\n",
        "                                class_names=class_names, discretize_continuous=True)\n",
        "              else:\n",
        "                  trainingdata = feature_combiner.fit_transform(dt_train_bucket)\n",
        "                  lime_explainer = lime.lime_tabular.LimeTabularExplainer(trainingdata,\n",
        "                                feature_names = feat_list,\n",
        "                                class_names=class_names, discretize_continuous=True)\n",
        "              duration = time.time() - start_time\n",
        "              print(\"Time taken to create LIME explainer:\", duration, \"seconds.\")\n",
        "\n",
        "\n",
        "              shap_times = []\n",
        "              lime_times = []\n",
        "\n",
        "              for instance in dt_testing_sample:\n",
        "                #generate data for SHAP\n",
        "                start_time = time.time()\n",
        "                if cls_method != \"lstm\":\n",
        "                  shap_explainer.shap_values(np.array([instance]), check_additivity = False)\n",
        "                else:\n",
        "                  shap_explainer.shap_values(instance)\n",
        "                duration = time.time() - start_time\n",
        "                shap_times.append(duration)\n",
        "\n",
        "                #generate data for LIME\n",
        "                start_time = time.time()\n",
        "                if cls_method == \"lstm\":\n",
        "                  lime_explainer.explain_instance(instance, cls.predict)\n",
        "                else:\n",
        "                  lime_explainer.explain_instance(instance, cls.predict_proba)\n",
        "                duration = time.time() - start_time\n",
        "                lime_times.append(duration)\n",
        "\n",
        "              sep_pref_len.extend(list(lens))\n",
        "              sep_feat_len.extend(list(feat_len))\n",
        "              all_shap_times.extend(shap_times)\n",
        "              all_lime_times.extend(lime_times)\n",
        "\n",
        "              for i in range(2):\n",
        "                all_pref_len.extend(sep_pref_len)\n",
        "                all_feat_len.extend(sep_feat_len)\n",
        "              all_times.extend(all_shap_times)\n",
        "              all_times.extend(all_lime_times)\n",
        "              all_types.extend([\"SHAP\"]*len(all_shap_times))\n",
        "              all_types.extend([\"LIME\"]*len(all_lime_times))\n",
        "\n",
        "          sep_data = pd.DataFrame(data = sep_data_dict)\n",
        "          sep_data.to_csv(sep_timing_path, index = False)\n",
        "\n",
        "          all_data = pd.DataFrame(data = all_data_dict)\n",
        "          all_data.to_csv(all_timing_path, index = False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Bucket 1\n",
            "Time taken to create SHAP explainer: 7.142890214920044 seconds.\n",
            "Time taken to create LIME explainer: 15.091449737548828 seconds.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tuJCUX69yiAO"
      },
      "source": [
        "fig, ax = plt.subplots()\n",
        "ax.plot(sep_data[\"Prefix Length\"], sep_data[\"SHAP Running Time\"], 'ro')#, y = ['SHAP Running Time', 'LIME Running Time'], x = ['Prefix Length'] )#, hue = data['Feature Vector Length'])\n",
        "ax.plot(sep_data[\"Prefix Length\"], sep_data[\"LIME Running Time\"], 'bo')#, y = ['SHAP Running Time', 'LIME Running Time'], x = ['Prefix Length'] )#, hue = data['Feature Vector Length'])\n",
        "#grid.map(sns.scatterplot, color=\".3\")\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UORiSY7W90Ow"
      },
      "source": [
        "sns.scatterplot(x = all_data['Prefix Length'], y = all_data['Running Time'], hue = all_data['Explainer'], size = all_data['Feature Vector Length'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}