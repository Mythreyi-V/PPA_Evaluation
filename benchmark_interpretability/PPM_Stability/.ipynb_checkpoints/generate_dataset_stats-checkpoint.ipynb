{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "import sys\n",
    "#PATH = '/content/drive/My Drive/PPM_Stability/'\n",
    "#PATH = \"C:/Users/velmurug/Documents/Stability Experiments/benchmark_interpretability/PPM_Stability/\"\n",
    "#PATH = \"C:/Users/Mythreyi/Documents/GitHub/Stability-Experiments/benchmark_interpretability/PPM_Stability/\"\n",
    "#PATH = \"C:/Users/mythr/Documents/GitHub/Stability-Experiments/benchmark_interpretability/PPM_Stability/\"\n",
    "PATH = \"C:/Users/n9455647/Documents/GitHub/Stability-Experiments/benchmark_interpretability/PPM_Stability/\"\n",
    "sys.path.append(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from DatasetManager import DatasetManager\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from sys import argv\n",
    "import pickle\n",
    "import random\n",
    "import joblib\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "#import tensorflow as tf\n",
    "#tf.keras.compat.disable_v2_backend()\n",
    "from tensorflow.keras.backend import print_tensor\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Dense, Embedding, Flatten, Input\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import Nadam, RMSprop\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import statistics\n",
    "import math\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "production\n",
      "xgboost_single_agg\n",
      "Bucket 1\n",
      "\tPositives in training set: 0.5879360465116279\n",
      "\tPositives in testing set: 0.4598155467720685\n",
      "Training instances: 1376\n",
      "Testing instances: 759\n",
      "Feature Vector Length: (162,)\n",
      "xgboost_prefix_agg\n",
      "Bucket 1\n",
      "\tPositives in training set: 0.5984848484848485\n",
      "\tPositives in testing set: 0.4318181818181818\n",
      "Training instances: 132\n",
      "Testing instances: 88\n",
      "Feature Vector Length: (146,)\n",
      "Bucket 2\n",
      "\tPositives in training set: 0.6031746031746031\n",
      "\tPositives in testing set: 0.4523809523809524\n",
      "Training instances: 126\n",
      "Testing instances: 84\n",
      "Feature Vector Length: (153,)\n",
      "Bucket 3\n",
      "\tPositives in training set: 0.6083333333333333\n",
      "\tPositives in testing set: 0.46153846153846156\n",
      "Training instances: 120\n",
      "Testing instances: 78\n",
      "Feature Vector Length: (150,)\n",
      "Bucket 4\n",
      "\tPositives in training set: 0.5909090909090909\n",
      "\tPositives in testing set: 0.4411764705882353\n",
      "Training instances: 110\n",
      "Testing instances: 68\n",
      "Feature Vector Length: (150,)\n",
      "Bucket 5\n",
      "\tPositives in training set: 0.5555555555555556\n",
      "\tPositives in testing set: 0.4576271186440678\n",
      "Training instances: 99\n",
      "Testing instances: 59\n",
      "Feature Vector Length: (150,)\n",
      "Bucket 6\n",
      "\tPositives in training set: 0.550561797752809\n",
      "\tPositives in testing set: 0.4444444444444444\n",
      "Training instances: 89\n",
      "Testing instances: 54\n",
      "Feature Vector Length: (156,)\n",
      "Bucket 7\n",
      "\tPositives in training set: 0.5411764705882353\n",
      "\tPositives in testing set: 0.43478260869565216\n",
      "Training instances: 85\n",
      "Testing instances: 46\n",
      "Feature Vector Length: (154,)\n",
      "Bucket 8\n",
      "\tPositives in training set: 0.5512820512820513\n",
      "\tPositives in testing set: 0.4318181818181818\n",
      "Training instances: 78\n",
      "Testing instances: 44\n",
      "Feature Vector Length: (151,)\n",
      "Bucket 9\n",
      "\tPositives in training set: 0.5416666666666666\n",
      "\tPositives in testing set: 0.4594594594594595\n",
      "Training instances: 72\n",
      "Testing instances: 37\n",
      "Feature Vector Length: (154,)\n",
      "Bucket 10\n",
      "\tPositives in training set: 0.5606060606060606\n",
      "\tPositives in testing set: 0.45161290322580644\n",
      "Training instances: 66\n",
      "Testing instances: 31\n",
      "Feature Vector Length: (154,)\n",
      "Bucket 11\n",
      "\tPositives in training set: 0.5666666666666667\n",
      "\tPositives in testing set: 0.5\n",
      "Training instances: 60\n",
      "Testing instances: 28\n",
      "Feature Vector Length: (148,)\n",
      "Bucket 12\n",
      "\tPositives in training set: 0.6037735849056604\n",
      "\tPositives in testing set: 0.5\n",
      "Training instances: 53\n",
      "Testing instances: 26\n",
      "Feature Vector Length: (148,)\n",
      "Bucket 13\n",
      "\tPositives in training set: 0.625\n",
      "\tPositives in testing set: 0.4782608695652174\n",
      "Training instances: 48\n",
      "Testing instances: 23\n",
      "Feature Vector Length: (148,)\n",
      "Bucket 14\n",
      "\tPositives in training set: 0.6341463414634146\n",
      "\tPositives in testing set: 0.5555555555555556\n",
      "Training instances: 41\n",
      "Testing instances: 18\n",
      "Feature Vector Length: (147,)\n",
      "Bucket 15\n",
      "\tPositives in training set: 0.6388888888888888\n",
      "\tPositives in testing set: 0.5625\n",
      "Training instances: 36\n",
      "Testing instances: 16\n",
      "Feature Vector Length: (143,)\n",
      "Bucket 16\n",
      "\tPositives in training set: 0.6551724137931034\n",
      "\tPositives in testing set: 0.5384615384615384\n",
      "Training instances: 29\n",
      "Testing instances: 13\n",
      "Feature Vector Length: (137,)\n",
      "Bucket 17\n",
      "\tPositives in training set: 0.64\n",
      "\tPositives in testing set: 0.5\n",
      "Training instances: 25\n",
      "Testing instances: 10\n",
      "Feature Vector Length: (137,)\n",
      "Bucket 18\n",
      "\tPositives in training set: 0.6086956521739131\n",
      "\tPositives in testing set: 0.5\n",
      "Training instances: 23\n",
      "Testing instances: 8\n",
      "Feature Vector Length: (142,)\n",
      "Bucket 19\n",
      "\tPositives in training set: 0.6086956521739131\n",
      "\tPositives in testing set: 0.5\n",
      "Training instances: 23\n",
      "Testing instances: 8\n",
      "Feature Vector Length: (139,)\n",
      "Bucket 20\n",
      "\tPositives in training set: 0.631578947368421\n",
      "\tPositives in testing set: 0.42857142857142855\n",
      "Training instances: 19\n",
      "Testing instances: 7\n",
      "Feature Vector Length: (139,)\n",
      "Bucket 21\n",
      "\tPositives in training set: 0.625\n",
      "\tPositives in testing set: 0.4\n",
      "Training instances: 16\n",
      "Testing instances: 5\n",
      "Feature Vector Length: (129,)\n",
      "Bucket 22\n",
      "\tPositives in training set: 0.6428571428571429\n",
      "\tPositives in testing set: 0.5\n",
      "Training instances: 14\n",
      "Testing instances: 4\n",
      "Feature Vector Length: (132,)\n",
      "Bucket 23\n",
      "\tPositives in training set: 0.6666666666666666\n",
      "\tPositives in testing set: 0.5\n",
      "Training instances: 12\n",
      "Testing instances: 4\n",
      "Feature Vector Length: (121,)\n",
      "xgboost_prefix_index\n",
      "Bucket 1\n",
      "\tPositives in training set: 0.5984848484848485\n",
      "\tPositives in testing set: 0.4318181818181818\n",
      "Training instances: 132\n",
      "Testing instances: 88\n",
      "Feature Vector Length: (100,)\n",
      "Bucket 2\n",
      "\tPositives in training set: 0.6031746031746031\n",
      "\tPositives in testing set: 0.4523809523809524\n",
      "Training instances: 126\n",
      "Testing instances: 84\n",
      "Feature Vector Length: (173,)\n",
      "Bucket 3\n",
      "\tPositives in training set: 0.6083333333333333\n",
      "\tPositives in testing set: 0.46153846153846156\n",
      "Training instances: 120\n",
      "Testing instances: 78\n",
      "Feature Vector Length: (250,)\n",
      "Bucket 4\n",
      "\tPositives in training set: 0.5909090909090909\n",
      "\tPositives in testing set: 0.4411764705882353\n",
      "Training instances: 110\n",
      "Testing instances: 68\n",
      "Feature Vector Length: (324,)\n",
      "Bucket 5\n",
      "\tPositives in training set: 0.5555555555555556\n",
      "\tPositives in testing set: 0.4576271186440678\n",
      "Training instances: 99\n",
      "Testing instances: 59\n",
      "Feature Vector Length: (397,)\n",
      "Bucket 6\n",
      "\tPositives in training set: 0.550561797752809\n",
      "\tPositives in testing set: 0.4444444444444444\n",
      "Training instances: 89\n",
      "Testing instances: 54\n",
      "Feature Vector Length: (470,)\n",
      "Bucket 7\n",
      "\tPositives in training set: 0.5411764705882353\n",
      "\tPositives in testing set: 0.43478260869565216\n",
      "Training instances: 85\n",
      "Testing instances: 46\n",
      "Feature Vector Length: (521,)\n",
      "Bucket 8\n",
      "\tPositives in training set: 0.5512820512820513\n",
      "\tPositives in testing set: 0.4318181818181818\n",
      "Training instances: 78\n",
      "Testing instances: 44\n",
      "Feature Vector Length: (593,)\n",
      "Bucket 9\n",
      "\tPositives in training set: 0.5416666666666666\n",
      "\tPositives in testing set: 0.4594594594594595\n",
      "Training instances: 72\n",
      "Testing instances: 37\n",
      "Feature Vector Length: (623,)\n",
      "Bucket 10\n",
      "\tPositives in training set: 0.5606060606060606\n",
      "\tPositives in testing set: 0.45161290322580644\n",
      "Training instances: 66\n",
      "Testing instances: 31\n",
      "Feature Vector Length: (684,)\n",
      "Bucket 11\n",
      "\tPositives in training set: 0.5666666666666667\n",
      "\tPositives in testing set: 0.5\n",
      "Training instances: 60\n",
      "Testing instances: 28\n",
      "Feature Vector Length: (714,)\n",
      "Bucket 12\n",
      "\tPositives in training set: 0.6037735849056604\n",
      "\tPositives in testing set: 0.5\n",
      "Training instances: 53\n",
      "Testing instances: 26\n",
      "Feature Vector Length: (751,)\n",
      "Bucket 13\n",
      "\tPositives in training set: 0.625\n",
      "\tPositives in testing set: 0.4782608695652174\n",
      "Training instances: 48\n",
      "Testing instances: 23\n",
      "Feature Vector Length: (810,)\n",
      "Bucket 14\n",
      "\tPositives in training set: 0.6341463414634146\n",
      "\tPositives in testing set: 0.5555555555555556\n",
      "Training instances: 41\n",
      "Testing instances: 18\n",
      "Feature Vector Length: (808,)\n",
      "Bucket 15\n",
      "\tPositives in training set: 0.6388888888888888\n",
      "\tPositives in testing set: 0.5625\n",
      "Training instances: 36\n",
      "Testing instances: 16\n",
      "Feature Vector Length: (779,)\n",
      "Bucket 16\n",
      "\tPositives in training set: 0.6551724137931034\n",
      "\tPositives in testing set: 0.5384615384615384\n",
      "Training instances: 29\n",
      "Testing instances: 13\n",
      "Feature Vector Length: (794,)\n",
      "Bucket 17\n",
      "\tPositives in training set: 0.64\n",
      "\tPositives in testing set: 0.5\n",
      "Training instances: 25\n",
      "Testing instances: 10\n",
      "Feature Vector Length: (796,)\n",
      "Bucket 18\n",
      "\tPositives in training set: 0.6086956521739131\n",
      "\tPositives in testing set: 0.5\n",
      "Training instances: 23\n",
      "Testing instances: 8\n",
      "Feature Vector Length: (861,)\n",
      "Bucket 19\n",
      "\tPositives in training set: 0.6086956521739131\n",
      "\tPositives in testing set: 0.5\n",
      "Training instances: 23\n",
      "Testing instances: 8\n",
      "Feature Vector Length: (890,)\n",
      "Bucket 20\n",
      "\tPositives in training set: 0.631578947368421\n",
      "\tPositives in testing set: 0.42857142857142855\n",
      "Training instances: 19\n",
      "Testing instances: 7\n",
      "Feature Vector Length: (844,)\n",
      "Bucket 21\n",
      "\tPositives in training set: 0.625\n",
      "\tPositives in testing set: 0.4\n",
      "Training instances: 16\n",
      "Testing instances: 5\n",
      "Feature Vector Length: (817,)\n",
      "Bucket 22\n",
      "\tPositives in training set: 0.6428571428571429\n",
      "\tPositives in testing set: 0.5\n",
      "Training instances: 14\n",
      "Testing instances: 4\n",
      "Feature Vector Length: (818,)\n",
      "Bucket 23\n",
      "\tPositives in training set: 0.6666666666666666\n",
      "\tPositives in testing set: 0.5\n",
      "Training instances: 12\n",
      "Testing instances: 4\n",
      "Feature Vector Length: (762,)\n",
      "lstm_single_3d\n",
      "Bucket all\n",
      "\tPositives in training set: 0.6344621513944223\n",
      "\tPositives in testing set: 0.4598155467720685\n",
      "Training instances: 1004\n",
      "Testing instances: 759\n",
      "Feature Vector Length: (23, 114)\n",
      "sepsis_cases\n",
      "xgboost_single_agg\n",
      "Bucket 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tPositives in training set: 0.15952702001867025\n",
      "\tPositives in testing set: 0.10870437224227839\n",
      "Training instances: 9641\n",
      "Testing instances: 2493\n",
      "Feature Vector Length: (274,)\n",
      "xgboost_prefix_agg\n",
      "Bucket 1\n",
      "\tPositives in training set: 0.1488\n",
      "\tPositives in testing set: 0.11464968152866242\n",
      "Training instances: 625\n",
      "Testing instances: 157\n",
      "Feature Vector Length: (192,)\n",
      "Bucket 2\n",
      "\tPositives in training set: 0.1488\n",
      "\tPositives in testing set: 0.11464968152866242\n",
      "Training instances: 625\n",
      "Testing instances: 157\n",
      "Feature Vector Length: (191,)\n",
      "Bucket 3\n",
      "\tPositives in training set: 0.1488\n",
      "\tPositives in testing set: 0.11464968152866242\n",
      "Training instances: 625\n",
      "Testing instances: 157\n",
      "Feature Vector Length: (197,)\n",
      "Bucket 4\n",
      "\tPositives in training set: 0.1488\n",
      "\tPositives in testing set: 0.11464968152866242\n",
      "Training instances: 625\n",
      "Testing instances: 157\n",
      "Feature Vector Length: (190,)\n",
      "Bucket 5\n",
      "\tPositives in training set: 0.1488\n",
      "\tPositives in testing set: 0.11464968152866242\n",
      "Training instances: 625\n",
      "Testing instances: 157\n",
      "Feature Vector Length: (195,)\n",
      "Bucket 6\n",
      "\tPositives in training set: 0.1492776886035313\n",
      "\tPositives in testing set: 0.11464968152866242\n",
      "Training instances: 623\n",
      "Testing instances: 157\n",
      "Feature Vector Length: (199,)\n",
      "Bucket 7\n",
      "\tPositives in training set: 0.15\n",
      "\tPositives in testing set: 0.11464968152866242\n",
      "Training instances: 620\n",
      "Testing instances: 157\n",
      "Feature Vector Length: (207,)\n",
      "Bucket 8\n",
      "\tPositives in training set: 0.15016501650165018\n",
      "\tPositives in testing set: 0.11464968152866242\n",
      "Training instances: 606\n",
      "Testing instances: 157\n",
      "Feature Vector Length: (218,)\n",
      "Bucket 9\n",
      "\tPositives in training set: 0.15228426395939088\n",
      "\tPositives in testing set: 0.11688311688311688\n",
      "Training instances: 591\n",
      "Testing instances: 154\n",
      "Feature Vector Length: (214,)\n",
      "Bucket 10\n",
      "\tPositives in training set: 0.15424610051993068\n",
      "\tPositives in testing set: 0.11842105263157894\n",
      "Training instances: 577\n",
      "Testing instances: 152\n",
      "Feature Vector Length: (212,)\n",
      "Bucket 11\n",
      "\tPositives in training set: 0.16261682242990655\n",
      "\tPositives in testing set: 0.11347517730496454\n",
      "Training instances: 535\n",
      "Testing instances: 141\n",
      "Feature Vector Length: (213,)\n",
      "Bucket 12\n",
      "\tPositives in training set: 0.1623246492985972\n",
      "\tPositives in testing set: 0.10852713178294573\n",
      "Training instances: 499\n",
      "Testing instances: 129\n",
      "Feature Vector Length: (206,)\n",
      "Bucket 13\n",
      "\tPositives in training set: 0.17067307692307693\n",
      "\tPositives in testing set: 0.09174311926605505\n",
      "Training instances: 416\n",
      "Testing instances: 109\n",
      "Feature Vector Length: (208,)\n",
      "Bucket 14\n",
      "\tPositives in training set: 0.18156424581005587\n",
      "\tPositives in testing set: 0.08888888888888889\n",
      "Training instances: 358\n",
      "Testing instances: 90\n",
      "Feature Vector Length: (202,)\n",
      "Bucket 15\n",
      "\tPositives in training set: 0.20141342756183744\n",
      "\tPositives in testing set: 0.10256410256410256\n",
      "Training instances: 283\n",
      "Testing instances: 78\n",
      "Feature Vector Length: (201,)\n",
      "Bucket 16\n",
      "\tPositives in training set: 0.19736842105263158\n",
      "\tPositives in testing set: 0.1\n",
      "Training instances: 228\n",
      "Testing instances: 60\n",
      "Feature Vector Length: (195,)\n",
      "Bucket 17\n",
      "\tPositives in training set: 0.18333333333333332\n",
      "\tPositives in testing set: 0.10204081632653061\n",
      "Training instances: 180\n",
      "Testing instances: 49\n",
      "Feature Vector Length: (184,)\n",
      "Bucket 18\n",
      "\tPositives in training set: 0.16447368421052633\n",
      "\tPositives in testing set: 0.05263157894736842\n",
      "Training instances: 152\n",
      "Testing instances: 38\n",
      "Feature Vector Length: (180,)\n",
      "Bucket 19\n",
      "\tPositives in training set: 0.16541353383458646\n",
      "\tPositives in testing set: 0.05555555555555555\n",
      "Training instances: 133\n",
      "Testing instances: 36\n",
      "Feature Vector Length: (172,)\n",
      "Bucket 20\n",
      "\tPositives in training set: 0.18181818181818182\n",
      "\tPositives in testing set: 0.0625\n",
      "Training instances: 121\n",
      "Testing instances: 32\n",
      "Feature Vector Length: (176,)\n",
      "Bucket 21\n",
      "\tPositives in training set: 0.18181818181818182\n",
      "\tPositives in testing set: 0.07142857142857142\n",
      "Training instances: 99\n",
      "Testing instances: 28\n",
      "Feature Vector Length: (170,)\n",
      "Bucket 22\n",
      "\tPositives in training set: 0.16470588235294117\n",
      "\tPositives in testing set: 0.08333333333333333\n",
      "Training instances: 85\n",
      "Testing instances: 24\n",
      "Feature Vector Length: (159,)\n",
      "Bucket 23\n",
      "\tPositives in training set: 0.17105263157894737\n",
      "\tPositives in testing set: 0.09523809523809523\n",
      "Training instances: 76\n",
      "Testing instances: 21\n",
      "Feature Vector Length: (167,)\n",
      "Bucket 24\n",
      "\tPositives in training set: 0.18181818181818182\n",
      "\tPositives in testing set: 0.10526315789473684\n",
      "Training instances: 66\n",
      "Testing instances: 19\n",
      "Feature Vector Length: (159,)\n",
      "Bucket 25\n",
      "\tPositives in training set: 0.1746031746031746\n",
      "\tPositives in testing set: 0.11764705882352941\n",
      "Training instances: 63\n",
      "Testing instances: 17\n",
      "Feature Vector Length: (153,)\n",
      "Bucket 26\n",
      "\tPositives in training set: 0.19298245614035087\n",
      "\tPositives in testing set: 0.125\n",
      "Training instances: 57\n",
      "Testing instances: 16\n",
      "Feature Vector Length: (157,)\n",
      "Bucket 27\n",
      "\tPositives in training set: 0.18867924528301888\n",
      "\tPositives in testing set: 0.125\n",
      "Training instances: 53\n",
      "Testing instances: 16\n",
      "Feature Vector Length: (155,)\n",
      "Bucket 28\n",
      "\tPositives in training set: 0.2\n",
      "\tPositives in testing set: 0.13333333333333333\n",
      "Training instances: 50\n",
      "Testing instances: 15\n",
      "Feature Vector Length: (155,)\n",
      "Bucket 29\n",
      "\tPositives in training set: 0.2222222222222222\n",
      "\tPositives in testing set: 0.15384615384615385\n",
      "Training instances: 45\n",
      "Testing instances: 13\n",
      "Feature Vector Length: (154,)\n",
      "xgboost_prefix_index\n",
      "Bucket 1\n",
      "\tPositives in training set: 0.1488\n",
      "\tPositives in testing set: 0.11464968152866242\n",
      "Training instances: 625\n",
      "Testing instances: 157\n",
      "Feature Vector Length: (147,)\n",
      "Bucket 2\n",
      "\tPositives in training set: 0.1488\n",
      "\tPositives in testing set: 0.11464968152866242\n",
      "Training instances: 625\n",
      "Testing instances: 157\n",
      "Feature Vector Length: (158,)\n",
      "Bucket 3\n",
      "\tPositives in training set: 0.1488\n",
      "\tPositives in testing set: 0.11464968152866242\n",
      "Training instances: 625\n",
      "Testing instances: 157\n",
      "Feature Vector Length: (188,)\n",
      "Bucket 4\n",
      "\tPositives in training set: 0.1488\n",
      "\tPositives in testing set: 0.11464968152866242\n",
      "Training instances: 625\n",
      "Testing instances: 157\n",
      "Feature Vector Length: (206,)\n",
      "Bucket 5\n",
      "\tPositives in training set: 0.1488\n",
      "\tPositives in testing set: 0.11464968152866242\n",
      "Training instances: 625\n",
      "Testing instances: 157\n",
      "Feature Vector Length: (236,)\n",
      "Bucket 6\n",
      "\tPositives in training set: 0.1492776886035313\n",
      "\tPositives in testing set: 0.11464968152866242\n",
      "Training instances: 623\n",
      "Testing instances: 157\n",
      "Feature Vector Length: (262,)\n",
      "Bucket 7\n",
      "\tPositives in training set: 0.15\n",
      "\tPositives in testing set: 0.11464968152866242\n",
      "Training instances: 620\n",
      "Testing instances: 157\n",
      "Feature Vector Length: (289,)\n",
      "Bucket 8\n",
      "\tPositives in training set: 0.15016501650165018\n",
      "\tPositives in testing set: 0.11464968152866242\n",
      "Training instances: 606\n",
      "Testing instances: 157\n",
      "Feature Vector Length: (320,)\n",
      "Bucket 9\n",
      "\tPositives in training set: 0.15228426395939088\n",
      "\tPositives in testing set: 0.11688311688311688\n",
      "Training instances: 591\n",
      "Testing instances: 154\n",
      "Feature Vector Length: (358,)\n",
      "Bucket 10\n",
      "\tPositives in training set: 0.15424610051993068\n",
      "\tPositives in testing set: 0.11842105263157894\n",
      "Training instances: 577\n",
      "Testing instances: 152\n",
      "Feature Vector Length: (386,)\n",
      "Bucket 11\n",
      "\tPositives in training set: 0.16261682242990655\n",
      "\tPositives in testing set: 0.11347517730496454\n",
      "Training instances: 535\n",
      "Testing instances: 141\n",
      "Feature Vector Length: (417,)\n",
      "Bucket 12\n",
      "\tPositives in training set: 0.1623246492985972\n",
      "\tPositives in testing set: 0.10852713178294573\n",
      "Training instances: 499\n",
      "Testing instances: 129\n",
      "Feature Vector Length: (435,)\n",
      "Bucket 13\n",
      "\tPositives in training set: 0.17067307692307693\n",
      "\tPositives in testing set: 0.09174311926605505\n",
      "Training instances: 416\n",
      "Testing instances: 109\n",
      "Feature Vector Length: (454,)\n",
      "Bucket 14\n",
      "\tPositives in training set: 0.18156424581005587\n",
      "\tPositives in testing set: 0.08888888888888889\n",
      "Training instances: 358\n",
      "Testing instances: 90\n",
      "Feature Vector Length: (464,)\n",
      "Bucket 15\n",
      "\tPositives in training set: 0.20141342756183744\n",
      "\tPositives in testing set: 0.10256410256410256\n",
      "Training instances: 283\n",
      "Testing instances: 78\n",
      "Feature Vector Length: (478,)\n",
      "Bucket 16\n",
      "\tPositives in training set: 0.19736842105263158\n",
      "\tPositives in testing set: 0.1\n",
      "Training instances: 228\n",
      "Testing instances: 60\n",
      "Feature Vector Length: (481,)\n",
      "Bucket 17\n",
      "\tPositives in training set: 0.18333333333333332\n",
      "\tPositives in testing set: 0.10204081632653061\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training instances: 180\n",
      "Testing instances: 49\n",
      "Feature Vector Length: (474,)\n",
      "Bucket 18\n",
      "\tPositives in training set: 0.16447368421052633\n",
      "\tPositives in testing set: 0.05263157894736842\n",
      "Training instances: 152\n",
      "Testing instances: 38\n",
      "Feature Vector Length: (476,)\n",
      "Bucket 19\n",
      "\tPositives in training set: 0.16541353383458646\n",
      "\tPositives in testing set: 0.05555555555555555\n",
      "Training instances: 133\n",
      "Testing instances: 36\n",
      "Feature Vector Length: (471,)\n",
      "Bucket 20\n",
      "\tPositives in training set: 0.18181818181818182\n",
      "\tPositives in testing set: 0.0625\n",
      "Training instances: 121\n",
      "Testing instances: 32\n",
      "Feature Vector Length: (506,)\n",
      "Bucket 21\n",
      "\tPositives in training set: 0.18181818181818182\n",
      "\tPositives in testing set: 0.07142857142857142\n",
      "Training instances: 99\n",
      "Testing instances: 28\n",
      "Feature Vector Length: (505,)\n",
      "Bucket 22\n",
      "\tPositives in training set: 0.16470588235294117\n",
      "\tPositives in testing set: 0.08333333333333333\n",
      "Training instances: 85\n",
      "Testing instances: 24\n",
      "Feature Vector Length: (484,)\n",
      "Bucket 23\n",
      "\tPositives in training set: 0.17105263157894737\n",
      "\tPositives in testing set: 0.09523809523809523\n",
      "Training instances: 76\n",
      "Testing instances: 21\n",
      "Feature Vector Length: (503,)\n",
      "Bucket 24\n",
      "\tPositives in training set: 0.18181818181818182\n",
      "\tPositives in testing set: 0.10526315789473684\n",
      "Training instances: 66\n",
      "Testing instances: 19\n",
      "Feature Vector Length: (528,)\n",
      "Bucket 25\n",
      "\tPositives in training set: 0.1746031746031746\n",
      "\tPositives in testing set: 0.11764705882352941\n",
      "Training instances: 63\n",
      "Testing instances: 17\n",
      "Feature Vector Length: (535,)\n",
      "Bucket 26\n",
      "\tPositives in training set: 0.19298245614035087\n",
      "\tPositives in testing set: 0.125\n",
      "Training instances: 57\n",
      "Testing instances: 16\n",
      "Feature Vector Length: (564,)\n",
      "Bucket 27\n",
      "\tPositives in training set: 0.18867924528301888\n",
      "\tPositives in testing set: 0.125\n",
      "Training instances: 53\n",
      "Testing instances: 16\n",
      "Feature Vector Length: (561,)\n",
      "Bucket 28\n",
      "\tPositives in training set: 0.2\n",
      "\tPositives in testing set: 0.13333333333333333\n",
      "Training instances: 50\n",
      "Testing instances: 15\n",
      "Feature Vector Length: (582,)\n",
      "Bucket 29\n",
      "\tPositives in training set: 0.2222222222222222\n",
      "\tPositives in testing set: 0.15384615384615385\n",
      "Training instances: 45\n",
      "Testing instances: 13\n",
      "Feature Vector Length: (595,)\n",
      "lstm_single_3d\n",
      "Bucket all\n",
      "\tPositives in training set: 0.16550885724209796\n",
      "\tPositives in testing set: 0.13805817287996722\n",
      "Training instances: 5758\n",
      "Testing instances: 4882\n",
      "Feature Vector Length: (29, 211)\n",
      "bpic2012\n",
      "xgboost_single_agg\n",
      "Bucket 1\n",
      "\tPositives in training set: 0.5010638748781419\n",
      "\tPositives in testing set: 0.5539380487645658\n"
     ]
    }
   ],
   "source": [
    "datasets = [\"production\", \"sepsis_cases\", \"bpic2012\"]\n",
    "methods = [\"xgboost_single_agg\", \"xgboost_prefix_agg\", \"xgboost_prefix_index\", \"lstm_single_3d\"]\n",
    "dataset_names = {\"production\":\"production\", \"sepsis_cases\":\"sepsis_cases_1\", \"bpic2012\":\"bpic2012_accepted\"}\n",
    "\n",
    "\n",
    "for data in datasets:\n",
    "    print(data)\n",
    "            \n",
    "    dataset_manager = DatasetManager(dataset_names[data])\n",
    "\n",
    "    \n",
    "    for method in methods:\n",
    "        print(method)\n",
    "        folder_loc = os.path.join(PATH, \"%s/%s/\" %(data, method))\n",
    "        \n",
    "        if \"single\" in method:\n",
    "            num_buckets = 1\n",
    "        else:\n",
    "            num_buckets = len([name for name in os.listdir(os.path.join(folder_loc,'models/'))])\n",
    "            \n",
    "        for bucket in range(num_buckets):\n",
    "            if \"lstm\" in method:\n",
    "                bucketID = \"all\"\n",
    "            else:\n",
    "                bucketID = bucket+1\n",
    "            print ('Bucket', bucketID)\n",
    "            \n",
    "            X_train_path = os.path.join(folder_loc, \"train_data/bucket_%s_prefixes.pickle\" % (bucketID))\n",
    "            with open (X_train_path, 'rb') as f:\n",
    "                dt_train_bucket = pickle.load(f)\n",
    "\n",
    "            X_test_path = os.path.join(folder_loc, \"test_data/bucket_%s_prefixes.pickle\" % (bucketID))\n",
    "            with open (X_test_path, 'rb') as f:\n",
    "                dt_test_bucket = pickle.load(f)\n",
    "            \n",
    "\n",
    "            if \"lstm\" not in method: \n",
    "                y_train = dataset_manager.get_label_numeric(dt_train_bucket)\n",
    "                y_test = dataset_manager.get_label_numeric(dt_test_bucket)\n",
    "            \n",
    "                train_pos_cases = len([i for i in y_train if i == 1])\n",
    "                train_neg_cases = len([i for i in y_train if i == 0])\n",
    "                train_pos_ratio = train_pos_cases/(train_pos_cases + train_neg_cases)\n",
    "                print(\"\\tPositives in training set:\", train_pos_ratio)\n",
    "\n",
    "                test_pos_cases = len([i for i in y_test if i == 1])\n",
    "                test_neg_cases = len([i for i in y_test if i == 0])\n",
    "                test_pos_ratio = test_pos_cases/(test_pos_cases + test_neg_cases)\n",
    "                print(\"\\tPositives in testing set:\", test_pos_ratio)\n",
    "                \n",
    "                feat_comb_path = os.path.join(folder_loc, \"bucketers_and_encoders/feature_combiner_bucket_%s.joblib\" % (bucketID))\n",
    "                with open (feat_comb_path, 'rb') as f:\n",
    "                    feature_combiner = joblib.load(f)\n",
    "                \n",
    "                train_x = feature_combiner.transform(dt_train_bucket)\n",
    "                test_x = feature_combiner.transform(dt_test_bucket)\n",
    "                vector_length = test_x[0].shape\n",
    "                \n",
    "                print(\"Training instances:\", len(y_train))\n",
    "                print(\"Testing instances:\", len(y_test))\n",
    "                print(\"Feature Vector Length:\", vector_length)\n",
    "                \n",
    "            else:\n",
    "                y_train_path = os.path.join(folder_loc, \"train_data/bucket_%s_labels.pickle\" % (bucketID))\n",
    "                with open (y_train_path, 'rb') as f:\n",
    "                    train_y = pickle.load(f)\n",
    "\n",
    "                y_test_path = os.path.join(folder_loc, \"test_data/bucket_%s_labels.pickle\" % (bucketID))\n",
    "                with open (y_test_path, 'rb') as f:\n",
    "                    test_y = pickle.load(f)\n",
    "                \n",
    "                y_train = [int(np.where(i==1)[0]) for i in train_y]\n",
    "                y_test = test_y\n",
    "               \n",
    "                train_pos_cases = len([i for i in y_train if i == 1])\n",
    "                train_neg_cases = len([i for i in y_train if i == 0])\n",
    "                train_pos_ratio = train_pos_cases/(train_pos_cases + train_neg_cases)\n",
    "                print(\"\\tPositives in training set:\", train_pos_ratio)\n",
    "\n",
    "                test_pos_cases = len([i for i in y_test if i == 1])\n",
    "                test_neg_cases = len([i for i in y_test if i == 0])\n",
    "                test_pos_ratio = test_pos_cases/(test_pos_cases + test_neg_cases)\n",
    "                print(\"\\tPositives in testing set:\", test_pos_ratio)\n",
    "                \n",
    "                vector_length = dt_test_bucket[0].shape\n",
    "                \n",
    "                print(\"Training instances:\", len(y_train))\n",
    "                print(\"Testing instances:\", len(y_test))\n",
    "                print(\"Feature Vector Length:\", vector_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\"production\", \"sepsis_cases\", \"bpic2012\"]\n",
    "methods = [\"xgboost_single_agg\", \"xgboost_prefix_agg\", \"xgboost_prefix_index\"]\n",
    "dataset_names = {\"production\":\"production\", \"sepsis_cases\":\"sepsis_cases_1\", \"bpic2012\":\"bpic2012_accepted\"}\n",
    "titles = {\"production\": \"Production: \", \"sepsis_cases\": \"Sepsis Cases: \", \"bpic2012\": \"BPIC2012: \",\n",
    "         \"xgboost_single_agg\": \"Single Bucket & Aggregate Encoding\", \"xgboost_prefix_agg\": \"Prefix-Length Buckets \\n& Aggregate Encoding\",\n",
    "         \"xgboost_prefix_index\": \"Prefix-Length Buckets \\n& Index-Based Encoding\"}\n",
    "\n",
    "for data in datasets:\n",
    "    print(data)\n",
    "            \n",
    "    dataset_manager = DatasetManager(dataset_names[data])\n",
    "\n",
    "    \n",
    "    for method in methods:\n",
    "        print(method)\n",
    "        folder_loc = os.path.join(PATH, \"%s/%s/\" %(data, method))\n",
    "        \n",
    "        if \"single\" in method:\n",
    "            num_buckets = 1\n",
    "        else:\n",
    "            num_buckets = len([name for name in os.listdir(os.path.join(folder_loc,'models/'))])\n",
    "            \n",
    "        train_lengths = []\n",
    "        train_predictions = []\n",
    "        train_y_all = []\n",
    "        \n",
    "        test_lengths = []\n",
    "        test_predictions = []\n",
    "        test_y_all = []\n",
    "            \n",
    "        for bucket in range(num_buckets):\n",
    "            if \"lstm\" in method:\n",
    "                bucketID = \"all\"\n",
    "            else:\n",
    "                bucketID = bucket+1\n",
    "            print ('Bucket', bucketID)\n",
    "            \n",
    "            X_train_path = os.path.join(folder_loc, \"train_data/bucket_%s_prefixes.pickle\" % (bucketID))\n",
    "            with open (X_train_path, 'rb') as f:\n",
    "                dt_train_bucket = pickle.load(f)\n",
    "\n",
    "            X_test_path = os.path.join(folder_loc, \"test_data/bucket_%s_prefixes.pickle\" % (bucketID))\n",
    "            with open (X_test_path, 'rb') as f:\n",
    "                dt_test_bucket = pickle.load(f)\n",
    "            \n",
    "\n",
    "            if \"lstm\" not in method: \n",
    "                y_train = dataset_manager.get_label_numeric(dt_train_bucket)\n",
    "                y_test = dataset_manager.get_label_numeric(dt_test_bucket)\n",
    "            \n",
    "                train_pos_cases = len([i for i in y_train if i == 1])\n",
    "                train_neg_cases = len([i for i in y_train if i == 0])\n",
    "                train_pos_ratio = train_pos_cases/(train_pos_cases + train_neg_cases)\n",
    "                #print(\"\\tPositives in training set:\", train_pos_ratio)\n",
    "\n",
    "                test_pos_cases = len([i for i in y_test if i == 1])\n",
    "                test_neg_cases = len([i for i in y_test if i == 0])\n",
    "                test_pos_ratio = test_pos_cases/(test_pos_cases + test_neg_cases)\n",
    "                #print(\"\\tPositives in testing set:\", test_pos_ratio)\n",
    "                \n",
    "                pipeline_path = os.path.join(folder_loc, \"pipelines/pipeline_bucket_%s.joblib\" % (bucketID))\n",
    "                with open (pipeline_path, 'rb') as f:\n",
    "                    pipeline = joblib.load(f)\n",
    "                \n",
    "                prefix_lengths = dataset_manager.get_prefix_lengths(dt_train_bucket)             \n",
    "                train_lengths.extend(prefix_lengths.values)\n",
    "                \n",
    "                prefix_lengths = dataset_manager.get_prefix_lengths(dt_test_bucket)             \n",
    "                test_lengths.extend(prefix_lengths.values)\n",
    "                \n",
    "                train_preds = pipeline.predict(dt_train_bucket)\n",
    "                train_predictions.extend(train_preds)\n",
    "                \n",
    "                test_preds = pipeline.predict(dt_test_bucket)\n",
    "                test_predictions.extend(test_preds)\n",
    "                \n",
    "                train_y_all.extend(y_train)\n",
    "                test_y_all.extend(y_test)\n",
    "                \n",
    "#             else:\n",
    "#                 y_train_path = os.path.join(folder_loc, \"train_data/bucket_%s_labels.pickle\" % (bucketID))\n",
    "#                 with open (y_train_path, 'rb') as f:\n",
    "#                     train_y = pickle.load(f)\n",
    "\n",
    "#                 y_test_path = os.path.join(folder_loc, \"test_data/bucket_%s_labels.pickle\" % (bucketID))\n",
    "#                 with open (y_test_path, 'rb') as f:\n",
    "#                     test_y = pickle.load(f)\n",
    "                \n",
    "#                 y_train = [int(np.where(i==1)[0]) for i in train_y]\n",
    "#                 y_test = test_y\n",
    "                \n",
    "#                 cls_path = os.path.join(folder_loc, \"cls/pred_cls.h5\")\n",
    "#                 cls = load_model(cls_path)\n",
    "                \n",
    "#                 preds = cls.predict(dt_train_bucket)\n",
    "#                 train_preds = [np.argmax(i, axis = -1) for i in preds]\n",
    "                \n",
    "#                 preds = cls.predict(dt_test_bucket)\n",
    "#                 test_preds = [np.argmax(i, axis = -1) for i in preds]\n",
    "            \n",
    "        lens = list(set(train_lengths))\n",
    "        train_accs = []\n",
    "        test_accs = []\n",
    "        for i in lens:\n",
    "            idxs = [n for n in range(len(train_lengths)) if train_lengths[n] == i]\n",
    "            preds = [train_predictions[n] for n in idxs]\n",
    "            actual = [train_y_all[n] for n in idxs]\n",
    "            accuracy = accuracy_score(actual, preds)\n",
    "            train_accs.append(accuracy)\n",
    "            \n",
    "            idxs = [n for n in range(len(test_lengths)) if test_lengths[n] == i]\n",
    "            preds = [test_predictions[n] for n in idxs]\n",
    "            actual = [test_y_all[n] for n in idxs]\n",
    "            accuracy = accuracy_score(actual, preds)\n",
    "            test_accs.append(accuracy)\n",
    "\n",
    "\n",
    "        plt.plot(lens, train_accs, label = 'train')\n",
    "        plt.plot(lens, test_accs, label = 'test')\n",
    "        plt.legend(loc = \"upper left\")\n",
    "        plt.ylim(0,1.1)\n",
    "        plt.title(titles[data]+titles[method])\n",
    "        plt.show()\n",
    "        \n",
    "        train_acc = accuracy_score(train_y_all, train_predictions)\n",
    "        print(\"\\tTraining accuracy:\", train_acc)\n",
    "        \n",
    "        train_pos_idx = [i for i in range(len(train_predictions)) if train_predictions[i] == 1]\n",
    "        actual = [train_y_all[i] for i in train_pos_idx]\n",
    "        preds = [train_predictions[i] for i in train_pos_idx]\n",
    "        train_pos_acc = accuracy_score(actual, preds)\n",
    "        print(\"\\t\\tTraining accuracy - Positives:\", train_pos_acc)\n",
    "        \n",
    "        train_neg_idx = [i for i in range(len(train_predictions)) if train_predictions[i] == 0]\n",
    "        actual = [train_y_all[i] for i in train_neg_idx]\n",
    "        preds = [train_predictions[i] for i in train_neg_idx]\n",
    "        train_neg_acc = accuracy_score(actual, preds)\n",
    "        print(\"\\t\\tTraining accuracy - Negatives:\", train_neg_acc)\n",
    "        \n",
    "        test_acc = accuracy_score(test_y_all, test_predictions)\n",
    "        print(\"\\tTesting accuracy:\", test_acc)\n",
    "        \n",
    "        test_pos_idx = [i for i in range(len(test_predictions)) if test_predictions[i] == 1]\n",
    "        actual = [test_y_all[i] for i in test_pos_idx]\n",
    "        preds = [test_predictions[i] for i in test_pos_idx]\n",
    "        test_pos_acc = accuracy_score(actual, preds)\n",
    "        print(\"\\t\\tTesting accuracy - Positives:\", test_pos_acc)\n",
    "        \n",
    "        test_neg_idx = [i for i in range(len(test_predictions)) if test_predictions[i] == 0]\n",
    "        actual = [test_y_all[i] for i in test_neg_idx]\n",
    "        preds = [test_predictions[i] for i in test_neg_idx]\n",
    "        test_neg_acc = accuracy_score(actual, preds)\n",
    "        print(\"\\t\\tTesting accuracy - Negatives:\", test_neg_acc)\n",
    "            \n",
    "        #break\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\"bpic2012\"]\n",
    "#datasets = [\"production\", \"sepsis_cases\", \"bpic2012\"]\n",
    "methods = [\"xgboost_single_agg\", \"xgboost_prefix_agg\", \"xgboost_prefix_index\"]\n",
    "dataset_names = {\"production\":\"production\", \"sepsis_cases\":\"sepsis_cases_1\", \"bpic2012\":\"bpic2012_accepted\"}\n",
    "data_names = {\"production\":\"Production\", \"sepsis_cases\":\"Sepsis Cases\", \"bpic2012\":\"BPIC2012\"}\n",
    "method_names = {\"xgboost_single_agg\": \"Single Bucket with Aggregate Encoding\", \n",
    "               \"xgboost_prefix_agg\": \"Prefix-Length Buckets with Aggregate Encoding\", \n",
    "               \"xgboost_prefix_index\": \"Prefix-Length Buckets with Index-Based Encoding\"}\n",
    "\n",
    "all_pref_lens = []\n",
    "all_feat_lens = []\n",
    "all_explainers = []\n",
    "all_times = []\n",
    "all_datasets = []\n",
    "all_methods = []\n",
    "\n",
    "data_dict = {\"Prefix lengths\": all_pref_lens, \"Feature vector sizes\": all_feat_lens, \"Explainer\": all_explainers,\n",
    "       \"Running time\": all_times, \"Dataset\": all_datasets, \"Method\": all_methods}\n",
    "\n",
    "for data in datasets:\n",
    "    print(data)\n",
    "            \n",
    "    dataset_manager = DatasetManager(dataset_names[data])\n",
    "\n",
    "    \n",
    "    for method in methods:\n",
    "        print(method)\n",
    "        folder_loc = os.path.join(PATH, \"%s/%s/\" %(data, method))\n",
    "        \n",
    "        all_timing_path = os.path.join(folder_loc, \"all_timing.csv\")\n",
    "        all_data = pd.read_csv(all_timing_path)\n",
    "        \n",
    "        all_pref_lens.extend(pd.Series(all_data['Prefix Length']))\n",
    "        all_feat_lens.extend(pd.Series(all_data['Feature Vector Length']))\n",
    "        all_explainers.extend(pd.Series(all_data['Explainer']))\n",
    "        all_times.extend(pd.Series(all_data['Running Time']))\n",
    "        \n",
    "        all_datasets.extend([data]*all_data.shape[0])\n",
    "        all_methods.extend([method]*all_data.shape[0])\n",
    "        \n",
    "comb_data = pd.DataFrame(data_dict)\n",
    "grid = sns.FacetGrid(comb_data, col = 'Method', hue = 'Explainer', \n",
    "                     legend_out = True, height = 3.5, aspect = 1)\n",
    "grid.map(sns.scatterplot, \"Prefix lengths\", \"Running time\", size= comb_data[\"Feature vector sizes\"])\n",
    "grid.set_axis_labels(\"Prefix lengths\", \"Running Time\")\n",
    "grid.add_legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_dict = {}\n",
    "for each in methods:\n",
    "    size_dict[each] = {}\n",
    "    method_data = comb_data[comb_data['Method'] == each]\n",
    "    keys = [str(i) for i in method_data['Prefix lengths'].unique()]\n",
    "    for key in keys:\n",
    "        length = [i for i in method_data[method_data['Prefix lengths'] == int(key)]['Feature vector sizes']][0]\n",
    "        size_dict[each][key] = length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\"bpic2012\"]\n",
    "#datasets = [\"production\", \"sepsis_cases\", \"bpic2012\"]\n",
    "methods = [\"xgboost_single_agg\", \"xgboost_prefix_agg\", \"xgboost_prefix_index\"]\n",
    "dataset_names = {\"production\":\"production\", \"sepsis_cases\":\"sepsis_cases_1\", \"bpic2012\":\"bpic2012_accepted\"}\n",
    "data_names = {\"production\":\"Production\", \"sepsis_cases\":\"Sepsis Cases\", \"bpic2012\":\"BPIC2012\"}\n",
    "method_names = {\"xgboost_single_agg\": \"Single Bucket with Aggregate Encoding\", \n",
    "               \"xgboost_prefix_agg\": \"Prefix-Length Buckets with Aggregate Encoding\", \n",
    "               \"xgboost_prefix_index\": \"Prefix-Length Buckets with Index-Based Encoding\"}\n",
    "\n",
    "all_pref_lens = []\n",
    "all_proba = []\n",
    "all_feat_vec = []\n",
    "all_explainers = []\n",
    "all_datasets = []\n",
    "all_methods = []\n",
    "all_stability = []\n",
    "hue_lens = []\n",
    "\n",
    "data_dict = {\"Prefix lengths\": all_pref_lens, \"Prediction Probability\": all_proba, \"Explainer\": all_explainers,\n",
    "       \"Stability\": all_stability, \"Dataset\": all_datasets, \"Method\": all_methods, \"Hue\": hue_lens, 'Feature Vector Sizes': all_feat_vec}\n",
    "\n",
    "for data in datasets:\n",
    "    print(data)\n",
    "            \n",
    "    dataset_manager = DatasetManager(dataset_names[data])\n",
    "\n",
    "    \n",
    "    for method in methods:\n",
    "        print(method)\n",
    "        folder_loc = os.path.join(PATH, \"%s/%s/samples/\" %(data, method))\n",
    "        \n",
    "        nr_events = []\n",
    "        proba = []\n",
    "        lime_stability = []\n",
    "        tree_shap_stability = []\n",
    "        feat_vec = []\n",
    "        \n",
    "        for each in os.listdir(folder_loc):\n",
    "            file  = os.path.join(folder_loc, each)\n",
    "            with open (file, 'rb') as f:\n",
    "                results = pickle.load(f)\n",
    "                \n",
    "            if results != []:\n",
    "                results = pd.DataFrame.from_records(results)\n",
    "            \n",
    "                bucket_events = pd.Series(results['nr_events'])\n",
    "                bucket_proba = pd.Series(results['proba'])\n",
    "                bucket_lime_stability = pd.Series(results['lime_stability'])\n",
    "                #bucket_tree_shap_stability = pd.Series(results['tree_shap_stability'])\n",
    "\n",
    "                nr_events.extend(bucket_events)\n",
    "                proba.extend(bucket_proba)\n",
    "                lime_stability.extend(bucket_lime_stability)\n",
    "                #tree_shap_stability.extend(bucket_tree_shap_stability)\n",
    "                \n",
    "        for length in nr_events:\n",
    "            feat_vec.append(size_dict[method][str(length)])\n",
    "        \n",
    "        plot_lens = list(set(nr_events))\n",
    "        plot_vecs = [size_dict[method][str(i)] for i in plot_lens]\n",
    "        \n",
    "        fig, ax = plt.subplots()\n",
    "        ax.plot(nr_events, lime_stability, 'ro', label = \"LIME\")\n",
    "        ax2 = ax.twinx()\n",
    "        ax2.plot(plot_lens, plot_vecs, label = \"Feature Vector\")\n",
    "        ax.set_xlabel(\"Prefix Length\")\n",
    "        ax.set_ylabel(\"Stability\")\n",
    "        ax.set_ylabel(\"Feature Vector Size\")\n",
    "        #ax.legend(frameon = False, bbox_to_anchor=(1, 1), loc = 'upper left')\n",
    "        #plt.yticks(np.arange(0,1, 0.1))\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax2.invert_yaxis()\n",
    "        #plt.title(\"Prefix length and stability by index \\nBPIC 2012 Prefix-Length Bucket with Index-Based Encoding\")\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "        #for i in range(2):\n",
    "        all_pref_lens.extend(nr_events)\n",
    "        all_proba.extend(proba)\n",
    "        all_datasets.extend([data]*len(nr_events))\n",
    "        all_methods.extend([method]*len(nr_events))\n",
    "        all_feat_vec.extend(feat_vec)\n",
    "\n",
    "        all_explainers.extend([\"LIME\"]*len(nr_events))\n",
    "#        all_explainers.extend([\"SHAP\"]*len(nr_events))\n",
    "        \n",
    "        all_stability.extend(lime_stability)\n",
    " #       all_stability.extend(tree_shap_stability)\n",
    "            \n",
    "        avg_lime_stability = np.mean(lime_stability)\n",
    "  #      avg_shap_stability = np.mean(tree_shap_stability)\n",
    "\n",
    "        print(\"LIME:\", avg_lime_stability)\n",
    "   #     print(\"SHAP:\", avg_shap_stability)\n",
    "            \n",
    "max_len = max(all_pref_lens)\n",
    "bins = np.arange(-5, max_len+10, 5)\n",
    "labels = []\n",
    "for i in range(len(bins)-1):\n",
    "    start = str(int(bins[i]+1))\n",
    "    end = str(int(bins[i+1]))\n",
    "    label = start+\"-\"+end+\" prefixes\"\n",
    "    labels.append(label)\n",
    "\n",
    "for length in all_pref_lens:\n",
    "    cur_bin = 0\n",
    "    while length > bins[cur_bin+1]:\n",
    "        cur_bin += 1\n",
    "    hue_lens.append(labels[cur_bin])\n",
    "\n",
    "    \n",
    "# def two_axes(x, y1, y2):\n",
    "#     ax = plt.gca()\n",
    "#     ax2 = ax.twinx()\n",
    "#     ax2.set_y_label('Feature Vector Size')\n",
    "#     ax.set_y_label('Stability')\n",
    "    \n",
    "#     ax.plot(x, y1, 'bo')\n",
    "#     ax2.plot(x, y2, color = 'red')\n",
    "\n",
    "    \n",
    "# comb_data = pd.DataFrame(data_dict)\n",
    "# comb_data = comb_data.sort_values('Method', ascending = False)\n",
    "# comb_data = comb_data.sort_values('Prefix lengths')\n",
    "# grid = sns.FacetGrid(comb_data, col = 'Method', row = 'Explainer', legend_out = True, height = 4, aspect = 1, palette = 'colorblind', ylim = (0, 1.1))\n",
    "# grid.map(sns.scatterplot, \"Prefix lengths\", \"Stability\")\n",
    "# grid.map(plt.plot, \"Prefix lengths\", 'Feature Vector Sizes')\n",
    "# for axes, (_, subdata) in zip(grid.axes, comb_data.groupby(['Method', 'Prefix lengths'])):\n",
    "#     ax2=ax.twinx()\n",
    "#     subdata.plot(x='Prefix lengths',y='Feature Vector Sizes', ax=ax2,legend=False,color='r')\n",
    "# grid.set_axis_labels(\"Prefix lengths\", \"Stability\")\n",
    "# grid.add_legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\"bpic2012\"]\n",
    "#datasets = [\"production\", \"sepsis_cases\", \"bpic2012\"]\n",
    "methods = [\"xgboost_single_agg\", \"xgboost_prefix_agg\", \"xgboost_prefix_index\"]\n",
    "dataset_names = {\"production\":\"production\", \"sepsis_cases\":\"sepsis_cases_1\", \"bpic2012\":\"bpic2012_accepted\"}\n",
    "data_names = {\"production\":\"Production\", \"sepsis_cases\":\"Sepsis Cases\", \"bpic2012\":\"BPIC2012\"}\n",
    "method_names = {\"xgboost_single_agg\": \"Single Bucket with Aggregate Encoding\", \n",
    "               \"xgboost_prefix_agg\": \"Prefix-Length Buckets with Aggregate Encoding\", \n",
    "               \"xgboost_prefix_index\": \"Prefix-Length Buckets with Index-Based Encoding\"}\n",
    "\n",
    "all_pref_lens = []\n",
    "all_proba = []\n",
    "all_explainers = []\n",
    "all_datasets = []\n",
    "all_methods = []\n",
    "all_stability = []\n",
    "hue_lens = []\n",
    "\n",
    "data_dict = {\"Prefix lengths\": all_pref_lens, \"Prediction Probability\": all_proba, \"Explainer\": all_explainers,\n",
    "       \"Stability\": all_stability, \"Dataset\": all_datasets, \"Method\": all_methods, \"Hue\": hue_lens}\n",
    "\n",
    "for data in datasets:\n",
    "    print(data)\n",
    "            \n",
    "    dataset_manager = DatasetManager(dataset_names[data])\n",
    "\n",
    "    \n",
    "    for method in methods:\n",
    "        print(method)\n",
    "        folder_loc = os.path.join(PATH, \"%s/%s/samples/\" %(data, method))\n",
    "        \n",
    "        nr_events = []\n",
    "        proba = []\n",
    "        lime_stability = []\n",
    "        tree_shap_stability = []\n",
    "        \n",
    "        for each in os.listdir(folder_loc):\n",
    "            file  = os.path.join(folder_loc, each)\n",
    "            with open (file, 'rb') as f:\n",
    "                results = pickle.load(f)\n",
    "            \n",
    "            if results != []:\n",
    "                results = pd.DataFrame.from_records(results)\n",
    "            \n",
    "                bucket_events = pd.Series(results['nr_events'])\n",
    "                bucket_proba = pd.Series(results['proba'])\n",
    "                bucket_lime_stability = pd.Series(results['lime_stability'])\n",
    "                #bucket_tree_shap_stability = pd.Series(results['tree_shap_stability'])\n",
    "\n",
    "                nr_events.extend(bucket_events)\n",
    "                proba.extend(bucket_proba)\n",
    "                lime_stability.extend(bucket_lime_stability)\n",
    "                #tree_shap_stability.extend(bucket_tree_shap_stability)\n",
    "                \n",
    "        #for i in range(2):\n",
    "        all_pref_lens.extend(nr_events)\n",
    "        all_proba.extend(proba)\n",
    "        all_datasets.extend([data]*len(nr_events))\n",
    "        all_methods.extend([method]*len(nr_events))\n",
    "\n",
    "        all_explainers.extend([\"LIME\"]*len(nr_events))\n",
    "#        all_explainers.extend([\"SHAP\"]*len(nr_events))\n",
    "        \n",
    "        all_stability.extend(lime_stability)\n",
    " #       all_stability.extend(tree_shap_stability)\n",
    "            \n",
    "        avg_lime_stability = np.mean(lime_stability)\n",
    "  #      avg_shap_stability = np.mean(tree_shap_stability)\n",
    "\n",
    "        print(\"LIME:\", avg_lime_stability)\n",
    "   #     print(\"SHAP:\", avg_shap_stability)\n",
    "            \n",
    "max_len = max(all_pref_lens)\n",
    "bins = np.arange(-5, max_len+10, 5)\n",
    "labels = []\n",
    "for i in range(len(bins)-1):\n",
    "    start = str(int(bins[i]+1))\n",
    "    end = str(int(bins[i+1]))\n",
    "    label = start+\"-\"+end+\" prefixes\"\n",
    "    labels.append(label)\n",
    "\n",
    "for length in all_pref_lens:\n",
    "    cur_bin = 0\n",
    "    while length > bins[cur_bin+1]:\n",
    "        cur_bin += 1\n",
    "    hue_lens.append(labels[cur_bin])\n",
    "                \n",
    "comb_data = pd.DataFrame(data_dict)\n",
    "#comb_data = comb_data.sort_values('Method', ascending = False)\n",
    "comb_data = comb_data.sort_values(['Prefix lengths', 'Explainer'])\n",
    "grid = sns.FacetGrid(comb_data, col = 'Method', row = 'Explainer', hue = 'Hue', \n",
    "                     legend_out = True, height = 4, aspect = 1, palette = 'colorblind', ylim = (0, 1.1))\n",
    "grid.map(sns.scatterplot, \"Prediction Probability\", \"Stability\")\n",
    "grid.set_axis_labels(\"Prediction Probability\", \"Stability\")\n",
    "grid.add_legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\"bpic2012\"]\n",
    "#datasets = [\"production\", \"sepsis_cases\", \"bpic2012\"]\n",
    "methods = [\"xgboost_single_agg\", \"xgboost_prefix_agg\", \"xgboost_prefix_index\"]\n",
    "dataset_names = {\"production\":\"production\", \"sepsis_cases\":\"sepsis_cases_1\", \"bpic2012\":\"bpic2012_accepted\"}\n",
    "data_names = {\"production\":\"Production\", \"sepsis_cases\":\"Sepsis Cases\", \"bpic2012\":\"BPIC2012\"}\n",
    "method_names = {\"xgboost_single_agg\": \"Single Bucket with Aggregate Encoding\", \n",
    "               \"xgboost_prefix_agg\": \"Prefix-Length Buckets with Aggregate Encoding\", \n",
    "               \"xgboost_prefix_index\": \"Prefix-Length Buckets with Index-Based Encoding\"}\n",
    "\n",
    "all_pref_lens = []\n",
    "all_proba = []\n",
    "all_explainers = []\n",
    "all_datasets = []\n",
    "all_methods = []\n",
    "all_stability = []\n",
    "hue_lens = []\n",
    "\n",
    "data_dict = {\"Prefix lengths\": all_pref_lens, \"Prediction Probability\": all_proba, \"Explainer\": all_explainers,\n",
    "       \"Stability\": all_stability, \"Dataset\": all_datasets, \"Method\": all_methods, \"Hue\": hue_lens}\n",
    "\n",
    "for data in datasets:\n",
    "    print(data)\n",
    "            \n",
    "    dataset_manager = DatasetManager(dataset_names[data])\n",
    "\n",
    "    \n",
    "    for method in methods:\n",
    "        print(method)\n",
    "        folder_loc = os.path.join(PATH, \"%s/%s/samples/\" %(data, method))\n",
    "        \n",
    "        nr_events = []\n",
    "        proba = []\n",
    "        lime_stability = []\n",
    "        tree_shap_stability = []\n",
    "        \n",
    "        for each in os.listdir(folder_loc):\n",
    "            file  = os.path.join(folder_loc, each)\n",
    "            with open (file, 'rb') as f:\n",
    "                results = pickle.load(f)\n",
    "            \n",
    "            if results != []:\n",
    "                results = pd.DataFrame.from_records(results)\n",
    "            \n",
    "                bucket_events = pd.Series(results['nr_events'])\n",
    "                bucket_proba = pd.Series(results['proba'])\n",
    "                bucket_lime_stability = pd.Series(results['adjusted_lime_importance_stability'])\n",
    "                #bucket_tree_shap_stability = pd.Series(results['adjusted_shap_importance_stability'])\n",
    "\n",
    "                nr_events.extend(bucket_events)\n",
    "                proba.extend(bucket_proba)\n",
    "                lime_stability.extend(bucket_lime_stability)\n",
    "                #tree_shap_stability.extend(bucket_tree_shap_stability)\n",
    "                \n",
    "        #for i in range(2):\n",
    "        all_pref_lens.extend(nr_events)\n",
    "        all_proba.extend(proba)\n",
    "        all_datasets.extend([data]*len(nr_events))\n",
    "        all_methods.extend([method]*len(nr_events))\n",
    "\n",
    "        all_explainers.extend([\"LIME\"]*len(nr_events))\n",
    "        #all_explainers.extend([\"SHAP\"]*len(nr_events))\n",
    "        \n",
    "        all_stability.extend(lime_stability)\n",
    "        #all_stability.extend(tree_shap_stability)\n",
    "            \n",
    "        avg_lime_stability = np.mean(lime_stability)\n",
    "        #avg_shap_stability = np.mean(tree_shap_stability)\n",
    "\n",
    "        print(\"LIME:\", avg_lime_stability)\n",
    "        #print(\"SHAP:\", avg_shap_stability)\n",
    "            \n",
    "max_len = max(all_pref_lens)\n",
    "bins = np.arange(-5, max_len+10, 5)\n",
    "labels = []\n",
    "for i in range(len(bins)-1):\n",
    "    start = str(int(bins[i]+1))\n",
    "    end = str(int(bins[i+1]))\n",
    "    label = start+\"-\"+end+\" prefixes\"\n",
    "    labels.append(label)\n",
    "\n",
    "for length in all_pref_lens:\n",
    "    cur_bin = 0\n",
    "    while length > bins[cur_bin+1]:\n",
    "        cur_bin += 1\n",
    "    hue_lens.append(labels[cur_bin])\n",
    "                \n",
    "comb_data = pd.DataFrame(data_dict)\n",
    "#comb_data = comb_data.sort_values('Method', ascending = False)\n",
    "comb_data = comb_data.sort_values(['Prefix lengths', 'Explainer'])\n",
    "grid = sns.FacetGrid(comb_data, col = 'Method', row = 'Explainer', hue = 'Hue', \n",
    "                     legend_out = True, height = 4, aspect = 1, palette = 'colorblind', ylim = (-1.1, 1.1))\n",
    "grid.map(sns.scatterplot, \"Prediction Probability\", \"Stability\")\n",
    "grid.set_axis_labels(\"Prediction Probability\", \"Stability\")\n",
    "grid.add_legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#datasets = [\"bpic2012\"]\n",
    "datasets = [\"production\", \"sepsis_cases\", \"bpic2012\"]\n",
    "#methods = [\"xgboost_prefix_index\"]\n",
    "methods = [\"xgboost_single_agg\", \"xgboost_prefix_agg\", \"xgboost_prefix_index\"]\n",
    "dataset_names = {\"production\":\"production\", \"sepsis_cases\":\"sepsis_cases_1\", \"bpic2012\":\"bpic2012_accepted\"}\n",
    "data_names = {\"production\":\"Production\", \"sepsis_cases\":\"Sepsis Cases\", \"bpic2012\":\"BPIC2012\"}\n",
    "method_names = {\"xgboost_single_agg\": \"Single Bucket with Aggregate Encoding\", \n",
    "               \"xgboost_prefix_agg\": \"Prefix-Length Buckets with Aggregate Encoding\", \n",
    "               \"xgboost_prefix_index\": \"Prefix-Length Buckets with Index-Based Encoding\"}\n",
    "\n",
    "all_pref_lens = []\n",
    "all_proba = []\n",
    "all_explainers = []\n",
    "all_datasets = []\n",
    "all_methods = []\n",
    "all_fidelity = []\n",
    "hue_lens = []\n",
    "\n",
    "data_dict = {\"Prefix lengths\": all_pref_lens, \"Prediction Probability\": all_proba, \"Explainer\": all_explainers,\n",
    "       \"Fidelity\": all_fidelity, \"Dataset\": all_datasets, \"Method\": all_methods, \"Hue\": hue_lens}\n",
    "\n",
    "for data in datasets:\n",
    "    print(data)\n",
    "            \n",
    "    dataset_manager = DatasetManager(dataset_names[data])\n",
    "\n",
    "    \n",
    "    for method in methods:\n",
    "        print(method)\n",
    "        folder_loc = os.path.join(PATH, \"%s/%s/samples/\" %(data, method))\n",
    "        \n",
    "        nr_events = []\n",
    "        proba = []\n",
    "        lime_fidelity = []\n",
    "        tree_shap_fidelity = []\n",
    "        \n",
    "        for each in os.listdir(folder_loc):\n",
    "            file  = os.path.join(folder_loc, each)\n",
    "            with open (file, 'rb') as f:\n",
    "                results = pickle.load(f)\n",
    "            \n",
    "            if results != []:\n",
    "                results = pd.DataFrame.from_records(results)\n",
    "            \n",
    "                bucket_events = pd.Series(results['nr_events'])\n",
    "                bucket_proba = pd.Series(results['proba'])\n",
    "                lime_diffs = pd.Series(results['lime_fid_change'])\n",
    "                shap_diffs = pd.Series(results['shap_fid_change'])\n",
    "                \n",
    "                lime_mape = []\n",
    "                shap_mape = []\n",
    "                \n",
    "                for j in range(len(lime_diffs)):\n",
    "                    p1 = bucket_proba[j]\n",
    "                    \n",
    "                    lime_diff = lime_diffs[j]\n",
    "                    shap_diff = shap_diffs[j]\n",
    "                                        \n",
    "                    lime_rel_changes = []\n",
    "                    shap_rel_changes = []\n",
    "                    \n",
    "                    for each in lime_diff:\n",
    "                        lime_rel_changes.append(abs(each)/p1)\n",
    "                        \n",
    "                    for each in shap_diff:\n",
    "                        shap_rel_changes.append(abs(each)/p1)\n",
    "                    \n",
    "                    lime_mape.append(sum(lime_rel_changes)/len(lime_rel_changes))\n",
    "                    shap_mape.append(sum(shap_rel_changes)/len(shap_rel_changes))\n",
    "\n",
    "                nr_events.extend(bucket_events)\n",
    "                proba.extend(bucket_proba)\n",
    "                lime_fidelity.extend(lime_mape)\n",
    "                tree_shap_fidelity.extend(shap_mape)\n",
    "                \n",
    "        for i in range(2):\n",
    "            all_pref_lens.extend(nr_events)\n",
    "            all_proba.extend(proba)\n",
    "            all_datasets.extend([data]*len(nr_events))\n",
    "            all_methods.extend([method]*len(nr_events))\n",
    "\n",
    "        all_explainers.extend([\"LIME\"]*len(nr_events))\n",
    "        all_explainers.extend([\"SHAP\"]*len(nr_events))\n",
    "\n",
    "        all_fidelity.extend(lime_fidelity)\n",
    "        all_fidelity.extend(tree_shap_fidelity)\n",
    "\n",
    "        avg_lime_fidelity = np.mean(lime_fidelity)\n",
    "        avg_shap_fidelity = np.mean(tree_shap_fidelity)\n",
    "\n",
    "        print(\"LIME:\", avg_lime_fidelity)\n",
    "        print(\"SHAP:\", avg_shap_fidelity)\n",
    "            \n",
    "max_len = max(all_pref_lens)\n",
    "bins = np.arange(-5, max_len+10, 5)\n",
    "labels = []\n",
    "for i in range(len(bins)-1):\n",
    "    start = str(int(bins[i]+1))\n",
    "    end = str(int(bins[i+1]))\n",
    "    label = start+\"-\"+end+\" prefixes\"\n",
    "    labels.append(label)\n",
    "\n",
    "for length in all_pref_lens:\n",
    "    cur_bin = 0\n",
    "    while length > bins[cur_bin+1]:\n",
    "        cur_bin += 1\n",
    "    hue_lens.append(labels[cur_bin])\n",
    "                \n",
    "comb_data = pd.DataFrame(data_dict)\n",
    "# comb_data = comb_data.sort_values(['Prefix lengths', 'Explainer'])\n",
    "# #data_fid = data_fid.sort_values('Method', ascending = False)\n",
    "# grid = sns.FacetGrid(comb_data, col = 'Dataset', row = 'Explainer', hue = 'Hue', \n",
    "#                      legend_out = True, height = 5, aspect = 1, palette = 'colorblind', ylim = (-0.1, 1.1))\n",
    "# grid.map(sns.scatterplot, \"Prediction Probability\", \"Fidelity\")\n",
    "# grid.set_axis_labels(\"Prefix lengths\", \"Change in Prediction Probability (MAPE)\")\n",
    "# grid.add_legend()\n",
    "# plt.show()\n",
    "for data in datasets:\n",
    "    data_fid = comb_data[comb_data[\"Dataset\"] == data]\n",
    "    data_fid = data_fid.sort_values(['Prefix lengths', 'Explainer'])\n",
    "    #data_fid = data_fid.sort_values('Method', ascending = False)\n",
    "    grid = sns.FacetGrid(data_fid, col = 'Method', row = 'Explainer', hue = 'Hue', \n",
    "                         legend_out = True, height = 3.75, aspect = 1, palette = 'colorblind', ylim = (-0.1, 1.1))\n",
    "    grid.map(sns.scatterplot, \"Prediction Probability\", \"Fidelity\")\n",
    "    grid.set_axis_labels(\"Prefix lengths\", \"Change in Prediction Probability (MAPE)\")\n",
    "    grid.add_legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = [\"single_agg\", \"prefix_agg\", \"prefix_index\", \"single_agg\", \"prefix_agg\", \"prefix_index\", \n",
    "           \"single_agg\", \"prefix_agg\", \"prefix_index\", \"single_agg\", \"prefix_agg\", \"prefix_index\",\n",
    "           \"single_agg\", \"prefix_agg\", \"prefix_index\", \"single_agg\", \"prefix_agg\", \"prefix_index\"]\n",
    "\n",
    "fidelity = [0.2604979788542514, 0.47366466356136006, 0.34376828026350076, 0.27469174680679204, 0.5089115965891036, 0.5080608320400524,\n",
    "            0.35960311142519635, 0.3744494652066537, 0.514911224795386, 0.4602775902898326, 0.4878459323145076, 0.5632143593134832,\n",
    "            0.372209343589421, 0.3785738615379294, 0.31649123221064696, 0.4145380647068754, 0.4164828308584071, 0.39770229602523155]\n",
    "explainers = [\"LIME\", \"LIME\", \"LIME\", \"SHAP\", \"SHAP\", \"SHAP\",\n",
    "             \"LIME\", \"LIME\", \"LIME\", \"SHAP\", \"SHAP\", \"SHAP\",\n",
    "             \"LIME\", \"LIME\", \"LIME\", \"SHAP\", \"SHAP\", \"SHAP\"]\n",
    "datasets = [\"Production\", \"Production\", \"Production\", \"Production\", \"Production\", \"Production\",\n",
    "            \"Sepsis Cases\", \"Sepsis Cases\", \"Sepsis Cases\", \"Sepsis Cases\", \"Sepsis Cases\", \"Sepsis Cases\",\n",
    "            \"BPIC2012\", \"BPIC2012\", \"BPIC2012\", \"BPIC2012\", \"BPIC2012\", \"BPIC2012\"]\n",
    "\n",
    "data_dict = {\"Dataset\": datasets, \"Explainer\": explainers, \"Fidelity\": fidelity, \"Method\": methods}\n",
    "\n",
    "data = pd.DataFrame(data_dict)\n",
    "\n",
    "grid = sns.FacetGrid(data,  col = 'Dataset',# hue = 'Explainer', \n",
    "                     legend_out = True, height = 4, aspect = 1)#, palette = 'colorblind')\n",
    "grid.map(sns.barplot, \"Method\", \"Fidelity\", hue = data[\"Explainer\"], palette = \"colorblind\")\n",
    "grid.set_axis_labels(\"Method\", \"Fidelity\")\n",
    "grid.add_legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(data[\"Method\"], data[\"Fidelity\"], hue = data['Explainer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "datasets = [\"production\", \"sepsis_cases\", \"bpic2012\"]\n",
    "methods = [\"lstm_single_3d\"]\n",
    "dataset_names = {\"production\":\"production\", \"sepsis_cases\":\"sepsis_cases_1\", \"bpic2012\":\"bpic2012_accepted\"}\n",
    "\n",
    "\n",
    "for data in datasets:\n",
    "    print(data)\n",
    "            \n",
    "    dataset_manager = DatasetManager(dataset_names[data])\n",
    "\n",
    "    \n",
    "    for method in methods:\n",
    "        print(method)\n",
    "        folder_loc = os.path.join(PATH, \"%s/%s/\" %(data, method))\n",
    "        \n",
    "        if \"single\" in method:\n",
    "            num_buckets = 1\n",
    "        else:\n",
    "            num_buckets = len([name for name in os.listdir(os.path.join(folder_loc,'models/'))])\n",
    "            \n",
    "        train_lengths = []\n",
    "        train_predictions = []\n",
    "        train_y_all = []\n",
    "        \n",
    "        test_lengths = []\n",
    "        test_predictions = []\n",
    "        test_y_all = []\n",
    "            \n",
    "        for bucket in range(num_buckets):\n",
    "            if \"lstm\" in method:\n",
    "                bucketID = \"all\"\n",
    "            else:\n",
    "                bucketID = bucket+1\n",
    "            print ('Bucket', bucketID)\n",
    "            \n",
    "            X_train_path = os.path.join(folder_loc, \"train_data/bucket_%s_prefixes.pickle\" % (bucketID))\n",
    "            with open (X_train_path, 'rb') as f:\n",
    "                dt_train_bucket = pickle.load(f)\n",
    "\n",
    "            X_test_path = os.path.join(folder_loc, \"test_data/bucket_%s_prefixes.pickle\" % (bucketID))\n",
    "            with open (X_test_path, 'rb') as f:\n",
    "                dt_test_bucket = pickle.load(f)\n",
    "            \n",
    "            for each in dt_train_bucket:\n",
    "                prefs = [np.any(row) for row in each]\n",
    "                pref_length = prefs.count(True)\n",
    "                train_lengths.append(pref_length)\n",
    "                \n",
    "            for each in dt_test_bucket:\n",
    "                prefs = [np.any(row) for row in each]\n",
    "                pref_length = prefs.count(True)\n",
    "                test_lengths.append(pref_length)\n",
    "\n",
    "            y_train_path = os.path.join(folder_loc, \"train_data/bucket_%s_labels.pickle\" % (bucketID))\n",
    "            with open (y_train_path, 'rb') as f:\n",
    "                train_y = pickle.load(f)\n",
    "\n",
    "            y_test_path = os.path.join(folder_loc, \"test_data/bucket_%s_labels.pickle\" % (bucketID))\n",
    "            with open (y_test_path, 'rb') as f:\n",
    "                test_y = pickle.load(f)\n",
    "\n",
    "            y_train = [int(np.where(i==1)[0]) for i in train_y]\n",
    "            y_test = test_y\n",
    "            \n",
    "            train_y_all.extend(y_train)\n",
    "            test_y_all.extend(y_test)\n",
    "\n",
    "            params_path = os.path.join(folder_loc, \"cls/params.pickle\")\n",
    "            with open (params_path, 'rb') as f:\n",
    "                args = pickle.load(f)\n",
    "                    \n",
    "            max_len = args['max_len']\n",
    "            data_dim = args['data_dim']\n",
    "            print(\"Parameters loaded\")\n",
    "\n",
    "            #create model\n",
    "            print(\"defining input layer\")\n",
    "            main_input = Input(shape=(max_len, data_dim), name='main_input')\n",
    "\n",
    "            print(\"adding lstm layers\")\n",
    "            if args[\"lstm_layers\"][\"layers\"] == \"one\":\n",
    "                l2_3 = LSTM(args['lstm1_nodes'], input_shape=(max_len, data_dim), implementation=2, \n",
    "                            kernel_initializer='glorot_uniform', return_sequences=False, \n",
    "                            recurrent_dropout=args['lstm1_dropouts'], stateful = False)(main_input)\n",
    "                b2_3 = BatchNormalization()(l2_3)\n",
    "\n",
    "            if args[\"lstm_layers\"][\"layers\"] == \"two\":\n",
    "                l1 = LSTM(args['lstm1_nodes'], input_shape=(max_len, data_dim), implementation=2, \n",
    "                        kernel_initializer='glorot_uniform', return_sequences=True, \n",
    "                        recurrent_dropout=args['lstm1_dropouts'], stateful = False)(main_input)\n",
    "                b1 = BatchNormalization()(l1)\n",
    "                l2_3 = LSTM(args[\"lstm_layers\"][\"lstm2_nodes\"], activation=\"sigmoid\", \n",
    "                            implementation=2, kernel_initializer='glorot_uniform', return_sequences=False, \n",
    "                            recurrent_dropout=args[\"lstm_layers\"][\"lstm2_dropouts\"], stateful = False)(b1)\n",
    "                b2_3 = BatchNormalization()(l2_3)\n",
    "\n",
    "            if args[\"lstm_layers\"][\"layers\"] == \"three\":\n",
    "                l1 = LSTM(args['lstm1_nodes'], input_shape=(max_len, data_dim),implementation=2, \n",
    "                        kernel_initializer='glorot_uniform', return_sequences=True, \n",
    "                        recurrent_dropout=args['lstm1_dropouts'], stateful = False)(main_input)\n",
    "                b1 = BatchNormalization()(l1)\n",
    "                l2 = LSTM(args[\"lstm_layers\"][\"lstm2_nodes\"], activation=\"sigmoid\", \n",
    "                            implementation=2, kernel_initializer='glorot_uniform', return_sequences=True, \n",
    "                            recurrent_dropout=args[\"lstm_layers\"][\"lstm2_dropouts\"], stateful = False)(b1)\n",
    "                b2 = BatchNormalization()(l2)\n",
    "                l2_3 = LSTM(args[\"lstm_layers\"][\"lstm3_nodes\"], activation=\"sigmoid\", \n",
    "                            implementation=2, kernel_initializer='glorot_uniform', return_sequences=False, \n",
    "                            recurrent_dropout=args[\"lstm_layers\"][\"lstm3_dropouts\"], stateful = False)(b2)\n",
    "                b2_3 = BatchNormalization()(l2_3)\n",
    "\n",
    "            print(\"adding dense layers\")\n",
    "            if args['dense_layers']['layers'] == \"two\":\n",
    "                d1 = Dense(args['dense_layers']['dense2_nodes'], activation = \"relu\")(b2_3)\n",
    "                outcome_output = Dense(2, activation='sigmoid', kernel_initializer='glorot_uniform', name='outcome_output')(d1)\n",
    "\n",
    "            else:\n",
    "                outcome_output = Dense(2, activation='sigmoid', kernel_initializer='glorot_uniform', name='outcome_output')(b2_3)\n",
    "\n",
    "            print(\"putting together layers\")\n",
    "            cls = Model(inputs=[main_input], outputs=[outcome_output])\n",
    "\n",
    "            print(\"choosing optimiser\")\n",
    "            if args['optimizer'] == \"adam\":\n",
    "                opt = Nadam(lr=args['learning_rate'], beta_1=0.9, beta_2=0.999, epsilon=1e-08, schedule_decay=0.004, clipvalue=3)\n",
    "            elif args['optimizer'] == \"rmsprop\":\n",
    "                opt = RMSprop(lr=args['learning_rate'], rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "            print(\"adding weights to model\")\n",
    "            #print(os.getcwd())\n",
    "            #print(folder_loc)\n",
    "            checkpoint_path = os.path.join(folder_loc, \"cls/cls.h5\")\n",
    "            #print(folder_loc)\n",
    "            #print(checkpoint_path)\n",
    "            #checkpointPath = \"C:\\\\Users\\\\mythr\\\\Documents\\\\GitHub\\\\Stability-Experiments\\\\benchmark_interpretability\\\\PPM_Stability\\\\production\\\\lstm_single_3d\\\\cls\\\\checkpoint.cpt\"\n",
    "\n",
    "            #with open (checkpoint_path, 'r') as f:\n",
    "            #    print(f)\n",
    "            #    cpt = cls.load_weights(f)\n",
    "            weights = cls.load_weights(checkpoint_path)\n",
    "            #print(weights.assert_consumed())\n",
    "\n",
    "            print(\"compiling model\")\n",
    "            cls.compile(loss='binary_crossentropy', optimizer=opt)\n",
    "\n",
    "            preds = cls.predict(dt_train_bucket)\n",
    "            train_preds = [np.argmax(i, axis = -1) for i in preds]\n",
    "            train_predictions.extend(train_preds)\n",
    "\n",
    "            preds = cls.predict(dt_test_bucket)\n",
    "            test_preds = [np.argmax(i, axis = -1) for i in preds]\n",
    "            test_predictions.extend(test_preds)\n",
    "            \n",
    "            \n",
    "            \n",
    "        lens = list(set(train_lengths))\n",
    "        train_accs = []\n",
    "        test_accs = []\n",
    "        for i in lens:\n",
    "            idxs = [n for n in range(len(train_lengths)) if train_lengths[n] == i]\n",
    "            preds = [train_predictions[n] for n in idxs]\n",
    "            actual = [train_y_all[n] for n in idxs]\n",
    "            accuracy = accuracy_score(actual, preds)\n",
    "            train_accs.append(accuracy)\n",
    "            \n",
    "            idxs = [n for n in range(len(test_lengths)) if test_lengths[n] == i]\n",
    "            preds = [test_predictions[n] for n in idxs]\n",
    "            actual = [test_y_all[n] for n in idxs]\n",
    "            accuracy = accuracy_score(actual, preds)\n",
    "            test_accs.append(accuracy)\n",
    "\n",
    "\n",
    "        plt.plot(lens, train_accs, label = 'train')\n",
    "        plt.plot(lens, test_accs, label = 'test')\n",
    "        plt.legend(loc = \"upper left\")\n",
    "        plt.title(data+method)\n",
    "        plt.show()\n",
    "        \n",
    "        train_acc = accuracy_score(train_y_all, train_predictions)\n",
    "        print(\"\\tTraining accuracy:\", train_acc)\n",
    "        \n",
    "        train_pos_idx = [i for i in range(len(train_predictions)) if train_predictions[i] == 1]\n",
    "        actual = [train_y_all[i] for i in train_pos_idx]\n",
    "        preds = [train_predictions[i] for i in train_pos_idx]\n",
    "        train_pos_acc = accuracy_score(actual, preds)\n",
    "        print(\"\\t\\tTraining accuracy - Positives:\", train_pos_acc)\n",
    "        \n",
    "        train_neg_idx = [i for i in range(len(train_predictions)) if train_predictions[i] == 0]\n",
    "        actual = [train_y_all[i] for i in train_neg_idx]\n",
    "        preds = [train_predictions[i] for i in train_neg_idx]\n",
    "        train_neg_acc = accuracy_score(actual, preds)\n",
    "        print(\"\\t\\tTraining accuracy - Negatives:\", train_neg_acc)\n",
    "        \n",
    "        test_acc = accuracy_score(test_y_all, test_predictions)\n",
    "        print(\"\\tTesting accuracy:\", test_acc)\n",
    "        \n",
    "        test_pos_idx = [i for i in range(len(test_predictions)) if test_predictions[i] == 1]\n",
    "        actual = [test_y_all[i] for i in test_pos_idx]\n",
    "        preds = [test_predictions[i] for i in test_pos_idx]\n",
    "        test_pos_acc = accuracy_score(actual, preds)\n",
    "        print(\"\\t\\tTesting accuracy - Positives:\", test_pos_acc)\n",
    "        \n",
    "        test_neg_idx = [i for i in range(len(test_predictions)) if test_predictions[i] == 0]\n",
    "        actual = [test_y_all[i] for i in test_neg_idx]\n",
    "        preds = [test_predictions[i] for i in test_neg_idx]\n",
    "        test_neg_acc = accuracy_score(actual, preds)\n",
    "        print(\"\\t\\tTesting accuracy - Negatives:\", test_neg_acc)\n",
    "            \n",
    "        #break\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
