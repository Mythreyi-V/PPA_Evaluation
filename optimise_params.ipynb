{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27846,
     "status": "ok",
     "timestamp": 1605608027790,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh6DP4Hn-qpp593Bc6lrayXXcdQv42KAS3zJ-Ay=s64",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "edGOGS4Qly-r",
    "outputId": "21021b16-f710-4bbe-9766-fabf50ac9db6"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "#Use if working on Colab\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "#PATH = '/content/drive/My Drive/PPM_Stability/'\n",
    "\n",
    "#If working locally\n",
    "PATH = os.getcwd()\n",
    "sys.path.append(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 27837,
     "status": "ok",
     "timestamp": 1605608027794,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh6DP4Hn-qpp593Bc6lrayXXcdQv42KAS3zJ-Ay=s64",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "g6rF9K-4l-a4"
   },
   "outputs": [],
   "source": [
    "#!pip install xgboost==1.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 35182,
     "status": "ok",
     "timestamp": 1605608035145,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh6DP4Hn-qpp593Bc6lrayXXcdQv42KAS3zJ-Ay=s64",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "2yT7AjVBlUVP"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#PPM-relevant modules\n",
    "import EncoderFactory\n",
    "#from DatasetManager_for_colab import DatasetManager\n",
    "from DatasetManager import DatasetManager\n",
    "import BucketFactory\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import resample\n",
    "\n",
    "\n",
    "import time\n",
    "import sys\n",
    "from sys import argv\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "#import catboost\n",
    "\n",
    "from tensorflow.keras.backend import print_tensor\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Embedding, Flatten, Input\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import Nadam, RMSprop\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "\n",
    "from hyperopt import Trials, STATUS_OK, tpe, fmin, hp\n",
    "import hyperopt\n",
    "from hyperopt.pyll.base import scope\n",
    "from hyperopt.pyll.stochastic import sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 35182,
     "status": "ok",
     "timestamp": 1605608035150,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh6DP4Hn-qpp593Bc6lrayXXcdQv42KAS3zJ-Ay=s64",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "kCUZV8MylUVY",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_and_evaluate_model(args):\n",
    "    global trial_nr\n",
    "    trial_nr += 1\n",
    "    \n",
    "    start = time.time()\n",
    "    score = 0\n",
    "    for cv_iter in range(n_splits):\n",
    "        \n",
    "        dt_test_prefixes = dt_prefixes[cv_iter]\n",
    "        dt_train_prefixes = pd.DataFrame()\n",
    "        for cv_train_iter in range(n_splits): \n",
    "            if cv_train_iter != cv_iter:\n",
    "                dt_train_prefixes = pd.concat([dt_train_prefixes, dt_prefixes[cv_train_iter]], axis=0, sort=False)\n",
    "        \n",
    "        # Bucketing prefixes based on control flow\n",
    "        bucketer_args = {'encoding_method':bucket_encoding, \n",
    "                         'case_id_col':dataset_manager.case_id_col, \n",
    "                         'cat_cols':[dataset_manager.activity_col], \n",
    "                         'num_cols':[], \n",
    "                         'random_state':random_state}\n",
    "        if bucket_method == \"cluster\":\n",
    "            bucketer_args[\"n_clusters\"] = args[\"n_clusters\"]\n",
    "        bucketer = BucketFactory.get_bucketer(bucket_method, **bucketer_args)\n",
    "        bucket_assignments_train = bucketer.fit_predict(dt_train_prefixes)\n",
    "        bucket_assignments_test = bucketer.predict(dt_test_prefixes)\n",
    "        \n",
    "        preds_all = []\n",
    "        test_y_all = []\n",
    "        if \"prefix\" in method_name:\n",
    "            scores = defaultdict(int)\n",
    "        for bucket in set(bucket_assignments_test):\n",
    "            relevant_train_cases_bucket = dataset_manager.get_indexes(dt_train_prefixes)[bucket_assignments_train == bucket]\n",
    "            relevant_test_cases_bucket = dataset_manager.get_indexes(dt_test_prefixes)[bucket_assignments_test == bucket]\n",
    "            dt_test_bucket = dataset_manager.get_relevant_data_by_indexes(dt_test_prefixes, relevant_test_cases_bucket)\n",
    "            test_y = dataset_manager.get_label_numeric(dt_test_bucket)\n",
    "            if len(relevant_train_cases_bucket) == 0:\n",
    "                preds = [class_ratios[cv_iter]] * len(relevant_test_cases_bucket)\n",
    "            else:\n",
    "                dt_train_bucket = dataset_manager.get_relevant_data_by_indexes(dt_train_prefixes, relevant_train_cases_bucket) # one row per event\n",
    "                train_y = dataset_manager.get_label_numeric(dt_train_bucket)\n",
    "\n",
    "                if len(set(train_y)) < 2:\n",
    "                    preds = [train_y[0]] * len(relevant_test_cases_bucket)\n",
    "                else:\n",
    "                    feature_combiner = FeatureUnion([(method, EncoderFactory.get_encoder(method, **cls_encoder_args)) for method in methods])\n",
    "\n",
    "                    if cls_method == \"rf\":\n",
    "                        cls = RandomForestClassifier(n_estimators=500,\n",
    "                                                        max_features=args['max_features'],\n",
    "                                                        random_state=random_state)\n",
    "\n",
    "                    elif cls_method == \"xgboost\":\n",
    "                        cls = xgb.XGBClassifier(objective='binary:logistic',\n",
    "                                                n_estimators=500,\n",
    "                                                learning_rate= args['learning_rate'],\n",
    "                                                subsample=args['subsample'],\n",
    "                                                max_depth=int(args['max_depth']),\n",
    "                                                colsample_bytree=args['colsample_bytree'],\n",
    "                                                min_child_weight=int(args['min_child_weight']),\n",
    "                                                seed=random_state)\n",
    "                    elif cls_method == \"cb\":\n",
    "                        cls =  catboost.CatBoostClassifier(learning_rate= args['learning_rate'],\n",
    "                                                            subsample=args['subsample'], depth=int(args['depth']))\n",
    "\n",
    "                    elif cls_method == \"logit\":\n",
    "                        cls = LogisticRegression(C=2**args['C'],\n",
    "                                                    random_state=random_state)\n",
    "                    elif cls_method == \"svm\":\n",
    "                        cls = SVC(C=2**args['C'],\n",
    "                                    gamma=2**args['gamma'],\n",
    "                                    random_state=random_state)\n",
    "\n",
    "                    if cls_method == \"svm\" or cls_method == \"logit\":\n",
    "                        pipeline = Pipeline([('encoder', feature_combiner), ('scaler', StandardScaler()), ('cls', cls)])\n",
    "                    else:\n",
    "                        pipeline = Pipeline([('encoder', feature_combiner), ('cls', cls)])\n",
    "                    pipeline.fit(dt_train_bucket, train_y)\n",
    "\n",
    "                    if cls_method == \"svm\":\n",
    "                        preds = pipeline.decision_function(dt_test_bucket)\n",
    "                    else:\n",
    "                        preds_pos_label_idx = np.where(cls.classes_ == 1)[0][0]\n",
    "                        preds = pipeline.predict_proba(dt_test_bucket)[:,preds_pos_label_idx]\n",
    "            \n",
    "            if \"prefix\" in method_name:\n",
    "                auc = 0.5\n",
    "                if len(set(test_y)) == 2: \n",
    "                    auc = roc_auc_score(test_y, preds)\n",
    "                scores[bucket] += auc\n",
    "            preds_all.extend(preds)\n",
    "            test_y_all.extend(test_y)\n",
    "\n",
    "        score += roc_auc_score(test_y_all, preds_all)\n",
    "        #acc = accuracy_score(test_y_all, preds_all)\n",
    "        auc = roc_auc_score(test_y_all, preds_all)\n",
    "        \n",
    "        #print('Accuracy:', acc, \"\\tROCAUC:\", auc)\n",
    "        print(\"ROCAUC:\", auc)\n",
    "    \n",
    "    if \"prefix\" in method_name:\n",
    "        for k, v in args.items():\n",
    "            for bucket, bucket_score in scores.items():\n",
    "                fout_all.write(\"%s;%s;%s;%s;%s;%s;%s;%s\\n\" % (trial_nr, dataset_name, cls_method, method_name, bucket, k, v, bucket_score / n_splits))   \n",
    "        fout_all.write(\"%s;%s;%s;%s;%s;%s;%s;%s\\n\" % (trial_nr, dataset_name, cls_method, method_name, 0, \"processing_time\", time.time() - start, 0))  \n",
    "    else:\n",
    "        for k, v in args.items():\n",
    "            fout_all.write(\"%s;%s;%s;%s;%s;%s;%s\\n\" % (trial_nr, dataset_name, cls_method, method_name, k, v, score / n_splits))   \n",
    "        fout_all.write(\"%s;%s;%s;%s;%s;%s;%s\\n\" % (trial_nr, dataset_name, cls_method, method_name, \"processing_time\", time.time() - start, 0))   \n",
    "    fout_all.flush()\n",
    "    return {'loss': -score / n_splits, 'status': STATUS_OK, 'model': cls}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1993341,
     "status": "ok",
     "timestamp": 1605609993320,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh6DP4Hn-qpp593Bc6lrayXXcdQv42KAS3zJ-Ay=s64",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "y_jI3dzJlUVf",
    "outputId": "b799e298-0c10-4db9-f0b8-b9b2dccbf6f9",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['production']\n",
      "splitting data\n",
      "single                                                                                                                 \n",
      "ROCAUC:                                                                                                                \n",
      "0.5                                                                                                                    \n",
      "single                                                                                                                 \n",
      "ROCAUC:                                                                                                                \n",
      "0.5                                                                                                                    \n",
      "single                                                                                                                 \n",
      "ROCAUC:                                                                                                                \n",
      "0.5                                                                                                                    \n",
      "single                                                                                                                 \n",
      "ROCAUC:                                                                                                                \n",
      "0.49777960526315784                                                                                                    \n",
      "single                                                                                                                 \n",
      "ROCAUC:                                                                                                                \n",
      "0.5100530317971644                                                                                                     \n",
      "single                                                                                                                 \n",
      "ROCAUC:                                                                                                                \n",
      "0.5                                                                                                                    \n",
      "single                                                                                                                 \n",
      "ROCAUC:                                                                                                                \n",
      "0.6423930921052632                                                                                                     \n",
      "single                                                                                                                 \n",
      "ROCAUC:                                                                                                                \n",
      "0.6888107290776412                                                                                                     \n",
      "single                                                                                                                 \n",
      "ROCAUC:                                                                                                                \n",
      "0.6597782258064515                                                                                                     \n",
      "100%|█████████████████████████████████████████████████| 3/3 [00:05<00:00,  1.89s/trial, best loss: -0.6636606823297853]\n"
     ]
    }
   ],
   "source": [
    "dataset_ref = \"production\"\n",
    "params_dir = \"params\"\n",
    "n_iter = 3\n",
    "bucket_method = \"single\"\n",
    "cls_encoding = \"agg\"\n",
    "cls_method = \"xgboost\"\n",
    "\n",
    "if bucket_method == \"state\":\n",
    "    bucket_encoding = \"last\"\n",
    "else:\n",
    "    bucket_encoding = \"agg\"\n",
    "\n",
    "method_name = \"%s_%s\"%(bucket_method, cls_encoding)\n",
    "\n",
    "dataset_ref_to_datasets = {\n",
    "    \"bpic2011\": [\"bpic2011_f%s\"%formula for formula in range(1,5)],\n",
    "    \"bpic2015\": [\"bpic2015_%s_f2\"%(municipality) for municipality in range(5,6)],\n",
    "    \"insurance\": [\"insurance_activity\", \"insurance_followup\"],\n",
    "    \"bpic2012\" : [\"bpic2012_accepted\"],\n",
    "    \"sepsis_cases\": [\"sepsis_cases_1\"],#, \"sepsis_cases_2\", \"sepsis_cases_4\"],\n",
    "    \"production\": [\"production\"]\n",
    "}\n",
    "\n",
    "encoding_dict = {\n",
    "    \"laststate\": [\"static\", \"last\"],\n",
    "    \"agg\": [\"static\", \"agg\"],\n",
    "    \"index\": [\"static\", \"index\"],\n",
    "    \"combined\": [\"static\", \"last\", \"agg\"],\n",
    "    \"3d\" : []\n",
    "}\n",
    "\n",
    "datasets = [dataset_ref] if dataset_ref not in dataset_ref_to_datasets else dataset_ref_to_datasets[dataset_ref]\n",
    "methods = encoding_dict[cls_encoding]\n",
    "print(datasets)\n",
    "    \n",
    "train_ratio = 0.8\n",
    "n_splits = 3\n",
    "random_state = 22\n",
    "\n",
    "# create results directory\n",
    "if not os.path.exists(os.path.join(params_dir)):\n",
    "    os.makedirs(os.path.join(params_dir))\n",
    "    \n",
    "for dataset_name in datasets:\n",
    "    \n",
    "    # read the data\n",
    "    dataset_manager = DatasetManager(dataset_name)\n",
    "    data = dataset_manager.read_dataset()\n",
    "    data = dataset_manager.balance_data(data)\n",
    "\n",
    "    cls_encoder_args = {'case_id_col': dataset_manager.case_id_col, \n",
    "                        'static_cat_cols': dataset_manager.static_cat_cols,\n",
    "                        'static_num_cols': dataset_manager.static_num_cols, \n",
    "                        'dynamic_cat_cols': dataset_manager.dynamic_cat_cols,\n",
    "                        'dynamic_num_cols': dataset_manager.dynamic_num_cols, \n",
    "                        'fillna': True}\n",
    "\n",
    "    # determine min and max (truncated) prefix lengths\n",
    "    min_prefix_length = 1\n",
    "    if \"traffic_fines\" in dataset_name:\n",
    "        max_prefix_length = 10\n",
    "    elif \"bpic2017\" in dataset_name:\n",
    "        max_prefix_length = min(20, dataset_manager.get_pos_case_length_quantile(data, 0.90))\n",
    "    else:\n",
    "        max_prefix_length = min(40, dataset_manager.get_pos_case_length_quantile(data, 0.90))\n",
    "\n",
    "    # split into training and test\n",
    "    print(\"splitting data\")\n",
    "    train, _ = dataset_manager.split_data_strict(data, train_ratio, split=\"temporal\")\n",
    "    \n",
    "    # prepare chunks for CV\n",
    "    dt_prefixes = []\n",
    "    class_ratios = []\n",
    "    for train_chunk, test_chunk in dataset_manager.get_stratified_split_generator(train, n_splits=n_splits):\n",
    "        class_ratios.append(dataset_manager.get_class_ratio(train_chunk))\n",
    "        # generate data where each prefix is a separate instance\n",
    "        dt_prefixes.append(dataset_manager.generate_prefix_data(test_chunk, min_prefix_length, max_prefix_length))\n",
    "    del train\n",
    "        \n",
    "    # set up search space\n",
    "    if cls_method == \"rf\":\n",
    "        space = {'max_features': hp.uniform('max_features', 0, 1)}\n",
    "    elif cls_method == \"xgboost\":\n",
    "        space = {'learning_rate': hp.uniform(\"learning_rate\", 0, 5),\n",
    "                 'subsample': hp.uniform(\"subsample\", 0.5, 1),\n",
    "                 'max_depth': scope.int(hp.quniform('max_depth', 1, 30, 1)),\n",
    "                 'colsample_bytree': hp.uniform(\"colsample_bytree\", 0, 1),\n",
    "                 'min_child_weight': scope.int(hp.quniform('min_child_weight', 0, 6, 1))}\n",
    "    elif cls_method == \"logit\":\n",
    "        space = {'C': hp.uniform('C', -15, 15)}\n",
    "    elif cls_method == \"svm\":\n",
    "        space = {'C': hp.uniform('C', -15, 15),\n",
    "                 'gamma': hp.uniform('gamma', -15, 15)}\n",
    "    elif cls_method == \"cb\":\n",
    "        space = {'learning_rate': hp.uniform(\"learning_rate\", 0, 1),\n",
    "                 'depth': scope.int(hp.quniform('max_depth', 4, 16, 1)),\n",
    "                 'subsample': hp.uniform(\"subsample\", 0.5, 1)}\n",
    "    elif cls_method == \"lstm\":\n",
    "        space = {'lstm1_nodes':hp.choice('units_lsmt1', [10,20,50]),#,100,150,200]),\n",
    "                 'lstm1_dropouts':hp.loguniform('dos_lstm1',np.log(0.001),np.log(0.5)), \n",
    "                 'lstm_layers': hp.choice('num_layers_lstm',[{'layers':'one'},\n",
    "                                {'layers':'two','lstm2_nodes':hp.choice('units_lstm_2', [10,20,50]),#,100,150,200]),\n",
    "                                'lstm2_dropouts':hp.loguniform('dos_lstm_2',np.log(0.001),np.log(0.5))},\n",
    "                                {'layers':'three','lstm2_nodes':hp.choice('units_lstm2', [10,20,50]),#,100,150,200]),\n",
    "                                'lstm2_dropouts':hp.loguniform('dos_lstm2',np.log(0.001),np.log(0.5)),\n",
    "                                'lstm3_nodes':hp.choice('units_lstm3', [10,20,50]),#,100,150,200]),\n",
    "                                'lstm3_dropouts':hp.loguniform('dos_lstm3',np.log(0.001),np.log(0.5))}]),\n",
    "                 'dense_layers': hp.choice('num_layers_dense',[{'layers':'one'},\n",
    "                                {'layers':'two','dense2_nodes':hp.choice('units_dense', [10,20,30,40])}]),\n",
    "                 \"optimizer\": hp.choice('optmz',[\"adam\", \"rmsprop\"]), \n",
    "                 'epochs':hp.choice('ep', [50, 100, 200, 300, 500]),\n",
    "                 \"batch_size\":hp.choice('bs',[8, 16, 32, 64, 128, 256]),\n",
    "                 \"learning_rate\":hp.loguniform('lr', np.log(0.0001), np.log(0.01))}\n",
    "        \n",
    "    if bucket_method == \"cluster\":\n",
    "        space['n_clusters'] = scope.int(hp.quniform('n_clusters', 2, 50, 1))\n",
    "\n",
    "    # optimize parameters\n",
    "    trial_nr = 1\n",
    "    trials = Trials()\n",
    "    fout_all = open(os.path.join(PATH, params_dir, \"param_optim_all_trials_%s_%s_%s.csv\" % (cls_method, dataset_name, method_name)), \"w\")\n",
    "    if \"prefix\" in method_name:\n",
    "        fout_all.write(\"%s;%s;%s;%s;%s;%s;%s;%s\\n\" % (\"iter\", \"dataset\", \"cls\", \"method\", \"nr_events\", \"param\", \"value\", \"score\"))   \n",
    "    else:\n",
    "        fout_all.write(\"%s;%s;%s;%s;%s;%s;%s\\n\" % (\"iter\", \"dataset\", \"cls\", \"method\", \"param\", \"value\", \"score\"))   \n",
    "    best = fmin(create_and_evaluate_model, space, algo=tpe.suggest, max_evals=n_iter, trials=trials, verbose=True)\n",
    "    fout_all.close()\n",
    "\n",
    "    # write the best parameters\n",
    "    best_params = hyperopt.space_eval(space, best)\n",
    "    outfile = os.path.join(PATH, params_dir, \"optimal_params_%s_%s_%s.pickle\" % (cls_method, dataset_name, method_name))\n",
    "    # write to file\n",
    "    with open(outfile, \"wb\") as fout:\n",
    "        pickle.dump(best_params, fout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1993332,
     "status": "ok",
     "timestamp": 1605609993324,
     "user": {
      "displayName": "Mythreyi Velmurugan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh6DP4Hn-qpp593Bc6lrayXXcdQv42KAS3zJ-Ay=s64",
      "userId": "09509504425224260690"
     },
     "user_tz": -600
    },
    "id": "hyFDK-SFa5fq",
    "outputId": "1fa48c4a-d93e-4751-dc63-6f1a8f6ac8b2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.6295214399492504,\n",
       " 'learning_rate': 0.8386828987068323,\n",
       " 'max_depth': 15,\n",
       " 'min_child_weight': 2,\n",
       " 'subsample': 0.7140914392810376}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Optimise Params.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
